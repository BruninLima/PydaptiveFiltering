window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "pydaptivefiltering", "modulename": "pydaptivefiltering", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.AdaptiveFilter", "modulename": "pydaptivefiltering", "qualname": "AdaptiveFilter", "kind": "class", "doc": "<p>Abstract base class for all adaptive filters.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order:\n    Order in the FIR sense (number of taps - 1). For non-FIR structures, it can be used\n    as a generic size indicator for base allocation.\nw_init:\n    Initial coefficient vector. If None, initialized to zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<ul>\n<li>Subclasses should set <code>supports_complex = True</code> if they support complex-valued data.</li>\n<li>Subclasses are expected to call <code>_record_history()</code> every iteration (or use helper methods)\nif they want coefficient trajectories.</li>\n</ul>\n", "bases": "abc.ABC"}, {"fullname": "pydaptivefiltering.AdaptiveFilter.supports_complex", "modulename": "pydaptivefiltering", "qualname": "AdaptiveFilter.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.AdaptiveFilter.filter_order", "modulename": "pydaptivefiltering", "qualname": "AdaptiveFilter.filter_order", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.AdaptiveFilter.regressor", "modulename": "pydaptivefiltering", "qualname": "AdaptiveFilter.regressor", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.AdaptiveFilter.w_history", "modulename": "pydaptivefiltering", "qualname": "AdaptiveFilter.w_history", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[numpy.ndarray]"}, {"fullname": "pydaptivefiltering.AdaptiveFilter.filter_signal", "modulename": "pydaptivefiltering", "qualname": "AdaptiveFilter.filter_signal", "kind": "function", "doc": "<p>Filter an input signal using current coefficients.</p>\n\n<p>Default implementation assumes an FIR structure with taps <code>self.w</code> and\nregressor convention:\n    x_k = [x[k], x[k-1], ..., x[k-m]]\nand output:\n    y[k] = w^H x_k   (Hermitian for complex)</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">],</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">complex</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.AdaptiveFilter.default_test_init_kwargs", "modulename": "pydaptivefiltering", "qualname": "AdaptiveFilter.default_test_init_kwargs", "kind": "function", "doc": "<p>Override in subclasses to provide init kwargs for standardized tests.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">order</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.AdaptiveFilter.optimize", "modulename": "pydaptivefiltering", "qualname": "AdaptiveFilter.optimize", "kind": "function", "doc": "<p>Run the adaptation procedure.</p>\n\n<p>Subclasses should return either:</p>\n\n<ul>\n<li>OptimizationResult (recommended), or</li>\n<li>dict-like with standardized keys, if you are migrating older code.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">],</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">complex</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">],</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">complex</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.AdaptiveFilter.reset_filter", "modulename": "pydaptivefiltering", "qualname": "AdaptiveFilter.reset_filter", "kind": "function", "doc": "<p>Reset coefficients and history.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">w_new</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">],</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">complex</span><span class=\"p\">],</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.LMS", "modulename": "pydaptivefiltering", "qualname": "LMS", "kind": "class", "doc": "<p>Complex Least-Mean Squares (LMS) adaptive filter.</p>\n\n<p>Standard complex LMS algorithm for adaptive FIR filtering, following Diniz\n(Alg. 3.2). The method performs a stochastic-gradient update using the\ninstantaneous a priori error.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\nstep_size : float, optional\n    Adaptation step size <code>mu</code>. Default is 1e-2.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>At iteration <code>k</code>, form the regressor vector (newest sample first):</p>\n\n<p>$$x_k = [x[k], x[k-1], \\ldots, x[k-M]]^T \\in \\mathbb{C}^{M+1}.$$</p>\n\n<p>The a priori output and error are</p>\n\n<p>$$y[k] = w^H[k] x_k, \\qquad e[k] = d[k] - y[k],$$</p>\n\n<p>and the LMS update is</p>\n\n<p>$$w[k+1] = w[k] + \\mu\\, e^*[k] \\, x_k.$$</p>\n\n<p>This implementation:\n    - uses complex arithmetic (<code>supports_complex=True</code>),\n    - returns the a priori error <code>e[k]</code>,\n    - records coefficient history via the base class.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.LMS.__init__", "modulename": "pydaptivefiltering", "qualname": "LMS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.LMS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "LMS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.LMS.step_size", "modulename": "pydaptivefiltering", "qualname": "LMS.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.LMS.optimize", "modulename": "pydaptivefiltering", "qualname": "LMS.optimize", "kind": "function", "doc": "<p>Executes the LMS adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Scalar output sequence, <code>y[k] = w^H[k] x_k</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.NLMS", "modulename": "pydaptivefiltering", "qualname": "NLMS", "kind": "class", "doc": "<p>Complex Normalized Least-Mean Squares (NLMS) adaptive filter.</p>\n\n<p>Normalized LMS algorithm for adaptive FIR filtering, following Diniz\n(Alg. 4.3). The method normalizes the step size by the instantaneous\nregressor energy to improve stability and reduce sensitivity to input\nscaling.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\nstep_size : float, optional\n    Base adaptation step size <code>mu</code>. Default is 1e-2.\ngamma : float, optional\n    Regularization constant <code>gamma</code> used in the normalization denominator\n    to avoid division by zero (or near-zero regressor energy). Default is 1e-6.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>At iteration <code>k</code>, form the regressor vector (newest sample first):</p>\n\n<p>$$x_k = [x[k], x[k-1], \\ldots, x[k-M]]^T \\in \\mathbb{C}^{M+1}.$$</p>\n\n<p>The a priori output and error are</p>\n\n<p>$$y[k] = w^H[k] x_k, \\qquad e[k] = d[k] - y[k].$$</p>\n\n<p>Define the instantaneous regressor energy</p>\n\n<p>$$\\|x_k\\|^2 = x_k^H x_k,$$</p>\n\n<p>and the normalized step size</p>\n\n<p>$$\\mu_k = \\frac{\\mu}{\\|x_k\\|^2 + \\gamma}.$$</p>\n\n<p>The NLMS update is then</p>\n\n<p>$$w[k+1] = w[k] + \\mu_k\\, e^*[k] \\, x_k.$$</p>\n\n<p>This implementation:\n    - uses complex arithmetic (<code>supports_complex=True</code>),\n    - returns the a priori error <code>e[k]</code>,\n    - records coefficient history via the base class.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.NLMS.__init__", "modulename": "pydaptivefiltering", "qualname": "NLMS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.NLMS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "NLMS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.NLMS.step_size", "modulename": "pydaptivefiltering", "qualname": "NLMS.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.NLMS.gamma", "modulename": "pydaptivefiltering", "qualname": "NLMS.gamma", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.NLMS.optimize", "modulename": "pydaptivefiltering", "qualname": "NLMS.optimize", "kind": "function", "doc": "<p>Executes the NLMS adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Scalar output sequence, <code>y[k] = w^H[k] x_k</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.AffineProjection", "modulename": "pydaptivefiltering", "qualname": "AffineProjection", "kind": "class", "doc": "<p>Complex Affine-Projection Algorithm (APA) adaptive filter.</p>\n\n<p>Affine-projection LMS-type algorithm that reuses the last <code>L+1</code> regressor\nvectors to accelerate convergence relative to LMS/NLMS, following Diniz\n(Alg. 4.6). Per iteration, the method solves a small linear system of size\n<code>(L+1) x (L+1)</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\nstep_size : float, optional\n    Adaptation step size (relaxation factor) <code>mu</code>. Default is 1e-2.\ngamma : float, optional\n    Diagonal loading (regularization) <code>gamma</code> applied to the projection\n    correlation matrix for numerical stability. Default is 1e-6.\nL : int, optional\n    Reuse factor (projection order). The algorithm uses <code>L + 1</code> most recent\n    regressors. Default is 2.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>At iteration <code>k</code>, form the projection matrix and desired vector:</p>\n\n<ul>\n<li><code>X(k) \u2208 C^{(L+1) x (M+1)}</code>, whose rows are regressor vectors, with the most\nrecent regressor at row 0.</li>\n<li><code>d_vec(k) \u2208 C^{L+1}</code>, stacking the most recent desired samples, with\n<code>d[k]</code> at index 0.</li>\n</ul>\n\n<p>The projection output and error vectors are:</p>\n\n<p>$$y_{vec}(k) = X(k)\\,w^*(k) \\in \\mathbb{C}^{L+1},$$</p>\n\n<p>$$e_{vec}(k) = d_{vec}(k) - y_{vec}(k).$$</p>\n\n<p>The update direction <code>u(k)</code> is obtained by solving the regularized system:</p>\n\n<p>$$(X(k)X^H(k) + \\gamma I_{L+1})\\,u(k) = e_{vec}(k),$$</p>\n\n<p>and the coefficient update is:</p>\n\n<p>$$w(k+1) = w(k) + \\mu X^H(k)\\,u(k).$$</p>\n\n<p>This implementation returns only the <em>most recent</em> scalar components:</p>\n\n<ul>\n<li><code>y[k] = y_vec(k)[0]</code></li>\n<li><code>e[k] = e_vec(k)[0]</code></li>\n</ul>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.AffineProjection.__init__", "modulename": "pydaptivefiltering", "qualname": "AffineProjection.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">L</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.AffineProjection.supports_complex", "modulename": "pydaptivefiltering", "qualname": "AffineProjection.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.AffineProjection.step_size", "modulename": "pydaptivefiltering", "qualname": "AffineProjection.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.AffineProjection.gamma", "modulename": "pydaptivefiltering", "qualname": "AffineProjection.gamma", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.AffineProjection.memory_length", "modulename": "pydaptivefiltering", "qualname": "AffineProjection.memory_length", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.AffineProjection.optimize", "modulename": "pydaptivefiltering", "qualname": "AffineProjection.optimize", "kind": "function", "doc": "<p>Executes the Affine Projection adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes the last internal states in <code>result.extra</code>:\n    <code>\"last_regressor_matrix\"</code> (<code>X(k)</code>) and\n    <code>\"last_correlation_matrix\"</code> (<code>X(k)X^H(k) + gamma I</code>).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Scalar output sequence, <code>y[k] = y_vec(k)[0]</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = e_vec(k)[0]</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.SignData", "modulename": "pydaptivefiltering", "qualname": "SignData", "kind": "class", "doc": "<p>Complex Sign-Data LMS adaptive filter.</p>\n\n<p>Low-complexity LMS variant in which the regressor vector is replaced by its\nelement-wise sign. This reduces multiplications (since the update uses a\nternary/sign regressor), at the expense of slower convergence and/or larger\nsteady-state misadjustment in many scenarios.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\nstep_size : float, optional\n    Adaptation step size <code>mu</code>. Default is 1e-2.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>At iteration <code>k</code>, form the regressor vector (newest sample first):</p>\n\n<p>$$x_k = [x[k], x[k-1], \\ldots, x[k-M]]^T \\in \\mathbb{C}^{M+1}.$$</p>\n\n<p>The a priori output and error are</p>\n\n<p>$$y[k] = w^H[k] x_k, \\qquad e[k] = d[k] - y[k].$$</p>\n\n<p>Define the element-wise sign regressor <code>\\operatorname{sign}(x_k)</code>.\nThe update implemented here is</p>\n\n<p>$$w[k+1] = w[k] + 2\\mu\\, e^*[k] \\, \\operatorname{sign}(x_k).$$</p>\n\n<p>Implementation details\n    - For complex inputs, <code>numpy.sign</code> applies element-wise and returns\n      <code>x/|x|</code> when <code>x != 0</code> and <code>0</code> when <code>x == 0</code>.\n    - The factor <code>2</code> in the update matches the implementation in this\n      module (consistent with common LMS gradient conventions).</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.SignData.__init__", "modulename": "pydaptivefiltering", "qualname": "SignData.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.SignData.supports_complex", "modulename": "pydaptivefiltering", "qualname": "SignData.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.SignData.step_size", "modulename": "pydaptivefiltering", "qualname": "SignData.step_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.SignData.optimize", "modulename": "pydaptivefiltering", "qualname": "SignData.optimize", "kind": "function", "doc": "<p>Executes the Sign-Data LMS adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes the last internal state in <code>result.extra</code>:\n    <code>\"last_sign_regressor\"</code> (<code>sign(x_k)</code>).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Scalar output sequence, <code>y[k] = w^H[k] x_k</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.SignError", "modulename": "pydaptivefiltering", "qualname": "SignError", "kind": "class", "doc": "<p>Sign-Error LMS adaptive filter (real-valued).</p>\n\n<p>Low-complexity LMS variant that replaces the instantaneous error by its sign.\nThis reduces multiplications and can improve robustness under impulsive noise\nin some scenarios, at the expense of slower convergence and/or larger\nsteady-state misadjustment.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\nstep_size : float, optional\n    Adaptation step size <code>mu</code>. Default is 1e-2.\nw_init : array_like of float, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Real-valued only\n    This implementation is restricted to real-valued signals and coefficients\n    (<code>supports_complex=False</code>). The constraint is enforced via\n    <code>@ensure_real_signals</code> on <code>optimize()</code>.</p>\n\n<p>At iteration <code>k</code>, form the regressor vector (newest sample first):</p>\n\n<p>$$x_k = [x[k], x[k-1], \\ldots, x[k-M]]^T \\in \\mathbb{R}^{M+1}.$$</p>\n\n<p>The a priori output and error are</p>\n\n<p>$$y[k] = w^T[k] x_k, \\qquad e[k] = d[k] - y[k].$$</p>\n\n<p>The sign-error update implemented here is</p>\n\n<p>$$w[k+1] = w[k] + \\mu\\, \\operatorname{sign}(e[k])\\, x_k.$$</p>\n\n<p>Implementation details\n    - <code>numpy.sign(0) = 0</code>; therefore if <code>e[k] == 0</code> the update is null.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.SignError.__init__", "modulename": "pydaptivefiltering", "qualname": "SignError.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.SignError.supports_complex", "modulename": "pydaptivefiltering", "qualname": "SignError.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.SignError.step_size", "modulename": "pydaptivefiltering", "qualname": "SignError.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.SignError.optimize", "modulename": "pydaptivefiltering", "qualname": "SignError.optimize", "kind": "function", "doc": "<p>Executes the Sign-Error LMS adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of float\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes the last internal state in <code>result.extra</code>:\n    <code>\"last_sign_error\"</code> (<code>sign(e[k])</code>).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        Scalar output sequence, <code>y[k] = w^T[k] x_k</code>.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.DualSign", "modulename": "pydaptivefiltering", "qualname": "DualSign", "kind": "class", "doc": "<p>Dual-Sign LMS (DS-LMS) adaptive filter (real-valued).</p>\n\n<p>Low-complexity LMS variant that uses the <em>sign</em> of the instantaneous error\nand a two-level (piecewise) effective gain selected by the error magnitude.\nThis can reduce the number of multiplications and may improve robustness\nunder impulsive noise in some scenarios, at the expense of larger steady-state\nmisadjustment.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\nrho : float\n    Threshold <code>rho</code> applied to <code>|e[k]|</code> to select the gain level.\ngamma : float\n    Gain multiplier applied when <code>|e[k]| &gt; rho</code> (typically <code>gamma &gt; 1</code>).\nstep : float, optional\n    Adaptation step size <code>mu</code>. Default is 1e-2.\nw_init : array_like of float, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.\nsafe_eps : float, optional\n    Small positive constant kept for API consistency across the library.\n    (Not used by this implementation.) Default is 1e-12.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Real-valued only\n    This implementation is restricted to real-valued signals and coefficients\n    (<code>supports_complex=False</code>). The constraint is enforced via\n    <code>@ensure_real_signals</code> on <code>optimize()</code>.</p>\n\n<p>Update rule (as implemented)\n    Let the regressor vector be</p>\n\n<pre><code>$$x_k = [x[k], x[k-1], \\ldots, x[k-M]]^T,$$\n\nwith output and error\n\n$$y[k] = w^T[k] x_k, \\qquad e[k] = d[k] - y[k].$$\n\nDefine the two-level signed term\n\n$$u[k] =\n</code></pre>\n\n<p>\\begin{cases}\n    \\operatorname{sign}(e[k]), &amp; |e[k]| \\le \\rho \\\n    \\gamma\\,\\operatorname{sign}(e[k]), &amp; |e[k]| &gt; \\rho\n\\end{cases}$$</p>\n\n<pre><code>and update\n\n$$w[k+1] = w[k] + 2\\mu\\,u[k]\\,x_k.$$\n</code></pre>\n\n<p>Implementation details\n    - <code>numpy.sign(0) = 0</code>; therefore if <code>e[k] == 0</code> the update is null.\n    - The factor <code>2</code> in the update matches the implementation in this\n      module (consistent with common LMS gradient conventions).</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.DualSign.__init__", "modulename": "pydaptivefiltering", "qualname": "DualSign.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">rho</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">step</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span>)</span>"}, {"fullname": "pydaptivefiltering.DualSign.supports_complex", "modulename": "pydaptivefiltering", "qualname": "DualSign.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.DualSign.rho", "modulename": "pydaptivefiltering", "qualname": "DualSign.rho", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.DualSign.gamma", "modulename": "pydaptivefiltering", "qualname": "DualSign.gamma", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.DualSign.step_size", "modulename": "pydaptivefiltering", "qualname": "DualSign.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.DualSign.optimize", "modulename": "pydaptivefiltering", "qualname": "DualSign.optimize", "kind": "function", "doc": "<p>Executes the DS-LMS adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of float\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        Scalar output sequence, <code>y[k] = w^T[k] x_k</code>.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.LMSNewton", "modulename": "pydaptivefiltering", "qualname": "LMSNewton", "kind": "class", "doc": "<p>Complex LMS-Newton adaptive filter.</p>\n\n<p>LMS-Newton accelerates the standard complex LMS by preconditioning the\ninstantaneous gradient with a recursive estimate of the inverse input\ncorrelation matrix. This often improves convergence speed for strongly\ncorrelated inputs, at the cost of maintaining and updating a full\n<code>(M+1) x (M+1)</code> matrix per iteration.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\nalpha : float\n    Forgetting factor <code>alpha</code> used in the inverse-correlation recursion,\n    with <code>0 &lt; alpha &lt; 1</code>. Values closer to 1 yield smoother tracking; smaller\n    values adapt faster.\ninitial_inv_rx : array_like of complex\n    Initial inverse correlation matrix <code>P(0)</code> with shape <code>(M + 1, M + 1)</code>.\n    Typical choices are scaled identities, e.g. <code>delta^{-1} I</code>.\nstep : float, optional\n    Adaptation step size <code>mu</code>. Default is 1e-2.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.\nsafe_eps : float, optional\n    Small positive constant used to guard denominators in the matrix recursion.\n    Default is 1e-12.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Complex-valued\n    This implementation assumes complex arithmetic (<code>supports_complex=True</code>),\n    with the a priori output computed as <code>y[k] = w^H[k] x_k</code>.</p>\n\n<p>Recursion (as implemented)\n    Let the regressor vector be</p>\n\n<pre><code>$$x_k = [x[k], x[k-1], \\ldots, x[k-M]]^T \\in \\mathbb{C}^{M+1},$$\n\nand define the output and a priori error as\n\n$$y[k] = w^H[k] x_k, \\qquad e[k] = d[k] - y[k].$$\n\nMaintain an estimate ``P[k] \\approx R_x^{-1}`` using a normalized rank-1 update.\nWith\n\n$$p_k = P[k] x_k, \\qquad \\phi_k = x_k^H p_k,$$\n\nthe denominator is\n\n$$\\mathrm{denom}_k = \\frac{1-\\alpha}{\\alpha} + \\phi_k,$$\n\nand the update used here is\n\n$$P[k+1] =\n</code></pre>\n\n<p>\\frac{1}{1-\\alpha}\n\\left(\n    P[k] - \\frac{p_k p_k^H}{\\mathrm{denom}_k}\n\\right).$$</p>\n\n<pre><code>The coefficient update uses the preconditioned regressor ``P[k+1] x_k``:\n\n$$w[k+1] = w[k] + \\mu\\, e^*[k] \\, (P[k+1] x_k).$$\n</code></pre>\n\n<p>Relationship to RLS\n    The recursion for <code>P</code> is algebraically similar to an RLS covariance update\n    with a particular normalization; however, the coefficient update remains\n    LMS-like, controlled by the step size <code>mu</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.LMSNewton.__init__", "modulename": "pydaptivefiltering", "qualname": "LMSNewton.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">initial_inv_rx</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span>)</span>"}, {"fullname": "pydaptivefiltering.LMSNewton.supports_complex", "modulename": "pydaptivefiltering", "qualname": "LMSNewton.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.LMSNewton.alpha", "modulename": "pydaptivefiltering", "qualname": "LMSNewton.alpha", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.LMSNewton.step_size", "modulename": "pydaptivefiltering", "qualname": "LMSNewton.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.LMSNewton.inv_rx", "modulename": "pydaptivefiltering", "qualname": "LMSNewton.inv_rx", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.LMSNewton.optimize", "modulename": "pydaptivefiltering", "qualname": "LMSNewton.optimize", "kind": "function", "doc": "<p>Executes the LMS-Newton adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Scalar output sequence, <code>y[k] = w^H[k] x_k</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.Power2ErrorLMS", "modulename": "pydaptivefiltering", "qualname": "Power2ErrorLMS", "kind": "class", "doc": "<p>Power-of-Two Error LMS adaptive filter (real-valued).</p>\n\n<p>LMS variant in which the instantaneous a priori error is quantized to a\npower-of-two level (with special cases for large and very small errors),\naiming to reduce computational complexity in fixed-point / low-cost\nimplementations.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\nbd : int\n    Word length (number of bits) used to define the small-error threshold\n    <code>2^{-bd+1}</code>.\ntau : float\n    Gain factor applied when <code>|e[k]|</code> is very small (below <code>2^{-bd+1}</code>).\nstep_size : float, optional\n    Adaptation step size <code>mu</code>. Default is 1e-2.\nw_init : array_like of float, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Real-valued only\n    This implementation is restricted to real-valued signals and coefficients\n    (<code>supports_complex=False</code>). The constraint is enforced via\n    <code>@ensure_real_signals</code> on <code>optimize()</code>.</p>\n\n<p>Signal model and LMS update\n    Let the regressor vector be</p>\n\n<pre><code>$$x_k = [x[k], x[k-1], \\ldots, x[k-M]]^T \\in \\mathbb{R}^{M+1},$$\n\nwith output and a priori error\n\n$$y[k] = w^T[k] x_k, \\qquad e[k] = d[k] - y[k].$$\n\nThe update uses a quantized error ``q(e[k])``:\n\n$$w[k+1] = w[k] + 2\\mu\\, q(e[k])\\, x_k.$$\n</code></pre>\n\n<p>Error quantization (as implemented)\n    Define the small-error threshold</p>\n\n<pre><code>$$\\epsilon = 2^{-bd+1}.$$\n\nThen the quantizer is\n\n$$q(e) =\n</code></pre>\n\n<p>\\begin{cases}\n    \\operatorname{sign}(e), &amp; |e| \\ge 1, \\\n    \\tau\\,\\operatorname{sign}(e), &amp; |e| &lt; \\epsilon, \\\n    2^{\\lfloor \\log_2(|e|) \\rfloor}\\,\\operatorname{sign}(e),\n    &amp; \\text{otherwise.}\n\\end{cases}$$</p>\n\n<pre><code>Note that ``numpy.sign(0) = 0``; therefore if ``e[k] == 0`` then\n``q(e[k]) = 0`` and the update is null.\n</code></pre>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.Power2ErrorLMS.__init__", "modulename": "pydaptivefiltering", "qualname": "Power2ErrorLMS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">bd</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">tau</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.Power2ErrorLMS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "Power2ErrorLMS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.Power2ErrorLMS.bd", "modulename": "pydaptivefiltering", "qualname": "Power2ErrorLMS.bd", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.Power2ErrorLMS.tau", "modulename": "pydaptivefiltering", "qualname": "Power2ErrorLMS.tau", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.Power2ErrorLMS.step_size", "modulename": "pydaptivefiltering", "qualname": "Power2ErrorLMS.step_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.Power2ErrorLMS.optimize", "modulename": "pydaptivefiltering", "qualname": "Power2ErrorLMS.optimize", "kind": "function", "doc": "<p>Executes the Power-of-Two Error LMS adaptation loop over paired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of float\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes the last internal states in <code>result.extra</code>:\n    <code>\"last_quantized_error\"</code> (<code>q(e[k])</code>) and <code>\"small_threshold\"</code>\n    (<code>2^{-bd+1}</code>).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        Scalar output sequence, <code>y[k] = w^T[k] x_k</code>.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.TDomainLMS", "modulename": "pydaptivefiltering", "qualname": "TDomainLMS", "kind": "class", "doc": "<p>Transform-Domain LMS with a user-provided transform matrix.</p>\n\n<p>Generic transform-domain LMS algorithm (Diniz, Alg. 4.4) parameterized by a\ntransform matrix <code>T</code>. At each iteration, the time-domain regressor is\nmapped to the transform domain, adaptation is performed with per-bin\nnormalization using a smoothed power estimate, and time-domain coefficients\nare recovered from the transform-domain weights.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\n    The transform size must be <code>(M + 1, M + 1)</code>.\ngamma : float\n    Regularization factor <code>gamma</code> used in the per-bin normalization\n    denominator to avoid division by zero (or near-zero power).\nalpha : float\n    Smoothing factor <code>alpha</code> for the transform-bin power estimate,\n    typically close to 1.\ninitial_power : float\n    Initial power estimate used to initialize all transform bins.\ntransform_matrix : array_like of complex\n    Transform matrix <code>T</code> with shape <code>(M + 1, M + 1)</code>.\n    Typically unitary (<code>T^H T = I</code>).\nstep_size : float, optional\n    Adaptation step size <code>mu</code>. Default is 1e-2.\nw_init : array_like of complex, optional\n    Initial **time-domain** coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>.\n    If None, initializes with zeros.\nassume_unitary : bool, optional\n    If True (default), maps transform-domain weights back to the time domain\n    using <code>w = T^H w_T</code> (fast). If False, uses a pseudo-inverse mapping\n    <code>w = pinv(T)^H w_T</code> (slower but works for non-unitary <code>T</code>).</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>At iteration <code>k</code>, form the time-domain regressor vector (newest sample first):</p>\n\n<p>$$x_k = [x[k], x[k-1], \\ldots, x[k-M]]^T \\in \\mathbb{C}^{M+1}.$$</p>\n\n<p>Define the transform-domain regressor:</p>\n\n<p>$$z_k = T x_k.$$</p>\n\n<p>Adaptation is performed in the transform domain with weights <code>w_T[k]</code>.\nThe a priori output and error are</p>\n\n<p>$$y[k] = w_T^H[k] z_k, \\qquad e[k] = d[k] - y[k].$$</p>\n\n<p>A smoothed per-bin power estimate <code>p[k]</code> is updated as</p>\n\n<p>$$p[k] = \\alpha\\,|z_k|^2 + (1-\\alpha)\\,p[k-1],$$</p>\n\n<p>where <code>|z_k|^2</code> is taken element-wise.</p>\n\n<p>The normalized transform-domain LMS update used here is</p>\n\n<p>$$w_T[k+1] = w_T[k] + \\mu\\, e^*[k] \\, \\frac{z_k}{\\gamma + p[k]},$$</p>\n\n<p>with element-wise division.</p>\n\n<p>Mapping back to time domain\n    If <code>T</code> is unitary (<code>T^H T = I</code>), then the inverse mapping is</p>\n\n<pre><code>$$w[k] = T^H w_T[k].$$\n\nIf ``T`` is not unitary and ``assume_unitary=False``, this implementation\nuses the pseudo-inverse mapping:\n\n$$w[k] = \\operatorname{pinv}(T)^H w_T[k].$$\n</code></pre>\n\n<p>Implementation details\n    - <code>OptimizationResult.coefficients</code> stores the <strong>time-domain</strong> coefficient\n      history recorded by the base class (<code>self.w</code> after mapping back).\n    - If <code>return_internal_states=True</code>, the transform-domain coefficient history\n      is returned in <code>result.extra[\"coefficients_transform\"]</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.TDomainLMS.__init__", "modulename": "pydaptivefiltering", "qualname": "TDomainLMS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">initial_power</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">transform_matrix</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">assume_unitary</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span>)</span>"}, {"fullname": "pydaptivefiltering.TDomainLMS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "TDomainLMS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.TDomainLMS.gamma", "modulename": "pydaptivefiltering", "qualname": "TDomainLMS.gamma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainLMS.alpha", "modulename": "pydaptivefiltering", "qualname": "TDomainLMS.alpha", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainLMS.step_size", "modulename": "pydaptivefiltering", "qualname": "TDomainLMS.step_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainLMS.N", "modulename": "pydaptivefiltering", "qualname": "TDomainLMS.N", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainLMS.T", "modulename": "pydaptivefiltering", "qualname": "TDomainLMS.T", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainLMS.w_T", "modulename": "pydaptivefiltering", "qualname": "TDomainLMS.w_T", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainLMS.power_vector", "modulename": "pydaptivefiltering", "qualname": "TDomainLMS.power_vector", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainLMS.optimize", "modulename": "pydaptivefiltering", "qualname": "TDomainLMS.optimize", "kind": "function", "doc": "<p>Executes the Transform-Domain LMS adaptation loop.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes transform-domain internal states in <code>result.extra</code>:\n    <code>\"coefficients_transform\"</code>, <code>\"power_vector_last\"</code>,\n    <code>\"transform_matrix\"</code>, and <code>\"assume_unitary\"</code>.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori output sequence, <code>y[k] = w_T^H[k] z_k</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of complex\n        <strong>Time-domain</strong> coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code> with:\n        - <code>coefficients_transform</code> : ndarray of complex\n            Transform-domain coefficient history.\n        - <code>power_vector_last</code> : ndarray of float\n            Final per-bin power estimate <code>p[k]</code>.\n        - <code>transform_matrix</code> : ndarray of complex\n            The transform matrix <code>T</code> used (shape <code>(M+1, M+1)</code>).\n        - <code>assume_unitary</code> : bool\n            Whether the inverse mapping assumed <code>T</code> is unitary.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.TDomainDCT", "modulename": "pydaptivefiltering", "qualname": "TDomainDCT", "kind": "class", "doc": "<p>Transform-Domain LMS using an orthonormal DCT (complex-valued).</p>\n\n<p>Transform-domain LMS algorithm (Diniz, Alg. 4.4) in which the time-domain\nregressor vector is mapped to a decorrelated transform domain using an\northonormal Discrete Cosine Transform (DCT). Adaptation is performed in the\ntransform domain with per-bin normalization based on a smoothed power\nestimate. The time-domain coefficient vector is recovered from the\ntransform-domain weights.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\ngamma : float\n    Regularization factor <code>gamma</code> used in the per-bin normalization\n    denominator to avoid division by zero (or near-zero power).\nalpha : float\n    Smoothing factor <code>alpha</code> for the transform-bin power estimate,\n    typically close to 1.\ninitial_power : float\n    Initial power estimate used to initialize all transform bins.\nstep_size : float, optional\n    Adaptation step size <code>mu</code>. Default is 1e-2.\nw_init : array_like of complex, optional\n    Initial time-domain coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>.\n    If None, initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>At iteration <code>k</code>, form the time-domain regressor vector (newest sample first):</p>\n\n<p>$$x_k = [x[k], x[k-1], \\ldots, x[k-M]]^T \\in \\mathbb{C}^{M+1}.$$</p>\n\n<p>Let <code>T</code> be the orthonormal DCT matrix of size <code>(M+1) x (M+1)</code>\n(real-valued, with <code>T^T T = I</code>). The transform-domain regressor is</p>\n\n<p>$$z_k = T x_k.$$</p>\n\n<p>Adaptation is performed in the transform domain with weights <code>w_z[k]</code>.\nThe a priori output and error are</p>\n\n<p>$$y[k] = w_z^H[k] z_k, \\qquad e[k] = d[k] - y[k].$$</p>\n\n<p>A smoothed per-bin power estimate <code>p[k]</code> is updated as</p>\n\n<p>$$p[k] = \\alpha\\,|z_k|^2 + (1-\\alpha)\\,p[k-1],$$</p>\n\n<p>where <code>|z_k|^2</code> is taken element-wise (i.e., <code>|z_{k,i}|^2</code>).</p>\n\n<p>The normalized transform-domain LMS update used here is</p>\n\n<p>$$w_z[k+1] = w_z[k] + \\mu\\, e^*[k] \\, \\frac{z_k}{\\gamma + p[k]},$$</p>\n\n<p>where the division is element-wise.</p>\n\n<p>The time-domain coefficients are recovered using orthonormality of <code>T</code>:</p>\n\n<p>$$w[k] = T^T w_z[k].$$</p>\n\n<p>Implementation details\n    - <code>OptimizationResult.coefficients</code> stores the <strong>time-domain</strong> coefficient\n      history recorded by the base class (<code>self.w</code> after the inverse transform).\n    - If <code>return_internal_states=True</code>, the transform-domain coefficient history\n      is returned in <code>result.extra[\"coefficients_dct\"]</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.TDomainDCT.__init__", "modulename": "pydaptivefiltering", "qualname": "TDomainDCT.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">initial_power</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.TDomainDCT.supports_complex", "modulename": "pydaptivefiltering", "qualname": "TDomainDCT.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.TDomainDCT.gamma", "modulename": "pydaptivefiltering", "qualname": "TDomainDCT.gamma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDCT.alpha", "modulename": "pydaptivefiltering", "qualname": "TDomainDCT.alpha", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDCT.step_size", "modulename": "pydaptivefiltering", "qualname": "TDomainDCT.step_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDCT.N", "modulename": "pydaptivefiltering", "qualname": "TDomainDCT.N", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDCT.T", "modulename": "pydaptivefiltering", "qualname": "TDomainDCT.T", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDCT.w_dct", "modulename": "pydaptivefiltering", "qualname": "TDomainDCT.w_dct", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDCT.power_vector", "modulename": "pydaptivefiltering", "qualname": "TDomainDCT.power_vector", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDCT.optimize", "modulename": "pydaptivefiltering", "qualname": "TDomainDCT.optimize", "kind": "function", "doc": "<p>Executes the Transform-Domain LMS (DCT) adaptation loop.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes transform-domain internal states in <code>result.extra</code>:\n    <code>\"coefficients_dct\"</code>, <code>\"power_vector_last\"</code>, and <code>\"dct_matrix\"</code>.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori output sequence, <code>y[k] = w_z^H[k] z_k</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of complex\n        <strong>Time-domain</strong> coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code> with:\n        - <code>coefficients_dct</code> : ndarray of complex\n            Transform-domain coefficient history.\n        - <code>power_vector_last</code> : ndarray of float\n            Final per-bin power estimate <code>p[k]</code>.\n        - <code>dct_matrix</code> : ndarray of float\n            The DCT matrix <code>T</code> used (shape <code>(M+1, M+1)</code>).</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.TDomainDFT", "modulename": "pydaptivefiltering", "qualname": "TDomainDFT", "kind": "class", "doc": "<p>Transform-Domain LMS using a unitary DFT (complex-valued).</p>\n\n<p>Transform-domain LMS algorithm (Diniz, Alg. 4.4) in which the time-domain\nregressor is mapped to the frequency domain using a <em>unitary</em> Discrete\nFourier Transform (DFT). Adaptation is performed in the transform domain\nwith per-bin normalization based on a smoothed power estimate. The time-domain\ncoefficient vector is recovered via the inverse unitary DFT.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\n    The DFT size is <code>N = M + 1</code>.\ngamma : float\n    Regularization factor <code>gamma</code> used in the per-bin normalization\n    denominator to avoid division by zero (or near-zero power).\nalpha : float\n    Smoothing factor <code>alpha</code> for the transform-bin power estimate,\n    typically close to 1.\ninitial_power : float\n    Initial power estimate used to initialize all transform bins.\nstep_size : float, optional\n    Adaptation step size <code>mu</code>. Default is 1e-2.\nw_init : array_like of complex, optional\n    Initial time-domain coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>.\n    If None, initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>At iteration <code>k</code>, form the time-domain regressor vector (newest sample first):</p>\n\n<p>$$x_k = [x[k], x[k-1], \\ldots, x[k-M]]^T \\in \\mathbb{C}^{N}.$$</p>\n\n<p>Define the <em>unitary</em> DFT (energy-preserving) transform-domain regressor:</p>\n\n<p>$$z_k = \\frac{\\mathrm{DFT}(x_k)}{\\sqrt{N}}.$$</p>\n\n<p>Adaptation is performed in the transform domain with weights <code>w_z[k]</code>.\nThe a priori output and error are</p>\n\n<p>$$y[k] = w_z^H[k] z_k, \\qquad e[k] = d[k] - y[k].$$</p>\n\n<p>A smoothed per-bin power estimate <code>p[k]</code> is updated as</p>\n\n<p>$$p[k] = \\alpha\\,|z_k|^2 + (1-\\alpha)\\,p[k-1],$$</p>\n\n<p>where <code>|z_k|^2</code> is taken element-wise.</p>\n\n<p>The normalized transform-domain LMS update used here is</p>\n\n<p>$$w_z[k+1] = w_z[k] + \\mu\\, e^*[k] \\, \\frac{z_k}{\\gamma + p[k]},$$</p>\n\n<p>with element-wise division.</p>\n\n<p>The time-domain coefficients are recovered via the inverse unitary DFT:</p>\n\n<p>$$w[k] = \\mathrm{IDFT}(w_z[k])\\,\\sqrt{N}.$$</p>\n\n<p>Implementation details\n    - <code>OptimizationResult.coefficients</code> stores the <strong>time-domain</strong> coefficient\n      history recorded by the base class (<code>self.w</code> after inverse transform).\n    - If <code>return_internal_states=True</code>, the transform-domain coefficient history\n      is returned in <code>result.extra[\"coefficients_dft\"]</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.TDomainDFT.__init__", "modulename": "pydaptivefiltering", "qualname": "TDomainDFT.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">initial_power</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.TDomainDFT.supports_complex", "modulename": "pydaptivefiltering", "qualname": "TDomainDFT.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.TDomainDFT.gamma", "modulename": "pydaptivefiltering", "qualname": "TDomainDFT.gamma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDFT.alpha", "modulename": "pydaptivefiltering", "qualname": "TDomainDFT.alpha", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDFT.step_size", "modulename": "pydaptivefiltering", "qualname": "TDomainDFT.step_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDFT.N", "modulename": "pydaptivefiltering", "qualname": "TDomainDFT.N", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDFT.w_dft", "modulename": "pydaptivefiltering", "qualname": "TDomainDFT.w_dft", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDFT.power_vector", "modulename": "pydaptivefiltering", "qualname": "TDomainDFT.power_vector", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.TDomainDFT.optimize", "modulename": "pydaptivefiltering", "qualname": "TDomainDFT.optimize", "kind": "function", "doc": "<p>Executes the Transform-Domain LMS (DFT) adaptation loop.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes transform-domain internal states in <code>result.extra</code>:\n    <code>\"coefficients_dft\"</code>, <code>\"power_vector_last\"</code>, and <code>\"sqrtN\"</code>.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori output sequence, <code>y[k] = w_z^H[k] z_k</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of complex\n        <strong>Time-domain</strong> coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code> with:\n        - <code>coefficients_dft</code> : ndarray of complex\n            Transform-domain coefficient history.\n        - <code>power_vector_last</code> : ndarray of float\n            Final per-bin power estimate <code>p[k]</code>.\n        - <code>sqrtN</code> : float\n            The unitary normalization factor <code>\\sqrt{N}</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.RLS", "modulename": "pydaptivefiltering", "qualname": "RLS", "kind": "class", "doc": "<p>Recursive Least Squares (RLS) adaptive filter (complex-valued).</p>\n\n<p>Exponentially-weighted least-squares adaptive FIR filter following\nDiniz (Alg. 5.3). The algorithm updates the coefficient vector using a\nKalman-gain-like direction and updates an inverse correlation matrix via\nthe matrix inversion lemma.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\ndelta : float\n    Positive initialization factor for the inverse correlation matrix:\n    <code>S_d(0) = (1/delta) I</code>.\nforgetting_factor : float\n    Forgetting factor <code>lambda</code> with <code>0 &lt; lambda &lt;= 1</code>.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.\nsafe_eps : float, optional\n    Small positive constant used to guard denominators. Default is 1e-12.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>At iteration <code>k</code>, form the regressor vector (tapped delay line):</p>\n\n<ul>\n<li><code>x_k = [x[k], x[k-1], ..., x[k-M]]^T  \u2208 \ud835\udd6e^{M+1}</code></li>\n</ul>\n\n<p>The a priori output and error are:</p>\n\n<p>$$y[k] = w^H[k] x_k, \\qquad e[k] = d[k] - y[k].$$</p>\n\n<p>Let <code>S_d[k-1] \u2208 \ud835\udd6e^{(M+1)\\times(M+1)}</code> denote the inverse correlation\nestimate. Define the intermediate vector:</p>\n\n<p>$$\\psi[k] = S_d[k-1] x_k.$$</p>\n\n<p>The gain denominator and gain vector are:</p>\n\n<p>$$\\Delta[k] = \\lambda + x_k^H \\psi[k]\n           = \\lambda + x_k^H S_d[k-1] x_k,$$</p>\n\n<p>$$g[k] = \\frac{\\psi[k]}{\\Delta[k]}.$$</p>\n\n<p>The coefficient update is:</p>\n\n<p>$$w[k+1] = w[k] + e^*[k] \\, g[k],$$</p>\n\n<p>and the inverse correlation update is:</p>\n\n<p>$$S_d[k] = \\frac{1}{\\lambda}\\Bigl(S_d[k-1] - g[k] \\psi^H[k]\\Bigr).$$</p>\n\n<p>A posteriori quantities\n    If <code>return_internal_states=True</code>, this implementation also computes the\n    a posteriori output/error using the updated weights:</p>\n\n<pre><code>$$y^{post}[k] = w^H[k+1] x_k, \\qquad e^{post}[k] = d[k] - y^{post}[k].$$\n</code></pre>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.RLS.__init__", "modulename": "pydaptivefiltering", "qualname": "RLS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">forgetting_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span>)</span>"}, {"fullname": "pydaptivefiltering.RLS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "RLS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.RLS.forgetting_factor", "modulename": "pydaptivefiltering", "qualname": "RLS.forgetting_factor", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.RLS.delta", "modulename": "pydaptivefiltering", "qualname": "RLS.delta", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.RLS.S_d", "modulename": "pydaptivefiltering", "qualname": "RLS.S_d", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.RLS.optimize", "modulename": "pydaptivefiltering", "qualname": "RLS.optimize", "kind": "function", "doc": "<p>Executes the RLS adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes a posteriori sequences and final internal states in\n    <code>result.extra</code> (see below).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        A priori output sequence, <code>y[k] = w^H[k] x_k</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        A priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code> with:\n        - <code>outputs_posteriori</code> : ndarray of complex\n            A posteriori output sequence, <code>y^{post}[k] = w^H[k+1] x_k</code>.\n        - <code>errors_posteriori</code> : ndarray of complex\n            A posteriori error sequence, <code>e^{post}[k] = d[k] - y^{post}[k]</code>.\n        - <code>S_d_last</code> : ndarray of complex\n            Final inverse correlation matrix <code>S_d</code>.\n        - <code>gain_last</code> : ndarray of complex\n            Last gain vector <code>g[k]</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.RLSAlt", "modulename": "pydaptivefiltering", "qualname": "RLSAlt", "kind": "class", "doc": "<p>Alternative RLS (RLS-Alt) adaptive filter (complex-valued).</p>\n\n<p>Alternative RLS algorithm based on Diniz (Alg. 5.4), designed to reduce\nthe computational burden of the standard RLS recursion by introducing an\nauxiliary vector <code>psi[k]</code>. The method maintains an estimate of the inverse\ninput correlation matrix and updates the coefficients using a Kalman-gain-like\nvector.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\ndelta : float\n    Positive initialization factor for the inverse correlation matrix:\n    <code>S_d(0) = (1/delta) I</code>.\nforgetting_factor : float\n    Forgetting factor <code>lambda</code> with <code>0 &lt; lambda &lt;= 1</code>.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.\nsafe_eps : float, optional\n    Small positive constant used to guard denominators. Default is 1e-12.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>At iteration <code>k</code>, form the regressor vector (tapped delay line):</p>\n\n<ul>\n<li><code>x_k = [x[k], x[k-1], ..., x[k-M]]^T  \u2208 \ud835\udd6e^{M+1}</code></li>\n</ul>\n\n<p>The a priori output and error are:</p>\n\n<p>$$y[k] = w^H[k] x_k, \\qquad e[k] = d[k] - y[k].$$</p>\n\n<p>The key auxiliary vector is:</p>\n\n<p>$$\\psi[k] = S_d[k-1] x_k,$$</p>\n\n<p>where <code>S_d[k-1]</code> is the inverse correlation estimate.</p>\n\n<p>Define the gain denominator:</p>\n\n<p>$$\\Delta[k] = \\lambda + x_k^H \\psi[k]\n           = \\lambda + x_k^H S_d[k-1] x_k,$$</p>\n\n<p>and the gain vector:</p>\n\n<p>$$g[k] = \\frac{\\psi[k]}{\\Delta[k]}.$$</p>\n\n<p>The coefficient update is:</p>\n\n<p>$$w[k+1] = w[k] + e^*[k] \\, g[k],$$</p>\n\n<p>and the inverse correlation update is:</p>\n\n<p>$$S_d[k] = \\frac{1}{\\lambda}\\Bigl(S_d[k-1] - g[k] \\psi^H[k]\\Bigr).$$</p>\n\n<p>A posteriori quantities\n    If requested, this implementation also computes the <em>a posteriori</em>\n    output/error using the updated weights:</p>\n\n<pre><code>$$y^{post}[k] = w^H[k+1] x_k, \\qquad e^{post}[k] = d[k] - y^{post}[k].$$\n</code></pre>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.RLSAlt.__init__", "modulename": "pydaptivefiltering", "qualname": "RLSAlt.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">forgetting_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span>)</span>"}, {"fullname": "pydaptivefiltering.RLSAlt.supports_complex", "modulename": "pydaptivefiltering", "qualname": "RLSAlt.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.RLSAlt.forgetting_factor", "modulename": "pydaptivefiltering", "qualname": "RLSAlt.forgetting_factor", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.RLSAlt.delta", "modulename": "pydaptivefiltering", "qualname": "RLSAlt.delta", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.RLSAlt.S_d", "modulename": "pydaptivefiltering", "qualname": "RLSAlt.S_d", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.RLSAlt.optimize", "modulename": "pydaptivefiltering", "qualname": "RLSAlt.optimize", "kind": "function", "doc": "<p>Executes the RLS-Alt adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes a posteriori sequences and the last internal states\n    in <code>result.extra</code> (see below).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        A priori output sequence, <code>y[k] = w^H[k] x_k</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        A priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code> with:\n        - <code>outputs_posteriori</code> : ndarray of complex\n            A posteriori output sequence, <code>y^{post}[k] = w^H[k+1] x_k</code>.\n        - <code>errors_posteriori</code> : ndarray of complex\n            A posteriori error sequence, <code>e^{post}[k] = d[k] - y^{post}[k]</code>.\n        - <code>S_d_last</code> : ndarray of complex\n            Final inverse correlation matrix <code>S_d</code>.\n        - <code>gain_last</code> : ndarray of complex\n            Last gain vector <code>g[k]</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.SMNLMS", "modulename": "pydaptivefiltering", "qualname": "SMNLMS", "kind": "class", "doc": "<p>Set-Membership Normalized LMS (SM-NLMS) adaptive filter (complex-valued).</p>\n\n<p>Implements Algorithm 6.1 (Diniz). The coefficients are updated <strong>only</strong> when\nthe magnitude of the a priori error exceeds a prescribed bound <code>gamma_bar</code>\n(set-membership criterion). When an update occurs, a normalized LMS-like\nstep is applied with an effective step factor that depends on <code>|e[k]|</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code> (number of coefficients is <code>M + 1</code>).\ngamma_bar : float\n    Set-membership bound <code>\\bar{\\gamma}</code> for the a priori error magnitude.\n    An update occurs only if <code>|e[k]| &gt; gamma_bar</code>.\ngamma : float\n    Regularization constant used in the NLMS denominator\n    <code>gamma + ||x_k||^2</code> to improve numerical stability.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code>, shape <code>(M + 1,)</code>. If None, zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Let the tapped-delay regressor be</p>\n\n<p>$$x_k = [x[k], x[k-1], \\dots, x[k-M]]^T \\in \\mathbb{C}^{M+1}.$$</p>\n\n<p>The a priori output and error are</p>\n\n<p>$$y[k] = w^H[k] x_k, \\qquad e[k] = d[k] - y[k].$$</p>\n\n<p>Set-membership condition\n    If <code>|e[k]| \\le \\bar{\\gamma}</code>, no update is performed.</p>\n\n<pre><code>If ``|e[k]| &gt; \\bar{\\gamma}``, define the SM step factor\n\n$$\\mu[k] = 1 - \\frac{\\bar{\\gamma}}{|e[k]|} \\in (0,1).$$\n</code></pre>\n\n<p>Normalized update (as implemented)\n    With <code>\\mathrm{den}[k] = \\gamma + \\|x_k\\|^2</code>, the coefficient update is</p>\n\n<pre><code>$$w[k+1] = w[k] + \\frac{\\mu[k]}{\\mathrm{den}[k]} \\, e^*[k] \\, x_k.$$\n</code></pre>\n\n<p>Returned error type\n    This implementation reports the <strong>a priori</strong> sequences (computed before\n    updating <code>w</code>), so <code>error_type=\"a_priori\"</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.SMNLMS.__init__", "modulename": "pydaptivefiltering", "qualname": "SMNLMS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gamma_bar</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.SMNLMS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "SMNLMS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.SMNLMS.gamma_bar", "modulename": "pydaptivefiltering", "qualname": "SMNLMS.gamma_bar", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.SMNLMS.gamma", "modulename": "pydaptivefiltering", "qualname": "SMNLMS.gamma", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.SMNLMS.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "SMNLMS.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SMNLMS.n_updates", "modulename": "pydaptivefiltering", "qualname": "SMNLMS.n_updates", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SMNLMS.optimize", "modulename": "pydaptivefiltering", "qualname": "SMNLMS.optimize", "kind": "function", "doc": "<p>Executes the SM-NLMS adaptation over paired sequences <code>x[k]</code> and <code>d[k]</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (flattened internally).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (flattened internally).\nverbose : bool, optional\n    If True, prints runtime and update statistics after completion.\nreturn_internal_states : bool, optional\n    If True, includes internal trajectories in <code>result.extra</code>:\n    <code>mu</code> and <code>den</code> (each length <code>N</code>). Entries are zero when no update occurs.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        A priori output sequence <code>y[k] = w^H[k] x_k</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        A priori error sequence <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict\n        Always present with:\n        - <code>\"n_updates\"</code> : int\n            Number of coefficient updates (iterations where <code>|e[k]| &gt; gamma_bar</code>).\n        - <code>\"update_mask\"</code> : ndarray of bool, shape <code>(N,)</code>\n            Boolean mask indicating which iterations performed updates.\n        Additionally present only if <code>return_internal_states=True</code>:\n        - <code>\"mu\"</code> : ndarray of float, shape <code>(N,)</code>\n            Step factor <code>mu[k]</code> (0 when no update).\n        - <code>\"den\"</code> : ndarray of float, shape <code>(N,)</code>\n            Denominator <code>gamma + ||x_k||^2</code> (0 when no update).</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.SMBNLMS", "modulename": "pydaptivefiltering", "qualname": "SMBNLMS", "kind": "class", "doc": "<p>Set-Membership Binormalized LMS (SM-BNLMS) adaptive filter (complex-valued).</p>\n\n<p>Implements Algorithm 6.5 (Diniz). This method can be viewed as a particular\nset-membership affine-projection (SM-AP) case with projection order <code>L = 1</code>,\ni.e., it reuses the current and previous regressors to build a low-cost\ntwo-vector update.</p>\n\n<p>The filter updates <strong>only</strong> when the magnitude of the a priori error exceeds\na prescribed bound <code>gamma_bar</code> (set-membership criterion).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code> (number of coefficients is <code>M + 1</code>).\ngamma_bar : float\n    Set-membership bound <code>\\bar{\\gamma}</code> for the a priori error magnitude.\n    An update occurs only if <code>|e[k]| &gt; gamma_bar</code>.\ngamma : float\n    Regularization factor used in the binormalized denominator. It must be\n    positive (or at least nonnegative) to improve numerical robustness.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code>, shape <code>(M + 1,)</code>. If None, zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Let the tapped-delay regressor be</p>\n\n<p>$$x_k = [x[k], x[k-1], \\dots, x[k-M]]^T \\in \\mathbb{C}^{M+1}$$</p>\n\n<p>and the previous regressor be <code>x_{k-1}</code> (as stored by the implementation).\nThe a priori output and error are</p>\n\n<p>$$y[k] = w^H[k] x_k, \\qquad e[k] = d[k] - y[k].$$</p>\n\n<p>Set-membership condition\n    If <code>|e[k]| \\le \\bar{\\gamma}</code>, no update is performed.</p>\n\n<pre><code>If ``|e[k]| &gt; \\bar{\\gamma}``, define the SM step factor\n\n$$\\mu[k] = 1 - \\frac{\\bar{\\gamma}}{|e[k]|} \\in (0,1).$$\n</code></pre>\n\n<p>Binormalized denominator\n    Define</p>\n\n<pre><code>$$a = \\|x_k\\|^2, \\quad b = \\|x_{k-1}\\|^2, \\quad c = x_{k-1}^H x_k,$$\n\nand\n\n$$\\mathrm{den}[k] = \\gamma + a b - |c|^2.$$\n\n(The code enforces a small positive floor if ``den`` becomes nonpositive.)\n</code></pre>\n\n<p>Update (as implemented)\n    The update uses two complex scalars <code>\\lambda_1</code> and <code>\\lambda_2</code>:</p>\n\n<pre><code>$$\\lambda_1[k] = \\frac{\\mu[k]\\, e[k] \\, \\|x_{k-1}\\|^2}{\\mathrm{den}[k]}, \\qquad\n</code></pre>\n\n<p>\\lambda_2[k] = -\\frac{\\mu[k]\\, e[k] \\, c^*}{\\mathrm{den}[k]}.$$</p>\n\n<pre><code>Then the coefficients are updated by\n\n$$w[k+1] = w[k] + \\lambda_1^*[k] x_k + \\lambda_2^*[k] x_{k-1}.$$\n</code></pre>\n\n<p>Returned error type\n    This implementation reports the <strong>a priori</strong> sequences (computed before\n    updating <code>w</code>), so <code>error_type=\"a_priori\"</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.SMBNLMS.__init__", "modulename": "pydaptivefiltering", "qualname": "SMBNLMS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gamma_bar</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.SMBNLMS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "SMBNLMS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.SMBNLMS.gamma_bar", "modulename": "pydaptivefiltering", "qualname": "SMBNLMS.gamma_bar", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.SMBNLMS.gamma", "modulename": "pydaptivefiltering", "qualname": "SMBNLMS.gamma", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.SMBNLMS.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "SMBNLMS.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SMBNLMS.regressor_prev", "modulename": "pydaptivefiltering", "qualname": "SMBNLMS.regressor_prev", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.SMBNLMS.n_updates", "modulename": "pydaptivefiltering", "qualname": "SMBNLMS.n_updates", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SMBNLMS.optimize", "modulename": "pydaptivefiltering", "qualname": "SMBNLMS.optimize", "kind": "function", "doc": "<p>Executes the SM-BNLMS adaptation over paired sequences <code>x[k]</code> and <code>d[k]</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (flattened internally).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (flattened internally).\nverbose : bool, optional\n    If True, prints runtime and update count after completion.\nreturn_internal_states : bool, optional\n    If True, includes internal trajectories in <code>result.extra</code>:\n    <code>mu</code>, <code>den</code>, <code>lambda1</code>, <code>lambda2</code> (each length <code>N</code>). Entries\n    are zero when no update occurs.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        A priori output sequence <code>y[k] = w^H[k] x_k</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        A priori error sequence <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict\n        Always present with:\n        - <code>\"n_updates\"</code> : int\n            Number of coefficient updates (iterations where <code>|e[k]| &gt; gamma_bar</code>).\n        - <code>\"update_mask\"</code> : ndarray of bool, shape <code>(N,)</code>\n            Boolean mask indicating which iterations performed updates.\n        Additionally present only if <code>return_internal_states=True</code>:\n        - <code>\"mu\"</code> : ndarray of float, shape <code>(N,)</code>\n            Step factor <code>mu[k]</code> (0 when no update).\n        - <code>\"den\"</code> : ndarray of float, shape <code>(N,)</code>\n            Denominator used in <code>lambda1/lambda2</code> (0 when no update).\n        - <code>\"lambda1\"</code> : ndarray of complex, shape <code>(N,)</code>\n            <code>lambda1[k]</code> (0 when no update).\n        - <code>\"lambda2\"</code> : ndarray of complex, shape <code>(N,)</code>\n            <code>lambda2[k]</code> (0 when no update).</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.SMAffineProjection", "modulename": "pydaptivefiltering", "qualname": "SMAffineProjection", "kind": "class", "doc": "<p>Set-Membership Affine-Projection (SM-AP) adaptive filter (complex-valued).</p>\n\n<p>Supervised affine-projection algorithm with <em>set-membership</em> updating,\nfollowing Diniz (Alg. 6.2). Coefficients are updated <strong>only</strong> when the\nmagnitude of the most-recent a priori error exceeds a prescribed bound\n<code>gamma_bar</code>. When an update occurs, the algorithm enforces a target\na posteriori error vector (provided by <code>gamma_bar_vector</code>).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\ngamma_bar : float\n    Set-membership bound for the (most recent) a priori error magnitude.\n    An update is performed only if <code>|e[k]| &gt; gamma_bar</code>.\ngamma_bar_vector : array_like of complex\n    Target a posteriori error vector with shape <code>(L + 1,)</code> (stored\n    internally as a column vector). This is algorithm-dependent and\n    corresponds to the desired post-update constraint in Alg. 6.2.\ngamma : float\n    Regularization factor <code>gamma</code> used in the affine-projection normal\n    equations to improve numerical stability.\nL : int\n    Data reuse factor (projection order). The affine-projection block size is\n    <code>P = L + 1</code>.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>At iteration <code>k</code>, form the regressor block matrix:</p>\n\n<ul>\n<li><code>X(k) \u2208 C^{(M+1) x (L+1)}</code>, whose columns are the most recent regressor\nvectors (newest in column 0).</li>\n</ul>\n\n<p>The affine-projection output vector is:</p>\n\n<p>$$y_{ap}(k) = X^H(k) w(k) \\in \\mathbb{C}^{L+1}.$$</p>\n\n<p>Let the stacked desired vector be:</p>\n\n<p>$$d_{ap}(k) \\in \\mathbb{C}^{L+1},$$</p>\n\n<p>with newest sample at index 0. The a priori error vector is:</p>\n\n<p>$$e_{ap}(k) = d_{ap}(k) - y_{ap}(k).$$</p>\n\n<p>This implementation uses the <em>most recent</em> scalar component as the reported\noutput and error:</p>\n\n<p>$$y[k] = y_{ap}(k)[0], \\qquad e[k] = e_{ap}(k)[0].$$</p>\n\n<p>Set-membership update rule\n    Update <strong>only if</strong>:</p>\n\n<pre><code>$$|e[k]| &gt; \\bar{\\gamma}.$$\n\nWhen updating, solve the regularized system:\n\n$$(X^H(k)X(k) + \\gamma I_{L+1})\\, s(k) =\n</code></pre>\n\n<p>\\bigl(e_{ap}(k) - \\bar{\\gamma}_{vec}^*(k)\\bigr),$$</p>\n\n<pre><code>and update the coefficients as:\n\n$$w(k+1) = w(k) + X(k)\\, s(k).$$\n\nHere ``\\bar{\\gamma}_{vec}`` is provided by ``gamma_bar_vector`` (stored\nas a column vector); complex conjugation is applied to match the internal\nconjugate-domain formulation used in the implementation.\n</code></pre>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.SMAffineProjection.__init__", "modulename": "pydaptivefiltering", "qualname": "SMAffineProjection.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gamma_bar</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">gamma_bar_vector</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">L</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.SMAffineProjection.supports_complex", "modulename": "pydaptivefiltering", "qualname": "SMAffineProjection.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.SMAffineProjection.gamma_bar", "modulename": "pydaptivefiltering", "qualname": "SMAffineProjection.gamma_bar", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.SMAffineProjection.gamma_bar_vector", "modulename": "pydaptivefiltering", "qualname": "SMAffineProjection.gamma_bar_vector", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.SMAffineProjection.gamma", "modulename": "pydaptivefiltering", "qualname": "SMAffineProjection.gamma", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.SMAffineProjection.L", "modulename": "pydaptivefiltering", "qualname": "SMAffineProjection.L", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SMAffineProjection.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "SMAffineProjection.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SMAffineProjection.regressor_matrix", "modulename": "pydaptivefiltering", "qualname": "SMAffineProjection.regressor_matrix", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.SMAffineProjection.n_updates", "modulename": "pydaptivefiltering", "qualname": "SMAffineProjection.n_updates", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SMAffineProjection.optimize", "modulename": "pydaptivefiltering", "qualname": "SMAffineProjection.optimize", "kind": "function", "doc": "<p>Executes the SM-AP adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints total runtime and update count after completion.\nreturn_internal_states : bool, optional\n    If True, includes the full a priori AP error-vector trajectory in\n    <code>result.extra</code> as <code>\"errors_vector\"</code> with shape <code>(N, L + 1)</code>.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori output sequence, <code>y[k] = y_{ap}(k)[0]</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = e_{ap}(k)[0]</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict\n        Always present with:\n        - <code>\"n_updates\"</code> : int\n            Number of coefficient updates (iterations where <code>|e[k]| &gt; gamma_bar</code>).\n        - <code>\"update_mask\"</code> : ndarray of bool, shape <code>(N,)</code>\n            Boolean mask indicating which iterations performed updates.\n        Additionally present only if <code>return_internal_states=True</code>:\n        - <code>\"errors_vector\"</code> : ndarray of complex, shape <code>(N, L + 1)</code>\n            Full affine-projection a priori error vectors over time.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.SimplifiedSMPUAP", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMPUAP", "kind": "class", "doc": "<p>Implements the Simplified Set-membership Partial-Update Affine-Projection (SM-Simp-PUAP)\nalgorithm for complex-valued data. (Algorithm 6.6, Diniz)</p>\n\n<h2 id=\"note\">Note</h2>\n\n<p>The original implementation warns that this algorithm is under development and may be unstable\nfor complex-valued simulations.</p>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.SimplifiedSMPUAP.__init__", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMPUAP.__init__", "kind": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order:\n    FIR filter order (number of taps - 1). Number of coefficients is filter_order + 1.\ngamma_bar:\n    Error magnitude threshold for triggering updates.\ngamma:\n    Regularization factor for the AP correlation matrix.\nL:\n    Reuse data factor / constraint length (projection order).\nup_selector:\n    Partial-update selector matrix with shape (M+1, N), entries in {0,1}.\n    Each column selects which coefficients are updated at iteration k.\nw_init:\n    Optional initial coefficient vector. If None, initializes to zeros.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gamma_bar</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">L</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">up_selector</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.SimplifiedSMPUAP.supports_complex", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMPUAP.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.SimplifiedSMPUAP.gamma_bar", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMPUAP.gamma_bar", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.SimplifiedSMPUAP.gamma", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMPUAP.gamma", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.SimplifiedSMPUAP.L", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMPUAP.L", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SimplifiedSMPUAP.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMPUAP.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SimplifiedSMPUAP.up_selector", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMPUAP.up_selector", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.SimplifiedSMPUAP.regressor_matrix", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMPUAP.regressor_matrix", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.SimplifiedSMPUAP.X_matrix", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMPUAP.X_matrix", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.SimplifiedSMPUAP.n_updates", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMPUAP.n_updates", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SimplifiedSMPUAP.optimize", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMPUAP.optimize", "kind": "function", "doc": "<p>Executes the SM-Simp-PUAP adaptation.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal:\n    Input signal x[k].\ndesired_signal:\n    Desired signal d[k].\nverbose:\n    If True, prints runtime and update count.\nreturn_internal_states:\n    If True, includes internal trajectories in result.extra.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    outputs:\n        A-priori output y[k] (first component of AP output vector).\n    errors:\n        A-priori error e[k] (first component of AP error vector).\n    coefficients:\n        History of coefficients stored in the base class.\n    error_type:\n        \"a_priori\".</p>\n\n<h2 id=\"extra-always\">Extra (always)</h2>\n\n<p>extra[\"n_updates\"]:\n    Number of coefficient updates (iterations where |e(k)| &gt; gamma_bar).\nextra[\"update_mask\"]:\n    Boolean array marking which iterations performed updates.</p>\n\n<h2 id=\"extra-when-return_internal_statestrue\">Extra (when return_internal_states=True)</h2>\n\n<p>extra[\"mu\"]:\n    Trajectory of mu[k] (0 when no update).\nextra[\"selected_count\"]:\n    Number of selected coefficients each iteration.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.SimplifiedSMAP", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMAP", "kind": "class", "doc": "<p>Simplified Set-Membership Affine Projection (SM-Simp-AP) adaptive filter\n(complex-valued).</p>\n\n<p>Implements Algorithm 6.3 (Diniz). This is a simplified affine-projection\nset-membership scheme where an AP-style regressor matrix of length <code>L+1</code>\nis maintained, but <strong>the update uses only the most recent column</strong> (the\ncurrent regressor vector). Updates occur only when the a priori error\nmagnitude exceeds <code>gamma_bar</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    FIR filter order <code>M</code> (number of coefficients is <code>M + 1</code>).\ngamma_bar : float\n    Set-membership bound <code>\\bar{\\gamma}</code> for the a priori error magnitude.\n    An update occurs only if <code>|e[k]| &gt; gamma_bar</code>.\ngamma : float\n    Regularization constant used in the normalization denominator\n    <code>gamma + ||x_k||^2</code>.\nL : int\n    Reuse data factor / constraint length. In this simplified variant it\n    mainly determines the number of columns kept in the internal AP-style\n    regressor matrix (size <code>(M+1) x (L+1)</code>); only the first column is used\n    in the update.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code>, shape <code>(M + 1,)</code>. If None, zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Regressor definition\n    The current tapped-delay regressor is</p>\n\n<pre><code>$$x_k = [x[k], x[k-1], \\dots, x[k-M]]^T \\in \\mathbb{C}^{M+1}.$$\n\nInternally, the algorithm maintains an AP regressor matrix\n\n$$X_k = [x_k, x_{k-1}, \\dots, x_{k-L}] \\in \\mathbb{C}^{(M+1)\\times(L+1)},$$\n\nbut the update uses only the first column ``x_k``.\n</code></pre>\n\n<p>A priori output and error (as implemented)\n    This implementation computes</p>\n\n<pre><code>$$y[k] = x_k^H w[k],$$\n\nand stores it as ``outputs[k]``.\nThe stored error is\n\n$$e[k] = d^*[k] - y[k].$$\n\n(This matches the semantics of your code; many texts use\n``e[k] = d[k] - w^H x_k``. If you want the textbook convention, you\u2019d\nremove the conjugation on ``d[k]`` and ensure ``y[k]=w^H x_k``.)\n</code></pre>\n\n<p>Set-membership condition\n    If <code>|e[k]| \\le \\bar{\\gamma}</code>, no update is performed.</p>\n\n<pre><code>If ``|e[k]| &gt; \\bar{\\gamma}``, define the scalar step factor\n\n$$s[k] = \\left(1 - \\frac{\\bar{\\gamma}}{|e[k]|}\\right) e[k].$$\n</code></pre>\n\n<p>Normalized update (simplified AP)\n    With <code>\\mathrm{den}[k] = \\gamma + \\|x_k\\|^2</code>, the coefficient update is</p>\n\n<pre><code>$$w[k+1] = w[k] + \\frac{s[k]}{\\mathrm{den}[k]} \\, x_k.$$\n</code></pre>\n\n<p>Returned error type\n    The returned sequences correspond to <strong>a priori</strong> quantities (computed\n    before updating <code>w</code>), so <code>error_type=\"a_priori\"</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.SimplifiedSMAP.__init__", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMAP.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gamma_bar</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">L</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.SimplifiedSMAP.supports_complex", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMAP.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.SimplifiedSMAP.gamma_bar", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMAP.gamma_bar", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.SimplifiedSMAP.gamma", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMAP.gamma", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.SimplifiedSMAP.L", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMAP.L", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SimplifiedSMAP.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMAP.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SimplifiedSMAP.regressor_matrix", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMAP.regressor_matrix", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.SimplifiedSMAP.X_matrix", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMAP.X_matrix", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.SimplifiedSMAP.n_updates", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMAP.n_updates", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SimplifiedSMAP.optimize", "modulename": "pydaptivefiltering", "qualname": "SimplifiedSMAP.optimize", "kind": "function", "doc": "<p>Executes the SM-Simp-AP adaptation.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code>, shape <code>(N,)</code> (flattened internally).\ndesired_signal : array_like of complex\n    Desired sequence <code>d[k]</code>, shape <code>(N,)</code> (flattened internally).\nverbose : bool, optional\n    If True, prints runtime and update statistics after completion.\nreturn_internal_states : bool, optional\n    If True, includes internal trajectories in <code>result.extra</code>:\n    <code>step_factor</code> and <code>den</code> (each length <code>N</code>). Entries are zero\n    when no update occurs.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        A priori output sequence.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        A priori error sequence (as in code: <code>e[k] = conj(d[k]) - y[k]</code>).\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict\n        Always present with:\n        - <code>\"n_updates\"</code> : int\n            Number of coefficient updates (iterations where <code>|e[k]| &gt; gamma_bar</code>).\n        - <code>\"update_mask\"</code> : ndarray of bool, shape <code>(N,)</code>\n            Boolean mask indicating which iterations performed updates.\n        Additionally present only if <code>return_internal_states=True</code>:\n        - <code>\"step_factor\"</code> : ndarray of complex, shape <code>(N,)</code>\n            Scalar factor <code>(1 - gamma_bar/|e|) * e</code> (0 when no update).\n        - <code>\"den\"</code> : ndarray of float, shape <code>(N,)</code>\n            Denominator <code>gamma + ||x_k||^2</code> (0 when no update).</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.LRLSPosteriori", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori", "kind": "class", "doc": "<p>Lattice RLS using a posteriori errors (LRLS, a posteriori form), complex-valued.</p>\n\n<p>Implements Diniz (Algorithm 7.1) in a lattice/ladder structure:</p>\n\n<p>1) <strong>Lattice prediction stage</strong> (order <code>M</code>):\n   Updates forward/backward a posteriori prediction errors and energy terms\n   using exponentially weighted recursions.</p>\n\n<p>2) <strong>Ladder (joint-process) stage</strong> (length <code>M+1</code>):\n   Updates the ladder coefficients <code>v</code> and produces the <strong>a posteriori</strong>\n   output error by progressively \"whitening\" the desired sample through the\n   backward-error vector.</p>\n\n<h2 id=\"library-conventions\">Library conventions</h2>\n\n<ul>\n<li>Complex-valued implementation (<code>supports_complex=True</code>).</li>\n<li>Ladder coefficients are stored in <code>self.v</code> with length <code>M+1</code>.</li>\n<li>For compatibility with <code>~pydaptivefiltering.base.AdaptiveFilter</code>,\n<code>self.w</code> mirrors <code>self.v</code> at each iteration and the base-class history\ncorresponds to the ladder coefficient trajectory.</li>\n</ul>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Lattice order <code>M</code> (number of sections). The ladder has <code>M+1</code> coefficients.\nlambda_factor : float, optional\n    Forgetting factor <code>lambda</code> used in the exponentially weighted recursions.\n    Default is 0.99.\nepsilon : float, optional\n    Initialization/regularization constant for the energy variables\n    (forward/backward). Default is 0.1.\nw_init : array_like of complex, optional\n    Optional initial ladder coefficients of length <code>M+1</code>. If None, initializes\n    with zeros.\ndenom_floor : float, optional\n    Small positive floor used to avoid division by (near) zero in normalization\n    terms (<code>gamma</code> variables and energy denominators). Default is 1e-12.\nxi_floor : float, optional\n    Floor applied to energy variables to keep them positive. If None, defaults\n    to <code>epsilon</code>.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Signals and dimensions\n<s>~</s><s>~</s><s>~</s><s>~</s>~~\nFor lattice order <code>M</code>:</p>\n\n<ul>\n<li><code>delta</code> has shape <code>(M,)</code> (lattice delta state)</li>\n<li><code>xi_f</code> and <code>xi_b</code> have shape <code>(M+1,)</code> (forward/backward energies)</li>\n<li><code>error_b_prev</code> and the per-sample <code>curr_err_b</code> have shape <code>(M+1,)</code>\n(backward-error vectors)</li>\n<li><code>v</code> and <code>delta_v</code> have shape <code>(M+1,)</code> (ladder state and coefficients)</li>\n</ul>\n\n<p>A posteriori error (as returned)\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~~\nThe ladder stage starts with <code>e_post = d[k]</code> and updates it as:</p>\n\n<p>$$e_{post}(k) \\leftarrow e_{post}(k) - v_m^*(k)\\, b_m(k),$$</p>\n\n<p>where \\( b_m(k) \\) are the components of the backward-error vector.\nThe final <code>e_post</code> is the <strong>a posteriori error</strong> returned in <code>errors[k]</code>,\nwhile the output estimate is returned as <code>outputs[k] = d[k] - e_post</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.__init__", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.__init__", "kind": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order:\n    Number of lattice sections M. Ladder has M+1 coefficients.\nlambda_factor:\n    Forgetting factor \u03bb.\nepsilon:\n    Energy initialization / regularization.\nw_init:\n    Optional initial ladder coefficient vector (length M+1). If None, zeros.\ndenom_floor:\n    Floor used to avoid division by (near) zero in normalization terms.\nxi_floor:\n    Floor used to keep energies positive (defaults to epsilon).</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">lambda_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.99</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">denom_floor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span>,</span><span class=\"param\">\t<span class=\"n\">xi_floor</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.supports_complex", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.lam", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.lam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.epsilon", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.epsilon", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.n_sections", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.n_sections", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.delta", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.delta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.xi_f", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.xi_f", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.xi_b", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.xi_b", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.error_b_prev", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.error_b_prev", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.delta_v", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.delta_v", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.w", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.w_history", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPosteriori.optimize", "modulename": "pydaptivefiltering", "qualname": "LRLSPosteriori.optimize", "kind": "function", "doc": "<p>Executes LRLS adaptation (a posteriori form) over paired sequences <code>x[k]</code> and <code>d[k]</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code>.\ndesired_signal : array_like of complex\n    Desired/reference sequence <code>d[k]</code> with shape <code>(N,)</code>.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, returns selected <em>final</em> internal states in <code>result.extra</code>\n    (not full trajectories).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Estimated output sequence. In this implementation:\n        <code>outputs[k] = d[k] - e_post[k]</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        A posteriori error produced by the ladder stage (final <code>e_post</code>).\n    - coefficients : ndarray\n        Ladder coefficient history (mirrors <code>self.v</code> via <code>self.w</code>).\n    - error_type : str\n        Set to <code>\"a_posteriori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code> (see below).</p>\n\n<h2 id=\"extra-when-return_internal_statestrue\">Extra (when return_internal_states=True)</h2>\n\n<p>xi_f : ndarray of float, shape <code>(M+1,)</code>\n    Final forward energies.\nxi_b : ndarray of float, shape <code>(M+1,)</code>\n    Final backward energies.\ndelta : ndarray of complex, shape <code>(M,)</code>\n    Final lattice delta state.\ndelta_v : ndarray of complex, shape <code>(M+1,)</code>\n    Final ladder delta state used to compute <code>v</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback", "kind": "class", "doc": "<p>Lattice RLS with a posteriori errors and Error Feedback (LRLS-EF), complex-valued.</p>\n\n<p>Implements the lattice/ladder RLS structure with error feedback described in\nDiniz (Algorithm 7.5). The method decomposes the adaptation into:</p>\n\n<p>1) <strong>Lattice prediction stage</strong>:\n   Updates forward/backward a posteriori prediction errors and associated\n   reflection-like variables via exponentially weighted energies.</p>\n\n<p>2) <strong>Ladder (joint-process) stage</strong>:\n   Estimates the ladder coefficients that map the lattice backward-error\n   vector into the desired response.</p>\n\n<p>In this implementation, the ladder coefficient vector is stored in <code>self.v</code>\n(length <code>M+1</code>). For compatibility with <code>~pydaptivefiltering.base.AdaptiveFilter</code>,\n<code>self.w</code> mirrors <code>self.v</code> at each iteration and the coefficient history\nrecorded by the base class corresponds to the ladder coefficients.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Lattice order <code>M</code> (number of sections). The ladder has <code>M+1</code> coefficients.\nlambda_factor : float, optional\n    Forgetting factor <code>lambda</code> used in the exponentially weighted recursions.\n    Default is 0.99.\nepsilon : float, optional\n    Positive initialization/regularization constant for forward and backward\n    energies. Default is 0.1.\nw_init : array_like of complex, optional\n    Optional initial ladder coefficients of length <code>M+1</code>. If None, initializes\n    with zeros.\nsafe_eps : float, optional\n    Small positive floor used to avoid division by (near) zero and to keep the\n    internal likelihood variables bounded. Default is 1e-12.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Signals and dimensions\n<s>~</s><s>~</s><s>~</s><s>~</s>~~\nThis class operates on complex-valued sequences. For lattice order <code>M</code>:</p>\n\n<ul>\n<li><code>delta</code> and <code>delta_v</code> have shape <code>(M+1,)</code></li>\n<li><code>xi_f</code> and <code>xi_b</code> have shape <code>(M+2,)</code> (energies per section plus guard)</li>\n<li><code>error_b_prev</code> has shape <code>(M+2,)</code> and stores the previous backward-error\nvector used for the error-feedback recursion.</li>\n<li>At each time k, the ladder regressor is the backward-error vector\n<code>curr_b[:M+1]</code>.</li>\n</ul>\n\n<p>Output computation\n<s>~</s><s>~</s><s>~</s>~~~\nThe estimated output is formed as a ladder combination:</p>\n\n<p>$$y(k) = \\mathbf{v}^H(k)\\, \\mathbf{b}(k),$$</p>\n\n<p>where \\( \\mathbf{b}(k) \\) corresponds to <code>curr_b[:M+1]</code> and\n\\( \\mathbf{v}(k) \\) is the ladder coefficient vector <code>self.v</code>.\nThe reported error is the output error \\( e(k)=d(k)-y(k) \\).</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.__init__", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.__init__", "kind": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order:\n    Lattice order M (number of sections). Ladder has M+1 coefficients.\nlambda_factor:\n    Forgetting factor \u03bb.\nepsilon:\n    Regularization/initialization constant for energies.\nw_init:\n    Optional initial ladder coefficients (length M+1). If None, zeros.\nsafe_eps:\n    Small positive floor used to avoid division by (near) zero.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">lambda_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.99</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span>)</span>"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.supports_complex", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.lam", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.lam", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.epsilon", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.epsilon", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.n_sections", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.n_sections", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.safe_eps", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.safe_eps", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.delta", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.delta", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.xi_f", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.xi_f", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.xi_b", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.xi_b", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.error_b_prev", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.error_b_prev", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.v", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.v", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.delta_v", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.delta_v", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.w", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.w_history", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSErrorFeedback.optimize", "modulename": "pydaptivefiltering", "qualname": "LRLSErrorFeedback.optimize", "kind": "function", "doc": "<p>Executes LRLS-EF adaptation for paired sequences <code>x[k]</code> and <code>d[k]</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code>.\ndesired_signal : array_like of complex\n    Desired/reference sequence <code>d[k]</code> with shape <code>(N,)</code>.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, returns selected <em>final</em> internal states in <code>result.extra</code>\n    (not full trajectories).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Estimated output sequence <code>y[k]</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        Output error sequence <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray\n        Ladder coefficient history (mirrors <code>self.v</code> via <code>self.w</code>).\n    - error_type : str\n        Set to <code>\"output_error\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code> (see below).</p>\n\n<h2 id=\"extra-when-return_internal_statestrue\">Extra (when return_internal_states=True)</h2>\n\n<p>xi_f : ndarray of float, shape <code>(M+2,)</code>\n    Final forward prediction-error energies.\nxi_b : ndarray of float, shape <code>(M+2,)</code>\n    Final backward prediction-error energies.\ndelta : ndarray of complex, shape <code>(M+1,)</code>\n    Final lattice delta (reflection-like) state.\ndelta_v : ndarray of complex, shape <code>(M+1,)</code>\n    Final ladder delta state used to compute <code>v</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.LRLSPriori", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori", "kind": "class", "doc": "<p>Lattice RLS using a priori errors (LRLS, a priori form), complex-valued.</p>\n\n<p>Implements Diniz (Algorithm 7.4) in a lattice/ladder structure:</p>\n\n<p>1) <strong>Lattice prediction stage</strong> (order <code>M</code>):\n   Produces forward a priori errors and a vector of backward errors, updating\n   reflection-like state variables and exponentially weighted energies.</p>\n\n<p>2) <strong>Ladder (joint-process) stage</strong> (length <code>M+1</code>):\n   Updates the ladder coefficients <code>v</code> using the a priori backward-error\n   vector and produces an <strong>a priori</strong> error associated with the desired signal.</p>\n\n<h2 id=\"library-conventions\">Library conventions</h2>\n\n<ul>\n<li>Complex-valued implementation (<code>supports_complex=True</code>).</li>\n<li>Ladder coefficients are stored in <code>self.v</code> with length <code>M+1</code>.</li>\n<li>For compatibility with <code>~pydaptivefiltering.base.AdaptiveFilter</code>,\n<code>self.w</code> mirrors <code>self.v</code> at each iteration and the base-class history\ncorresponds to the ladder coefficient trajectory.</li>\n</ul>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Lattice order <code>M</code> (number of sections). The ladder has <code>M+1</code> coefficients.\nlambda_factor : float, optional\n    Forgetting factor <code>lambda</code> used in the exponentially weighted recursions.\n    Default is 0.99.\nepsilon : float, optional\n    Initialization/regularization constant for the energy variables\n    (forward/backward). Default is 0.1.\nw_init : array_like of complex, optional\n    Optional initial ladder coefficients of length <code>M+1</code>. If None, initializes\n    with zeros.\ndenom_floor : float, optional\n    Small positive floor used to avoid division by (near) zero in normalization\n    terms (<code>gamma</code> variables and energy denominators). Default is 1e-12.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Signals and dimensions\n<s>~</s><s>~</s><s>~</s><s>~</s>~~\nFor lattice order <code>M</code>:</p>\n\n<ul>\n<li><code>delta</code> has shape <code>(M,)</code> (lattice delta state)</li>\n<li><code>xi_f</code> and <code>xi_b</code> have shape <code>(M+1,)</code> (forward/backward energies)</li>\n<li><code>error_b_prev</code> and per-sample <code>alpha_b</code> have shape <code>(M+1,)</code>\n(backward-error vectors)</li>\n<li><code>v</code> and <code>delta_v</code> have shape <code>(M+1,)</code> (ladder coefficients and state)</li>\n</ul>\n\n<p>A priori error (as returned)\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~~\nThe ladder stage starts with <code>alpha_e = d[k]</code> and removes components\ncorrelated with the backward-error vector:</p>\n\n<p>$$\\alpha_e \\leftarrow \\alpha_e - v_m^*(k)\\, b_m(k),$$</p>\n\n<p>where \\( b_m(k) \\) are the backward errors (<code>alpha_b[m]</code>). The final\nvalue is then scaled by the final lattice normalization factor <code>gamma</code>:</p>\n\n<p>$$e_{pri}(k) = \\gamma(k)\\, \\alpha_e(k).$$</p>\n\n<p>This scaled error is returned in <code>errors[k]</code>, and the output estimate is\nreturned as <code>outputs[k] = d[k] - e_pri[k]</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.LRLSPriori.__init__", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.__init__", "kind": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order:\n    Number of lattice sections M. Ladder has M+1 coefficients.\nlambda_factor:\n    Forgetting factor \u03bb.\nepsilon:\n    Energy initialization / regularization.\nw_init:\n    Optional initial ladder coefficient vector (length M+1). If None, zeros.\ndenom_floor:\n    Floor used to avoid division by (near) zero in normalization terms.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">lambda_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.99</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">denom_floor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span>)</span>"}, {"fullname": "pydaptivefiltering.LRLSPriori.supports_complex", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.LRLSPriori.lam", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.lam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPriori.epsilon", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.epsilon", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPriori.n_sections", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.n_sections", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPriori.delta", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.delta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPriori.xi_f", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.xi_f", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPriori.xi_b", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.xi_b", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPriori.error_b_prev", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.error_b_prev", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPriori.delta_v", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.delta_v", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPriori.w", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPriori.w_history", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.LRLSPriori.optimize", "modulename": "pydaptivefiltering", "qualname": "LRLSPriori.optimize", "kind": "function", "doc": "<p>Executes LRLS adaptation (a priori form) over paired sequences <code>x[k]</code> and <code>d[k]</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code>.\ndesired_signal : array_like of complex\n    Desired/reference sequence <code>d[k]</code> with shape <code>(N,)</code>.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, returns selected <em>final</em> internal states in <code>result.extra</code>\n    (not full trajectories).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Estimated output sequence. In this implementation:\n        <code>outputs[k] = d[k] - e_pri[k]</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        A priori ladder error scaled by the final lattice normalization\n        factor: <code>e_pri[k] = gamma[k] * alpha_e[k]</code>.\n    - coefficients : ndarray\n        Ladder coefficient history (mirrors <code>self.v</code> via <code>self.w</code>).\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code> (see below).</p>\n\n<h2 id=\"extra-when-return_internal_statestrue\">Extra (when return_internal_states=True)</h2>\n\n<p>xi_f : ndarray of float, shape <code>(M+1,)</code>\n    Final forward energies.\nxi_b : ndarray of float, shape <code>(M+1,)</code>\n    Final backward energies.\ndelta : ndarray of complex, shape <code>(M,)</code>\n    Final lattice delta state.\ndelta_v : ndarray of complex, shape <code>(M+1,)</code>\n    Final ladder delta state used to compute <code>v</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.NormalizedLRLS", "modulename": "pydaptivefiltering", "qualname": "NormalizedLRLS", "kind": "class", "doc": "<p>Normalized Lattice RLS (NLRLS) based on a posteriori error, complex-valued.</p>\n\n<p>Implements Diniz (Algorithm 7.6). This variant introduces <em>normalized</em>\ninternal variables so that key quantities (normalized forward/backward errors\nand reflection-like coefficients) are designed to be magnitude-bounded by 1,\nimproving numerical robustness.</p>\n\n<p>The algorithm has two coupled stages:</p>\n\n<p>1) <strong>Prediction stage (lattice, order M)</strong>:\n   Computes normalized forward/backward a posteriori errors (<code>bar_f</code>, <code>bar_b</code>)\n   and updates normalized reflection-like coefficients <code>rho</code>.</p>\n\n<p>2) <strong>Estimation stage (normalized ladder, length M+1)</strong>:\n   Updates normalized coefficients <code>rho_v</code> and produces a normalized\n   a posteriori error <code>bar_e</code>. The returned error is the <em>de-normalized</em>\n   error <code>e = bar_e * xi_half</code>.</p>\n\n<h2 id=\"library-conventions\">Library conventions</h2>\n\n<ul>\n<li>Complex-valued implementation (<code>supports_complex=True</code>).</li>\n<li>The exposed coefficient vector is <code>rho_v</code> (length <code>M+1</code>).\nFor compatibility with <code>~pydaptivefiltering.base.AdaptiveFilter</code>:\n<ul>\n<li><code>self.w</code> mirrors <code>self.rho_v</code> at each iteration.</li>\n<li>history recorded by <code>_record_history()</code> corresponds to <code>rho_v</code>.</li>\n</ul></li>\n</ul>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Lattice order <code>M</code> (number of sections). The estimation stage uses\n    <code>M+1</code> coefficients.\nlambda_factor : float, optional\n    Forgetting factor <code>lambda</code> used in the exponentially weighted updates.\n    Default is 0.99.\nepsilon : float, optional\n    Small positive constant used for regularization in normalizations,\n    magnitude clipping, and denominator protection. Default is 1e-6.\nw_init : array_like of complex, optional\n    Optional initialization for <code>rho_v</code> with length <code>M+1</code>.\n    If None, initializes with zeros.\ndenom_floor : float, optional\n    Extra floor for denominators and sqrt protections. Default is 1e-12.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Normalized variables\n<s>~</s><s>~</s><s>~</s><s>~</s>\nThe implementation uses the following normalized quantities:</p>\n\n<ul>\n<li><code>xi_half</code>: square-root energy tracker (scalar). It normalizes the input/output\nso that normalized errors stay bounded.</li>\n<li><code>bar_f</code>: normalized forward error for the current section.</li>\n<li><code>bar_b_prev</code> / <code>bar_b_curr</code>: normalized backward error vectors, shape <code>(M+1,)</code>.</li>\n<li><code>bar_e</code>: normalized a posteriori error in the estimation stage.</li>\n<li><code>rho</code>: normalized reflection-like coefficients for the lattice stage, shape <code>(M,)</code>.</li>\n<li><code>rho_v</code>: normalized coefficients for the estimation stage, shape <code>(M+1,)</code>.</li>\n</ul>\n\n<p>Magnitude bounding\n<s>~</s><s>~</s><s>~</s>~~~\nSeveral variables are clipped to satisfy <code>|z| &lt;= 1</code>. The terms\n<code>sqrt(1 - |z|^2)</code> act like cosine factors in the normalized recursions and\nare safeguarded with <code>_safe_sqrt</code> to avoid negative arguments caused by\nround-off.</p>\n\n<p>Output and error returned\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>\nThe filter returns the de-normalized a posteriori error:</p>\n\n<p><code>errors[k] = bar_e[k] * xi_half[k]</code></p>\n\n<p>and the output estimate:</p>\n\n<p><code>outputs[k] = d[k] - errors[k]</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.NormalizedLRLS.__init__", "modulename": "pydaptivefiltering", "qualname": "NormalizedLRLS.__init__", "kind": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order:\n    Number of lattice sections M. The estimation stage uses M+1 coefficients.\nlambda_factor:\n    Forgetting factor \u03bb.\nepsilon:\n    Regularization used in normalizations and clipping.\nw_init:\n    Optional initialization for rho_v (length M+1). If None, zeros.\ndenom_floor:\n    Extra floor for denominators / sqrt protections.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">lambda_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.99</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">denom_floor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span>)</span>"}, {"fullname": "pydaptivefiltering.NormalizedLRLS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "NormalizedLRLS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.NormalizedLRLS.lam", "modulename": "pydaptivefiltering", "qualname": "NormalizedLRLS.lam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.NormalizedLRLS.epsilon", "modulename": "pydaptivefiltering", "qualname": "NormalizedLRLS.epsilon", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.NormalizedLRLS.n_sections", "modulename": "pydaptivefiltering", "qualname": "NormalizedLRLS.n_sections", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.NormalizedLRLS.rho", "modulename": "pydaptivefiltering", "qualname": "NormalizedLRLS.rho", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.NormalizedLRLS.bar_b_prev", "modulename": "pydaptivefiltering", "qualname": "NormalizedLRLS.bar_b_prev", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.NormalizedLRLS.xi_half", "modulename": "pydaptivefiltering", "qualname": "NormalizedLRLS.xi_half", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.NormalizedLRLS.w", "modulename": "pydaptivefiltering", "qualname": "NormalizedLRLS.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.NormalizedLRLS.w_history", "modulename": "pydaptivefiltering", "qualname": "NormalizedLRLS.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.NormalizedLRLS.optimize", "modulename": "pydaptivefiltering", "qualname": "NormalizedLRLS.optimize", "kind": "function", "doc": "<p>Run the Normalized LRLS (NLRLS) recursion over paired sequences <code>x[k]</code> and <code>d[k]</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code>.\ndesired_signal : array_like of complex\n    Desired/reference sequence <code>d[k]</code> with shape <code>(N,)</code>.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, returns selected <em>final</em> internal states in <code>result.extra</code>\n    (not full trajectories).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    outputs : ndarray of complex, shape <code>(N,)</code>\n        Estimated output sequence <code>y[k] = d[k] - e_post[k]</code>.\n    errors : ndarray of complex, shape <code>(N,)</code>\n        De-normalized a posteriori error <code>e_post[k] = bar_e[k] * xi_half[k]</code>.\n    coefficients : ndarray\n        History of <code>rho_v</code> (mirrors <code>self.rho_v</code> via <code>self.w</code>).\n    error_type : str\n        Set to <code>\"a_posteriori\"</code>.\n    extra : dict, optional\n        Present only if <code>return_internal_states=True</code> (see below).</p>\n\n<h2 id=\"extra-when-return_internal_statestrue\">Extra (when return_internal_states=True)</h2>\n\n<p>rho : ndarray of complex, shape <code>(M,)</code>\n    Final normalized lattice reflection-like coefficients.\nrho_v : ndarray of complex, shape <code>(M+1,)</code>\n    Final normalized estimation-stage coefficients.\nxi_half : float\n    Final square-root energy tracker used for normalization.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.FastRLS", "modulename": "pydaptivefiltering", "qualname": "FastRLS", "kind": "class", "doc": "<p>Fast Transversal Recursive Least-Squares (FT-RLS) algorithm (complex-valued).</p>\n\n<p>The Fast Transversal RLS (also called Fast RLS) is a computationally\nefficient alternative to standard RLS. By exploiting shift-structure in the\nregressor and using coupled forward/backward linear prediction recursions,\nit reduces the per-sample complexity from \\( O(M^2) \\) (standard RLS) to\napproximately \\( O(M) \\).</p>\n\n<p>This implementation follows Diniz (Alg. 8.1) and maintains internal state\nfor forward and backward predictors, as well as the conversion (likelihood)\nvariable \\( \\gamma(k) \\) that maps a priori to a posteriori quantities.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\nforgetting_factor : float, optional\n    Exponential forgetting factor <code>lambda</code>. Typical values are in\n    <code>[0.95, 1.0]</code>; values closer to 1 give longer memory. Default is 0.99.\nepsilon : float, optional\n    Positive initialization for the minimum prediction-error energies\n    (regularization), used as \\( \\xi_{\\min}(0) \\) in the recursions.\n    Default is 0.1.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.\nsafe_eps : float, optional (keyword-only)\n    Small constant used to guard divisions in internal recursions when\n    denominators approach zero. Default is 1e-30.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Convention\n<s>~</s><s>~</s>\nAt time <code>k</code>, the regressor is formed (most recent sample first) as:</p>\n\n<p>$$x_k = [x[k], x[k-1], \\ldots, x[k-M]]^T.$$</p>\n\n<p>A priori vs a posteriori\n<s>~</s><s>~</s><s>~</s><s>~</s>~~~~\nThe a priori output and error are:</p>\n\n<p>$$y(k) = w^H(k-1) x_k, \\qquad e(k) = d(k) - y(k).$$</p>\n\n<p>This implementation also computes the a posteriori error using the\nconversion variable \\( \\gamma(k) \\) (from the FT-RLS recursions):</p>\n\n<p>$$e_{\\text{post}}(k) = \\gamma(k)\\, e(k), \\qquad\ny_{\\text{post}}(k) = d(k) - e_{\\text{post}}(k).$$</p>\n\n<p>The main-filter coefficient update uses the normalized gain-like vector\nproduced by the transversal recursions (<code>phi_hat_n</code> in the code):</p>\n\n<p>$$w(k) = w(k-1) + \\phi(k)\\, e_{\\text{post}}^*(k),$$</p>\n\n<p>where \\( \\phi(k) \\) corresponds to the internal vector <code>phi_hat_n</code>.</p>\n\n<p>Returned internals\n<s>~</s><s>~</s><s>~</s>~~~\nThe method always returns a posteriori sequences in <code>extra</code>:\n<code>outputs_posteriori</code> and <code>errors_posteriori</code>. If\n<code>return_internal_states=True</code>, it additionally returns tracks of\n<code>gamma</code> and the forward minimum prediction-error energy <code>xi_min_f</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.FastRLS.__init__", "modulename": "pydaptivefiltering", "qualname": "FastRLS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">forgetting_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.99</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-30</span></span>)</span>"}, {"fullname": "pydaptivefiltering.FastRLS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "FastRLS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.FastRLS.forgetting_factor", "modulename": "pydaptivefiltering", "qualname": "FastRLS.forgetting_factor", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.FastRLS.epsilon", "modulename": "pydaptivefiltering", "qualname": "FastRLS.epsilon", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.FastRLS.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "FastRLS.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.FastRLS.w", "modulename": "pydaptivefiltering", "qualname": "FastRLS.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.FastRLS.optimize", "modulename": "pydaptivefiltering", "qualname": "FastRLS.optimize", "kind": "function", "doc": "<p>Executes the FT-RLS adaptation loop.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of complex\n    Desired/reference sequence <code>d[k]</code> with shape <code>(N,)</code> (will be\n    flattened). Must have the same length as <code>input_signal</code>.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes additional internal trajectories in\n    <code>result.extra</code>:\n    - <code>\"gamma\"</code>: ndarray of float, shape <code>(N,)</code> with \\( \\gamma(k) \\).\n    - <code>\"xi_min_f\"</code>: ndarray of float, shape <code>(N,)</code> with the forward\n      minimum prediction-error energy \\( \\xi_{f,\\min}(k) \\).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        A priori output sequence <code>y[k] = w^H(k-1) x_k</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        A priori error sequence <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict\n        Always includes:\n        - <code>\"outputs_posteriori\"</code>: ndarray of complex, shape <code>(N,)</code>.\n        - <code>\"errors_posteriori\"</code>: ndarray of complex, shape <code>(N,)</code>.\n        Additionally includes <code>\"gamma\"</code> and <code>\"xi_min_f\"</code> if\n        <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.StabFastRLS", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS", "kind": "class", "doc": "<p>Stabilized Fast Transversal RLS (SFT-RLS) algorithm (real-valued).</p>\n\n<p>The Stabilized Fast Transversal RLS is a numerically robust variant of the\nFast Transversal RLS. It preserves the approximately \\( O(M) \\) per-sample\ncomplexity of transversal RLS recursions while improving stability in\nfinite-precision arithmetic by introducing feedback stabilization in the\nbackward prediction recursion (via <code>kappa1</code>, <code>kappa2</code>, <code>kappa3</code>) and by\nguarding divisions/energies through floors and optional clipping.</p>\n\n<p>This implementation corresponds to Diniz (Alg. 8.2) and is restricted to\n<strong>real-valued</strong> input/desired sequences (enforced by <code>ensure_real_signals</code>).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\nforgetting_factor : float, optional\n    Exponential forgetting factor <code>lambda</code>. Default is 0.99.\nepsilon : float, optional\n    Positive initialization for the minimum prediction-error energies\n    (regularization), used as \\( \\xi_{\\min}(0) \\) in the recursions.\n    Default is 1e-1.\nkappa1, kappa2, kappa3 : float, optional\n    Stabilization constants used to form stabilized versions of the backward\n    prediction error. Defaults are 1.5, 2.5, and 1.0.\nw_init : array_like of float, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.\ndenom_floor : float, optional\n    Safety floor used to clamp denominators before inversion to prevent\n    overflow/underflow and non-finite values during internal recursions.\n    If None, a small value based on machine <code>tiny</code> is used.\nxi_floor : float, optional\n    Safety floor for prediction error energies (e.g., <code>xi_min_f</code>,\n    <code>xi_min_b</code>). If None, a small value based on machine <code>tiny</code> is used.\ngamma_clip : float, optional\n    Optional clipping threshold applied to an intermediate conversion factor\n    to avoid extreme values (singularities). If None, no clipping is applied.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Convention\n<s>~</s><s>~</s>\nAt time <code>k</code>, the internal regressor window has length <code>M + 2</code> (denoted\n<code>r</code> in the code) and is formed in reverse order (most recent sample first).\nThe main adaptive filter uses the first <code>M + 1</code> entries of this window.</p>\n\n<p>A priori vs a posteriori\n<s>~</s><s>~</s><s>~</s><s>~</s>~~~~\nThe a priori output and error are:</p>\n\n<p>$$y(k) = w^T(k-1) x_k, \\qquad e(k) = d(k) - y(k),$$</p>\n\n<p>and the a posteriori error returned by this implementation is:</p>\n\n<p>$$e_{\\text{post}}(k) = \\gamma(k)\\, e(k),$$</p>\n\n<p>where \\( \\gamma(k) \\) is produced by the stabilized transversal recursions.</p>\n\n<p>Stabilization with kappa\n<s>~</s><s>~</s><s>~</s><s>~</s>~~~~\nThe algorithm forms stabilized backward-error combinations (three variants)\nfrom two backward-error lines in the recursion (named <code>e_b_line1</code> and\n<code>e_b_line2</code> in the code). Conceptually:</p>\n\n<p>$$e_{b,i}(k) = \\kappa_i\\, e_{b,2}(k) + (1-\\kappa_i)\\, e_{b,1}(k),$$</p>\n\n<p>for \\( \\kappa_i \\in {\\kappa_1, \\kappa_2, \\kappa_3} \\).</p>\n\n<p>Numerical safeguards\n<s>~</s><s>~</s><s>~</s><s>~</s>\nSeveral denominators are clamped to <code>denom_floor</code> before inversion and\nminimum energies are floored by <code>xi_floor</code>. The counts of clamp events are\ntracked and returned in <code>extra[\"clamp_stats\"]</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<p>Implementation*, 5th ed., Algorithm 8.2.</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.StabFastRLS.__init__", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">forgetting_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.99</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">kappa1</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span>,</span><span class=\"param\">\t<span class=\"n\">kappa2</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">2.5</span>,</span><span class=\"param\">\t<span class=\"n\">kappa3</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">denom_floor</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">xi_floor</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">gamma_clip</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.StabFastRLS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.StabFastRLS.lambda_", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.lambda_", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.StabFastRLS.epsilon", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.epsilon", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.StabFastRLS.kappa1", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.kappa1", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.StabFastRLS.kappa2", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.kappa2", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.StabFastRLS.kappa3", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.kappa3", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.StabFastRLS.denom_floor", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.denom_floor", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.StabFastRLS.xi_floor", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.xi_floor", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.StabFastRLS.gamma_clip", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.gamma_clip", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[float]"}, {"fullname": "pydaptivefiltering.StabFastRLS.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.StabFastRLS.filter_order", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.filter_order", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.StabFastRLS.w", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.StabFastRLS.optimize", "modulename": "pydaptivefiltering", "qualname": "StabFastRLS.optimize", "kind": "function", "doc": "<p>Executes the stabilized FT-RLS adaptation loop (real-valued).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Real-valued input sequence <code>x[k]</code> with shape <code>(N,)</code>.\ndesired_signal : array_like of float\n    Real-valued desired/reference sequence <code>d[k]</code> with shape <code>(N,)</code>.\n    Must have the same length as <code>input_signal</code>.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes internal trajectories in <code>result.extra</code>:\n    - <code>\"xi_min_f\"</code>: ndarray of float, shape <code>(N,)</code> (forward minimum\n      prediction-error energy).\n    - <code>\"xi_min_b\"</code>: ndarray of float, shape <code>(N,)</code> (backward minimum\n      prediction-error energy).\n    - <code>\"gamma\"</code>: ndarray of float, shape <code>(N,)</code> (conversion factor).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        A priori output sequence <code>y[k]</code>.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        A priori error sequence <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict\n        Always includes:\n        - <code>\"errors_posteriori\"</code>: ndarray of float, shape <code>(N,)</code> with\n          \\( e_{\\text{post}}(k) \\).\n        - <code>\"clamp_stats\"</code>: dict with counters of denominator clamps.\n        Additionally includes <code>\"xi_min_f\"</code>, <code>\"xi_min_b\"</code>, and\n        <code>\"gamma\"</code> if <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.QRRLS", "modulename": "pydaptivefiltering", "qualname": "QRRLS", "kind": "class", "doc": "<p>QR-RLS adaptive filter using Givens rotations (real-valued).</p>\n\n<p>QR-decomposition RLS implementation based on Diniz (Alg. 9.1, 3rd ed.),\nfollowing the reference MATLAB routine <code>QR_RLS.m</code>. This variant maintains\ninternal state variables closely matching the MATLAB code and applies\nsequential real Givens rotations to a stacked system.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M+1</code>.\nlamb : float, optional\n    Forgetting factor <code>lambda</code> with <code>0 &lt; lambda &lt;= 1</code>. Default is 0.99.\nw_init : array_like of float, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M+1,)</code>. If None,\n    initializes with zeros.\ndenom_floor : float, optional\n    Small positive floor used to avoid division by (near) zero in scalar\n    denominators. Default is 1e-18.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Real-valued only\n    This implementation is restricted to real-valued signals and coefficients\n    (<code>supports_complex=False</code>). The constraint is enforced via\n    <code>@ensure_real_signals</code> on <code>optimize()</code>.</p>\n\n<p>State variables (MATLAB naming)\n    This implementation keeps the same key state variables as <code>QR_RLS.m</code>:</p>\n\n<pre><code>- ``ULineMatrix`` : ndarray, shape ``(M+1, M+1)``\n  Upper-triangular-like matrix updated by sequential Givens rotations.\n- ``dLine_q2`` : ndarray, shape ``(M+1,)``\n  Transformed desired vector accumulated through the same rotations.\n- ``gamma`` : float\n  Scalar accumulated as the product of Givens cosines in each iteration.\n</code></pre>\n\n<p>Givens-rotation structure (high level)\n    At each iteration, the algorithm applies Givens rotations to eliminate\n    components of the stacked vector <code>[regressor; ULineMatrix]</code> while\n    applying the same rotations to <code>[d_line; dLine_q2]</code>. The resulting\n    system is then solved by back-substitution to obtain the updated weights.</p>\n\n<p>Output/error conventions (MATLAB-style)\n    The returned <code>errors</code> correspond to the MATLAB <code>errorVector</code>:</p>\n\n<pre><code>$$e[k] = d_{line}[k] \\cdot \\gamma[k],$$\n\nand the reported output is computed as:\n\n$$y[k] = d[k] - e[k].$$\n\nSince this error is formed after the rotation steps (i.e., after the\nQR-update stage), the method sets ``error_type=\"a_posteriori\"``.\n</code></pre>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.QRRLS.__init__", "modulename": "pydaptivefiltering", "qualname": "QRRLS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">lamb</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.99</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">denom_floor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-18</span></span>)</span>"}, {"fullname": "pydaptivefiltering.QRRLS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "QRRLS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.QRRLS.lamb", "modulename": "pydaptivefiltering", "qualname": "QRRLS.lamb", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.QRRLS.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "QRRLS.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.QRRLS.ULineMatrix", "modulename": "pydaptivefiltering", "qualname": "QRRLS.ULineMatrix", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.QRRLS.dLine_q2", "modulename": "pydaptivefiltering", "qualname": "QRRLS.dLine_q2", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.QRRLS.w", "modulename": "pydaptivefiltering", "qualname": "QRRLS.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.QRRLS.w_history", "modulename": "pydaptivefiltering", "qualname": "QRRLS.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.QRRLS.optimize", "modulename": "pydaptivefiltering", "qualname": "QRRLS.optimize", "kind": "function", "doc": "<p>Executes the QR-RLS adaptation loop (MATLAB-style recursion).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Real-valued input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of float\n    Real-valued desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes the last internal states in <code>result.extra</code>:\n    <code>\"ULineMatrix_last\"</code>, <code>\"dLine_q2_last\"</code>, <code>\"gamma_last\"</code>,\n    and <code>\"d_line_last\"</code>.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        Scalar output sequence as computed by the MATLAB-style routine:\n        <code>y[k] = d[k] - e[k]</code>.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        MATLAB-style a posteriori error quantity:\n        <code>e[k] = d_line[k] * gamma[k]</code>.\n    - coefficients : ndarray of float\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_posteriori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code> with:\n        - <code>ULineMatrix_last</code> : ndarray\n            Final <code>ULineMatrix</code>.\n        - <code>dLine_q2_last</code> : ndarray\n            Final <code>dLine_q2</code>.\n        - <code>gamma_last</code> : float\n            <code>gamma</code> at the last iteration.\n        - <code>d_line_last</code> : float\n            <code>d_line</code> at the last iteration.\n        - <code>forgetting_factor</code> : float\n            The forgetting factor <code>lambda</code> used.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.ErrorEquation", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation", "kind": "class", "doc": "<p>Equation-Error RLS for adaptive IIR filtering (real-valued).</p>\n\n<p>The equation-error approach avoids the non-convexity of direct IIR\noutput-error minimization by adapting the coefficients using an auxiliary\n(linear-in-parameters) error in which past outputs in the feedback path are\nreplaced by past desired samples. This yields a quadratic (RLS-suitable)\ncriterion while still producing a \"true IIR\" output for evaluation.</p>\n\n<p>This implementation follows Diniz (3rd ed., Alg. 10.3) and is restricted to\n<strong>real-valued</strong> signals (enforced by <code>ensure_real_signals</code>).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>zeros_order : int\n    Numerator order <code>N</code> (number of zeros). The feedforward part has\n    <code>N + 1</code> coefficients.\npoles_order : int\n    Denominator order <code>M</code> (number of poles). The feedback part has <code>M</code>\n    coefficients.\nforgetting_factor : float, optional\n    Exponential forgetting factor <code>lambda</code>. Default is 0.99.\nepsilon : float, optional\n    Positive initialization for the inverse correlation matrix used by RLS.\n    Internally, the inverse covariance is initialized as:</p>\n\n<pre><code>$$S(0) = \\frac{1}{\\epsilon} I.$$\n\nDefault is 1e-3.\n</code></pre>\n\n<p>w_init : array_like of float, optional\n    Optional initial coefficient vector. If provided, it should have shape\n    <code>(M + N + 1,)</code> following the parameter order described below. If None,\n    the implementation initializes with zeros (and ignores <code>w_init</code>).</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Parameterization (as implemented)\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~~\nThe coefficient vector is arranged as:</p>\n\n<ul>\n<li><code>w[:M]</code>: feedback (pole) coefficients (denoted <code>a</code> in literature)</li>\n<li><code>w[M:]</code>: feedforward (zero) coefficients (denoted <code>b</code>)</li>\n</ul>\n\n<p>Regressors and two outputs\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~\nAt time <code>k</code>, define <code>reg_x = [x(k), x(k-1), ..., x(k-N)]^T</code>.\nThe algorithm forms two regressors:</p>\n\n<ul>\n<li><p>Output regressor (uses past <em>true outputs</em>):</p>\n\n<p>$$\\varphi_y(k) = [y(k-1), \\ldots, y(k-M),\\; x(k), \\ldots, x(k-N)]^T.$$</p></li>\n<li><p>Equation regressor (uses past <em>desired samples</em>):</p>\n\n<p>$$\\varphi_e(k) = [d(k-1), \\ldots, d(k-M),\\; x(k), \\ldots, x(k-N)]^T.$$</p></li>\n</ul>\n\n<p>The reported output is the \"true IIR\" output computed with the output\nregressor:</p>\n\n<p>$$y(k) = w^T(k)\\, \\varphi_y(k),$$</p>\n\n<p>while the auxiliary \"equation\" output is:</p>\n\n<p>$$y_{eq}(k) = w^T(k)\\, \\varphi_e(k).$$</p>\n\n<p>The adaptation is driven by the <em>equation error</em>:</p>\n\n<p>$$e_{eq}(k) = d(k) - y_{eq}(k),$$</p>\n\n<p>whereas the \"output error\" used for evaluation is:</p>\n\n<p>$$e(k) = d(k) - y(k).$$</p>\n\n<p>Stability procedure\n<s>~</s><s>~</s><s>~</s>~~~~\nAfter each update, the feedback coefficients <code>w[:M]</code> are stabilized by\nreflecting any poles outside the unit circle back inside (pole reflection).</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.ErrorEquation.__init__", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">zeros_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">poles_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">forgetting_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.99</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.ErrorEquation.supports_complex", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.ErrorEquation.zeros_order", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation.zeros_order", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.ErrorEquation.poles_order", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation.poles_order", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.ErrorEquation.forgetting_factor", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation.forgetting_factor", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.ErrorEquation.epsilon", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation.epsilon", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.ErrorEquation.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.ErrorEquation.Sd", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation.Sd", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.ErrorEquation.y_buffer", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation.y_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.ErrorEquation.d_buffer", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation.d_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.ErrorEquation.w", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.ErrorEquation.optimize", "modulename": "pydaptivefiltering", "qualname": "ErrorEquation.optimize", "kind": "function", "doc": "<p>Executes the equation-error RLS adaptation loop.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Real-valued input sequence <code>x[k]</code> with shape <code>(N,)</code>.\ndesired_signal : array_like of float\n    Real-valued desired/reference sequence <code>d[k]</code> with shape <code>(N,)</code>.\n    Must have the same length as <code>input_signal</code>.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes the time history of the feedback (pole)\n    coefficients in <code>result.extra[\"a_coefficients\"]</code> with shape\n    <code>(N, poles_order)</code> (or None if <code>poles_order == 0</code>).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        \"True IIR\" output sequence <code>y[k]</code> computed with past outputs.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Output error sequence <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"equation_error\"</code>.\n    - extra : dict\n        Always includes:\n        - <code>\"auxiliary_errors\"</code>: ndarray of float, shape <code>(N,)</code> with\n          the equation error <code>e_eq[k] = d[k] - y_eq[k]</code> used to drive\n          the RLS update.\n        Additionally includes <code>\"a_coefficients\"</code> if\n        <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.GaussNewton", "modulename": "pydaptivefiltering", "qualname": "GaussNewton", "kind": "class", "doc": "<p>Gauss-Newton (recursive) output-error adaptation for IIR filters (real-valued).</p>\n\n<p>This method targets the output-error (OE) criterion for IIR adaptive filtering,\ni.e., it adapts coefficients to reduce the squared error\n\\( e(k) = d(k) - y(k) \\) where \\( y(k) \\) is produced by the <em>recursive</em>\n(IIR) structure.</p>\n\n<p>The Gauss-Newton idea is to approximate the Hessian of the OE cost by an\nouter-product model based on a sensitivity (Jacobian-like) vector\n\\( \\phi(k) \\). In this implementation, the associated inverse matrix\n(named <code>Sd</code>) is updated recursively in an RLS-like fashion with an\nexponential smoothing factor <code>alpha</code>. This yields faster convergence than\nplain gradient descent at the cost of roughly \\( O((M+N)^2) \\) operations\nper sample.</p>\n\n<p>This is a modified version of Diniz (3rd ed., Alg. 10.1). The implementation\nis restricted to <strong>real-valued</strong> signals (enforced by <code>ensure_real_signals</code>).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>zeros_order : int\n    Numerator order <code>N</code> (number of zeros). The feedforward part has\n    <code>N + 1</code> coefficients.\npoles_order : int\n    Denominator order <code>M</code> (number of poles). The feedback part has <code>M</code>\n    coefficients.\nalpha : float, optional\n    Smoothing factor used in the recursive update of the inverse Hessian-like\n    matrix <code>Sd</code>. Must satisfy <code>0 &lt; alpha &lt; 1</code>. Smaller values yield\n    slower adaptation of <code>Sd</code> (more memory). Default is 0.05.\nstep_size : float, optional\n    Step size applied to the Gauss-Newton direction. Default is 1.0.\ndelta : float, optional\n    Positive regularization parameter for initializing <code>Sd</code> as\n    \\( S(0) = \\delta^{-1} I \\). Default is 1e-3.\nw_init : array_like of float, optional\n    Optional initial coefficient vector. If provided, it should have shape\n    <code>(M + N + 1,)</code> following the parameter order described below. If None,\n    the implementation initializes with zeros (and ignores <code>w_init</code>).</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Parameterization (as implemented)\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~~\nThe coefficient vector is arranged as:</p>\n\n<ul>\n<li><code>w[:M]</code>: feedback (pole) coefficients (often denoted <code>a</code>)</li>\n<li><code>w[M:]</code>: feedforward (zero) coefficients (often denoted <code>b</code>)</li>\n</ul>\n\n<p>Regressor and OE error (as implemented)\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~~~\nWith <code>reg_x = [x(k), x(k-1), ..., x(k-N)]^T</code> and an internal buffer of the\nlast <code>M</code> outputs, the code forms:</p>\n\n<p>$$\\varphi(k) = [y(k-1), \\ldots, y(k-M),\\; x(k), \\ldots, x(k-N)]^T,$$</p>\n\n<p>computes:</p>\n\n<p>$$y(k) = w^T(k)\\, \\varphi(k), \\qquad e(k) = d(k) - y(k),$$</p>\n\n<p>and uses <code>e(k)</code> as the output-error signal reported in <code>errors</code>.</p>\n\n<p>Sensitivity vector and Gauss-Newton recursion\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>\nThe update direction is built from filtered sensitivity signals stored in\ninternal buffers (<code>x_line_buffer</code> and <code>y_line_buffer</code>). The code forms:</p>\n\n<p>$$\\phi(k) =\n[\\underline{y}(k-1), \\ldots, \\underline{y}(k-M),\\;\n -\\underline{x}(k), \\ldots, -\\underline{x}(k-N)]^T.$$</p>\n\n<p>Given <code>psi = Sd * phi</code> and the scalar denominator</p>\n\n<p>$$\\text{den}(k) = \\frac{1-\\alpha}{\\alpha} + \\phi^T(k)\\, Sd(k-1)\\, \\phi(k),$$</p>\n\n<p>the inverse Hessian-like matrix is updated as:</p>\n\n<p>$$Sd(k) = \\frac{1}{1-\\alpha}\\left(Sd(k-1) - \\frac{\\psi(k)\\psi^T(k)}{\\text{den}(k)}\\right),$$</p>\n\n<p>and the coefficient update is:</p>\n\n<p>$$w(k+1) = w(k) - \\mu\\, Sd(k)\\, \\phi(k)\\, e(k).$$</p>\n\n<p>Stability procedure\n<s>~</s><s>~</s><s>~</s>~~~~\nAfter each update, the feedback coefficients <code>w[:M]</code> are stabilized by\nreflecting poles outside the unit circle back inside (pole reflection).</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.GaussNewton.__init__", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">zeros_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">poles_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.05</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.GaussNewton.supports_complex", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.GaussNewton.zeros_order", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.zeros_order", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.GaussNewton.poles_order", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.poles_order", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.GaussNewton.alpha", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.alpha", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.GaussNewton.step_size", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.GaussNewton.delta", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.delta", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.GaussNewton.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.GaussNewton.Sd", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.Sd", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.GaussNewton.y_buffer", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.y_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.GaussNewton.x_line_buffer", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.x_line_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.GaussNewton.y_line_buffer", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.y_line_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.GaussNewton.w", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.GaussNewton.optimize", "modulename": "pydaptivefiltering", "qualname": "GaussNewton.optimize", "kind": "function", "doc": "<p>Executes the (recursive) Gauss-Newton OE adaptation loop.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Real-valued input sequence <code>x[k]</code> with shape <code>(N,)</code>.\ndesired_signal : array_like of float\n    Real-valued desired/reference sequence <code>d[k]</code> with shape <code>(N,)</code>.\n    Must have the same length as <code>input_signal</code>.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes sensitivity trajectories in <code>result.extra</code>:\n    - <code>\"x_sensitivity\"</code>: ndarray of float, shape <code>(N,)</code> with the\n      scalar sensitivity signal \\( \\underline{x}(k) \\).\n    - <code>\"y_sensitivity\"</code>: ndarray of float, shape <code>(N,)</code> with the\n      scalar sensitivity signal \\( \\underline{y}(k) \\).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        Output sequence <code>y[k]</code> produced by the current IIR structure.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Output error sequence <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"output_error\"</code>.\n    - extra : dict\n        Empty unless <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.GaussNewtonGradient", "modulename": "pydaptivefiltering", "qualname": "GaussNewtonGradient", "kind": "class", "doc": "<p>Gradient-based Gauss-Newton (output-error) adaptation for IIR filters (real-valued).</p>\n\n<p>This method targets the output-error (OE) criterion for IIR adaptive filtering,\ni.e., it adapts the coefficients to minimize the squared error\n\\( e(k) = d(k) - y(k) \\) where \\( y(k) \\) is produced by the <em>recursive</em>\n(IIR) structure.</p>\n\n<p>Compared to the classical Gauss-Newton approach, this implementation uses a\nsimplified <em>gradient</em> update (no matrix inversions) while still leveraging\nfiltered sensitivity signals to approximate how the output changes with\nrespect to pole/zero coefficients.</p>\n\n<p>This is a modified version of Diniz (3rd ed., Alg. 10.1). The implementation\nis restricted to <strong>real-valued</strong> signals (enforced by <code>ensure_real_signals</code>).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>zeros_order : int\n    Numerator order <code>N</code> (number of zeros). The feedforward part has\n    <code>N + 1</code> coefficients.\npoles_order : int\n    Denominator order <code>M</code> (number of poles). The feedback part has <code>M</code>\n    coefficients.\nstep_size : float, optional\n    Adaptation step size <code>mu</code>. Default is 1e-3.\nw_init : array_like of float, optional\n    Optional initial coefficient vector. If provided, it should have shape\n    <code>(M + N + 1,)</code> following the parameter order described below. If None,\n    the implementation initializes with zeros (and ignores <code>w_init</code>).</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Parameterization (as implemented)\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~~\nThe coefficient vector is arranged as:</p>\n\n<ul>\n<li><code>w[:M]</code>: feedback (pole) coefficients (often denoted <code>a</code>)</li>\n<li><code>w[M:]</code>: feedforward (zero) coefficients (often denoted <code>b</code>)</li>\n</ul>\n\n<p>Regressor and output (as implemented)\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~\nWith <code>reg_x = [x(k), x(k-1), ..., x(k-N)]^T</code> and an internal buffer of the\nlast <code>M</code> outputs, this implementation forms:</p>\n\n<p>$$\\varphi(k) = [y(k-1), \\ldots, y(k-M),\\; x(k), \\ldots, x(k-N)]^T,$$</p>\n\n<p>and computes the (recursive) output used by the OE criterion as:</p>\n\n<p>$$y(k) = w^T(k)\\, \\varphi(k), \\qquad e(k) = d(k) - y(k).$$</p>\n\n<p>Sensitivity-based gradient factor\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~~\nThe update direction is built from filtered sensitivity signals stored in\ninternal buffers (<code>x_line_buffer</code> and <code>y_line_buffer</code>). The code forms:</p>\n\n<p>$$\\phi(k) =\n[\\underline{y}(k-1), \\ldots, \\underline{y}(k-M),\\;\n -\\underline{x}(k), \\ldots, -\\underline{x}(k-N)]^T,$$</p>\n\n<p>and applies the per-sample gradient step:</p>\n\n<p>$$w(k+1) = w(k) - \\mu\\, \\phi(k)\\, e(k).$$</p>\n\n<p>Stability procedure\n<s>~</s><s>~</s><s>~</s>~~~~\nAfter each update, the feedback coefficients <code>w[:M]</code> are stabilized by\nreflecting poles outside the unit circle back inside (pole reflection).</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.GaussNewtonGradient.__init__", "modulename": "pydaptivefiltering", "qualname": "GaussNewtonGradient.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">zeros_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">poles_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.GaussNewtonGradient.supports_complex", "modulename": "pydaptivefiltering", "qualname": "GaussNewtonGradient.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.GaussNewtonGradient.zeros_order", "modulename": "pydaptivefiltering", "qualname": "GaussNewtonGradient.zeros_order", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.GaussNewtonGradient.poles_order", "modulename": "pydaptivefiltering", "qualname": "GaussNewtonGradient.poles_order", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.GaussNewtonGradient.step_size", "modulename": "pydaptivefiltering", "qualname": "GaussNewtonGradient.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.GaussNewtonGradient.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "GaussNewtonGradient.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.GaussNewtonGradient.y_buffer", "modulename": "pydaptivefiltering", "qualname": "GaussNewtonGradient.y_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.GaussNewtonGradient.x_line_buffer", "modulename": "pydaptivefiltering", "qualname": "GaussNewtonGradient.x_line_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.GaussNewtonGradient.y_line_buffer", "modulename": "pydaptivefiltering", "qualname": "GaussNewtonGradient.y_line_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.GaussNewtonGradient.w", "modulename": "pydaptivefiltering", "qualname": "GaussNewtonGradient.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.GaussNewtonGradient.optimize", "modulename": "pydaptivefiltering", "qualname": "GaussNewtonGradient.optimize", "kind": "function", "doc": "<p>Executes the gradient-based Gauss-Newton (OE) adaptation loop.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Real-valued input sequence <code>x[k]</code> with shape <code>(N,)</code>.\ndesired_signal : array_like of float\n    Real-valued desired/reference sequence <code>d[k]</code> with shape <code>(N,)</code>.\n    Must have the same length as <code>input_signal</code>.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes sensitivity trajectories in <code>result.extra</code>:\n    - <code>\"x_sensitivity\"</code>: ndarray of float, shape <code>(N,)</code> with the\n      scalar sensitivity signal \\( \\underline{x}(k) \\) produced by\n      the recursion in the code.\n    - <code>\"y_sensitivity\"</code>: ndarray of float, shape <code>(N,)</code> with the\n      scalar sensitivity signal \\( \\underline{y}(k) \\) produced by\n      the recursion in the code.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        Output sequence <code>y[k]</code> produced by the current IIR structure.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Output error sequence <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"output_error\"</code>.\n    - extra : dict\n        Empty unless <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.RLSIIR", "modulename": "pydaptivefiltering", "qualname": "RLSIIR", "kind": "class", "doc": "<p>RLS-like output-error adaptation for IIR filters (real-valued).</p>\n\n<p>This algorithm applies an RLS-style recursion to the IIR output-error (OE)\nproblem. Rather than minimizing a linear FIR error, it uses filtered\nsensitivity signals to build a Jacobian-like vector \\( \\phi(k) \\) that\napproximates how the IIR output changes with respect to the pole/zero\nparameters. The inverse correlation matrix (named <code>Sd</code>) scales the update,\ntypically yielding faster convergence than plain gradient methods.</p>\n\n<p>The implementation corresponds to a modified form of Diniz (3rd ed.,\nAlg. 10.1) and is restricted to <strong>real-valued</strong> signals (enforced by\n<code>ensure_real_signals</code>).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>zeros_order : int\n    Numerator order <code>N</code> (number of zeros). The feedforward part has\n    <code>N + 1</code> coefficients.\npoles_order : int\n    Denominator order <code>M</code> (number of poles). The feedback part has <code>M</code>\n    coefficients.\nforgetting_factor : float, optional\n    Exponential forgetting factor <code>lambda</code> used in the recursive update of\n    <code>Sd</code>. Typical values are in <code>[0.9, 1.0]</code>. Default is 0.99.\ndelta : float, optional\n    Positive regularization parameter for initializing <code>Sd</code> as\n    \\( S(0) = \\delta^{-1} I \\). Default is 1e-3.\nw_init : array_like of float, optional\n    Optional initial coefficient vector. If provided, it should have shape\n    <code>(M + N + 1,)</code> following the parameter order described below. If None,\n    the implementation initializes with zeros (and ignores <code>w_init</code>).</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Parameterization (as implemented)\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~~\nThe coefficient vector is arranged as:</p>\n\n<ul>\n<li><code>w[:M]</code>: feedback (pole) coefficients (often denoted <code>a</code>)</li>\n<li><code>w[M:]</code>: feedforward (zero) coefficients (often denoted <code>b</code>)</li>\n</ul>\n\n<p>OE output and error (as implemented)\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~\nWith <code>reg_x = [x(k), x(k-1), ..., x(k-N)]^T</code> and an internal buffer of the\nlast <code>M</code> outputs, the code forms:</p>\n\n<p>$$\\varphi(k) = [y(k-1), \\ldots, y(k-M),\\; x(k), \\ldots, x(k-N)]^T,$$</p>\n\n<p>computes:</p>\n\n<p>$$y(k) = w^T(k)\\, \\varphi(k), \\qquad e(k) = d(k) - y(k),$$</p>\n\n<p>and reports <code>e(k)</code> as the output-error sequence.</p>\n\n<p>Sensitivity vector and RLS recursion\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~\nFiltered sensitivity signals stored in internal buffers (<code>x_line_buffer</code>\nand <code>y_line_buffer</code>) are used to build:</p>\n\n<p>$$\\phi(k) =\n[\\underline{y}(k-1), \\ldots, \\underline{y}(k-M),\\;\n -\\underline{x}(k), \\ldots, -\\underline{x}(k-N)]^T.$$</p>\n\n<p>The inverse correlation matrix <code>Sd</code> is updated in an RLS-like manner:</p>\n\n<p>$$\\psi(k) = Sd(k-1)\\, \\phi(k), \\quad\n\\text{den}(k) = \\lambda + \\phi^T(k)\\, \\psi(k),$$</p>\n\n<p>$$Sd(k) = \\frac{1}{\\lambda}\n        \\left(Sd(k-1) - \\frac{\\psi(k)\\psi^T(k)}{\\text{den}(k)}\\right).$$</p>\n\n<p>The coefficient update used here is:</p>\n\n<p>$$w(k+1) = w(k) - Sd(k)\\, \\phi(k)\\, e(k).$$</p>\n\n<p>(Note: this implementation does not expose an additional step-size parameter;\nthe effective step is governed by <code>Sd</code>.)</p>\n\n<p>Stability procedure\n<s>~</s><s>~</s><s>~</s>~~~~\nAfter each update, the feedback coefficients <code>w[:M]</code> are stabilized by\nreflecting poles outside the unit circle back inside (pole reflection).</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.RLSIIR.__init__", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">zeros_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">poles_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">forgetting_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.99</span>,</span><span class=\"param\">\t<span class=\"n\">delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.RLSIIR.supports_complex", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.RLSIIR.zeros_order", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.zeros_order", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.RLSIIR.poles_order", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.poles_order", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.RLSIIR.forgetting_factor", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.forgetting_factor", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.RLSIIR.delta", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.delta", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.RLSIIR.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.RLSIIR.Sd", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.Sd", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.RLSIIR.y_buffer", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.y_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.RLSIIR.x_line_buffer", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.x_line_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.RLSIIR.y_line_buffer", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.y_line_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.RLSIIR.w", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.RLSIIR.optimize", "modulename": "pydaptivefiltering", "qualname": "RLSIIR.optimize", "kind": "function", "doc": "<p>Executes the RLS-IIR (OE) adaptation loop.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Real-valued input sequence <code>x[k]</code> with shape <code>(N,)</code>.\ndesired_signal : array_like of float\n    Real-valued desired/reference sequence <code>d[k]</code> with shape <code>(N,)</code>.\n    Must have the same length as <code>input_signal</code>.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes sensitivity trajectories in <code>result.extra</code>:\n    - <code>\"x_sensitivity\"</code>: ndarray of float, shape <code>(N,)</code> with the\n      scalar sensitivity signal \\( \\underline{x}(k) \\).\n    - <code>\"y_sensitivity\"</code>: ndarray of float, shape <code>(N,)</code> with the\n      scalar sensitivity signal \\( \\underline{y}(k) \\).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        Output sequence <code>y[k]</code> produced by the current IIR structure.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Output error sequence <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"output_error\"</code>.\n    - extra : dict\n        Empty unless <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.SteiglitzMcBride", "modulename": "pydaptivefiltering", "qualname": "SteiglitzMcBride", "kind": "class", "doc": "<p>Steiglitz\u2013McBride (SM) adaptive algorithm for IIR filters (real-valued).</p>\n\n<p>The Steiglitz\u2013McBride method is an iterative output-error (OE) approach\nimplemented via a sequence of <em>prefiltered equation-error</em> updates. The key\nidea is to prefilter both the input <code>x[k]</code> and the desired signal <code>d[k]</code>\nby the inverse of the current denominator estimate, \\( 1/A(z) \\). This\ntransforms the OE problem into a (locally) more linear regression and often\nimproves convergence compared to directly minimizing the OE surface.</p>\n\n<p>This implementation follows the structure of Diniz (3rd ed., Alg. 10.4),\nusing per-sample prefiltering recursions and a gradient-type update driven\nby the <em>filtered equation error</em>. It is restricted to <strong>real-valued</strong>\nsignals (enforced by <code>ensure_real_signals</code>).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>zeros_order : int\n    Numerator order <code>N</code> (number of zeros). The feedforward part has\n    <code>N + 1</code> coefficients.\npoles_order : int\n    Denominator order <code>M</code> (number of poles). The feedback part has <code>M</code>\n    coefficients.\nstep_size : float, optional\n    Adaptation step size <code>mu</code> for the SM update. Default is 1e-3.\nw_init : array_like of float, optional\n    Optional initial coefficient vector. If provided, it should have shape\n    <code>(M + N + 1,)</code> following the parameter order described below. If None,\n    the implementation initializes with zeros (and ignores <code>w_init</code>).</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Parameterization (as implemented)\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~~\nThe coefficient vector is arranged as:</p>\n\n<ul>\n<li><code>w[:M]</code>: feedback (pole) coefficients (often denoted <code>a</code>)</li>\n<li><code>w[M:]</code>: feedforward (zero) coefficients (often denoted <code>b</code>)</li>\n</ul>\n\n<p>\"True\" IIR output and output error\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~~~\nWith <code>reg_x = [x(k), x(k-1), ..., x(k-N)]^T</code> and an internal buffer of the\nlast <code>M</code> outputs, the method computes a \"true IIR\" output:</p>\n\n<p>$$y(k) = w^T(k)\\, [y(k-1),\\ldots,y(k-M),\\; x(k),\\ldots,x(k-N)]^T,$$</p>\n\n<p>and the reported output error:</p>\n\n<p>$$e(k) = d(k) - y(k).$$</p>\n\n<p>Prefiltering by 1/A(z) (as implemented)\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~~~\nLet <code>a</code> be the current feedback coefficient vector. The code implements\nthe prefilter \\( 1/A(z) \\) through the recursions:</p>\n\n<p>$$x_f(k) = x(k) + a^T x_f(k-1:k-M), \\qquad\nd_f(k) = d(k) + a^T d_f(k-1:k-M),$$</p>\n\n<p>where the past filtered values are stored in <code>xf_buffer</code> and <code>df_buffer</code>.</p>\n\n<p>Filtered equation error and update\n<s>~</s><s>~</s><s>~</s><s>~</s><s>~</s><s>~</s>~~~~\nThe adaptation uses an auxiliary regressor built from the filtered signals\n(named <code>regressor_s</code> in the code). For <code>M &gt; 0</code>:</p>\n\n<p>$$\\varphi_s(k) = [d_f(k-1),\\ldots,d_f(k-M),\\; x_f(k),\\ldots,x_f(k-N)]^T,$$</p>\n\n<p>and for <code>M = 0</code> it reduces to the FIR case using only\n<code>[x_f(k),\\ldots,x_f(k-N)]</code>.</p>\n\n<p>The filtered equation error is:</p>\n\n<p>$$e_s(k) = d_f(k) - w^T(k)\\, \\varphi_s(k),$$</p>\n\n<p>and the coefficient update used here is:</p>\n\n<p>$$w(k+1) = w(k) + 2\\mu\\, \\varphi_s(k)\\, e_s(k).$$</p>\n\n<p>Stability procedure\n<s>~</s><s>~</s><s>~</s>~~~~\nAfter each update (for <code>M &gt; 0</code>), the feedback coefficients <code>w[:M]</code> are\nstabilized by reflecting poles outside the unit circle back inside (pole\nreflection). This helps keep the prefilter \\( 1/A(z) \\) stable.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.SteiglitzMcBride.__init__", "modulename": "pydaptivefiltering", "qualname": "SteiglitzMcBride.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">zeros_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">poles_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.SteiglitzMcBride.supports_complex", "modulename": "pydaptivefiltering", "qualname": "SteiglitzMcBride.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.SteiglitzMcBride.zeros_order", "modulename": "pydaptivefiltering", "qualname": "SteiglitzMcBride.zeros_order", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SteiglitzMcBride.poles_order", "modulename": "pydaptivefiltering", "qualname": "SteiglitzMcBride.poles_order", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SteiglitzMcBride.step_size", "modulename": "pydaptivefiltering", "qualname": "SteiglitzMcBride.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.SteiglitzMcBride.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "SteiglitzMcBride.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.SteiglitzMcBride.y_buffer", "modulename": "pydaptivefiltering", "qualname": "SteiglitzMcBride.y_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.SteiglitzMcBride.xf_buffer", "modulename": "pydaptivefiltering", "qualname": "SteiglitzMcBride.xf_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.SteiglitzMcBride.df_buffer", "modulename": "pydaptivefiltering", "qualname": "SteiglitzMcBride.df_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.SteiglitzMcBride.w", "modulename": "pydaptivefiltering", "qualname": "SteiglitzMcBride.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.SteiglitzMcBride.optimize", "modulename": "pydaptivefiltering", "qualname": "SteiglitzMcBride.optimize", "kind": "function", "doc": "<p>Executes the Steiglitz\u2013McBride adaptation loop.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Real-valued input sequence <code>x[k]</code> with shape <code>(N,)</code>.\ndesired_signal : array_like of float\n    Real-valued desired/reference sequence <code>d[k]</code> with shape <code>(N,)</code>.\n    Must have the same length as <code>input_signal</code>.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes the filtered equation-error trajectory in\n    <code>result.extra[\"auxiliary_error\"]</code> with shape <code>(N,)</code>.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        \"True IIR\" output sequence <code>y[k]</code>.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Output error sequence <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_posteriori\"</code> (the update is driven by the filtered\n        equation error).\n    - extra : dict\n        Empty unless <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.BilinearRLS", "modulename": "pydaptivefiltering", "qualname": "BilinearRLS", "kind": "class", "doc": "<p>Bilinear RLS adaptive filter (real-valued).</p>\n\n<p>RLS algorithm with a fixed 4-dimensional <em>bilinear</em> regressor structure,\nfollowing Diniz (Alg. 11.3). The regressor couples the current input with\npast desired samples to model a simple bilinear relationship.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>forgetting_factor : float, optional\n    Forgetting factor <code>lambda</code> with <code>0 &lt; lambda &lt;= 1</code>. Default is 0.98.\ndelta : float, optional\n    Regularization parameter used to initialize the inverse correlation\n    matrix as <code>P(0) = I/delta</code> (requires <code>delta &gt; 0</code>). Default is 1.0.\nw_init : array_like of float, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(4,)</code>. If None,\n    initializes with zeros.\nsafe_eps : float, optional\n    Small positive constant used to guard denominators. Default is 1e-12.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Real-valued only\n    This implementation is restricted to real-valued signals and coefficients\n    (<code>supports_complex=False</code>). The constraint is enforced via\n    <code>@ensure_real_signals</code> on <code>optimize()</code>.</p>\n\n<p>Bilinear regressor (as implemented)\n    This implementation uses a 4-component regressor:</p>\n\n<pre><code>$$u[k] =\n</code></pre>\n\n<p>\\begin{bmatrix}\n    x[k] \\\n    d[k-1] \\\n    x[k]d[k-1] \\\n    x[k-1]d[k-1]\n\\end{bmatrix}\n\\in \\mathbb{R}^{4}.$$</p>\n\n<pre><code>The state ``x[k-1]`` and ``d[k-1]`` are taken from the previous iteration,\nwith ``x[-1] = 0`` and ``d[-1] = 0`` at initialization.\n</code></pre>\n\n<p>RLS recursion (a priori form)\n    With</p>\n\n<pre><code>$$y[k] = w^T[k-1] u[k], \\qquad e[k] = d[k] - y[k],$$\n\nthe gain vector is\n\n$$g[k] = \\frac{P[k-1] u[k]}{\\lambda + u^T[k] P[k-1] u[k]},$$\n\nthe inverse correlation update is\n\n$$P[k] = \\frac{1}{\\lambda}\\left(P[k-1] - g[k] u^T[k] P[k-1]\\right),$$\n\nand the coefficient update is\n\n$$w[k] = w[k-1] + g[k] e[k].$$\n</code></pre>\n\n<p>Implementation details\n    - The denominator <code>lambda + u^T P u</code> is guarded by <code>safe_eps</code> to avoid\n      numerical issues when very small.\n    - Coefficient history is recorded via the base class.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.BilinearRLS.__init__", "modulename": "pydaptivefiltering", "qualname": "BilinearRLS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">forgetting_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.98</span>,</span><span class=\"param\">\t<span class=\"n\">delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span>)</span>"}, {"fullname": "pydaptivefiltering.BilinearRLS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "BilinearRLS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.BilinearRLS.lambda_factor", "modulename": "pydaptivefiltering", "qualname": "BilinearRLS.lambda_factor", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.BilinearRLS.delta", "modulename": "pydaptivefiltering", "qualname": "BilinearRLS.delta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.BilinearRLS.P", "modulename": "pydaptivefiltering", "qualname": "BilinearRLS.P", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.BilinearRLS.optimize", "modulename": "pydaptivefiltering", "qualname": "BilinearRLS.optimize", "kind": "function", "doc": "<p>Executes the bilinear RLS adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of float\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes the last internal states in <code>result.extra</code>:\n    <code>\"P_last\"</code>, <code>\"last_regressor\"</code> (<code>u[k]</code>), and <code>\"last_gain\"</code> (<code>g[k]</code>).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        Scalar a priori output sequence, <code>y[k] = w^T[k-1] u[k]</code>.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.ComplexRBF", "modulename": "pydaptivefiltering", "qualname": "ComplexRBF", "kind": "class", "doc": "<p>Complex Radial Basis Function (CRBF) network (complex-valued).</p>\n\n<p>Implements a complex-valued RBF adaptive model (Algorithm 11.6 - Diniz).\nThe model output is computed as:</p>\n\n<pre><code>f_p(u) = exp( -||u - c_p||^2 / sigma_p^2 )\ny[k]   = w^H f(u_k)\n</code></pre>\n\n<p>where:</p>\n\n<ul>\n<li>u_k is the input regressor (dimension = input_dim),</li>\n<li>c_p are complex centers (\"vet\" in the original code),</li>\n<li>sigma_p are real spreads,</li>\n<li>w are complex neuron weights.</li>\n</ul>\n\n<h2 id=\"input-handling\">Input handling</h2>\n\n<p>This implementation accepts two input formats in <code>optimize</code>:</p>\n\n<p>1) 1D input signal x[k] (shape (N,)):\n   A tapped-delay regressor u_k of length <code>input_dim</code> is formed internally.</p>\n\n<p>2) 2D regressor matrix U (shape (N, input_dim)):\n   Each row is used directly as u_k.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<ul>\n<li>Complex-valued implementation (<code>supports_complex=True</code>).</li>\n<li>The base class <code>filter_order</code> is used here as a size indicator (n_neurons-1).</li>\n<li><code>OptimizationResult.coefficients</code> stores the history of neuron weights <code>w</code>.\nCenters and spreads can be returned via <code>result.extra</code> when requested.</li>\n</ul>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.ComplexRBF.__init__", "modulename": "pydaptivefiltering", "qualname": "ComplexRBF.__init__", "kind": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>n_neurons:\n    Number of RBF neurons.\ninput_dim:\n    Dimension of the input regressor u_k.\nur:\n    Step-size for centers update.\nuw:\n    Step-size for weights update.\nus:\n    Step-size for spread (sigma) update.\nw_init:\n    Optional initial neuron weights (length n_neurons). If None, random complex.\nsigma_init:\n    Initial spread value used for all neurons (must be &gt; 0).\nrng:\n    Optional numpy random generator for reproducible initialization.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">n_neurons</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">ur</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">uw</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">us</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">sigma_init</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">rng</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">_generator</span><span class=\"o\">.</span><span class=\"n\">Generator</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.ComplexRBF.supports_complex", "modulename": "pydaptivefiltering", "qualname": "ComplexRBF.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.ComplexRBF.n_neurons", "modulename": "pydaptivefiltering", "qualname": "ComplexRBF.n_neurons", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.ComplexRBF.input_dim", "modulename": "pydaptivefiltering", "qualname": "ComplexRBF.input_dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.ComplexRBF.ur", "modulename": "pydaptivefiltering", "qualname": "ComplexRBF.ur", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.ComplexRBF.uw", "modulename": "pydaptivefiltering", "qualname": "ComplexRBF.uw", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.ComplexRBF.us", "modulename": "pydaptivefiltering", "qualname": "ComplexRBF.us", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.ComplexRBF.vet", "modulename": "pydaptivefiltering", "qualname": "ComplexRBF.vet", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.ComplexRBF.sigma", "modulename": "pydaptivefiltering", "qualname": "ComplexRBF.sigma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.ComplexRBF.w_history", "modulename": "pydaptivefiltering", "qualname": "ComplexRBF.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.ComplexRBF.optimize", "modulename": "pydaptivefiltering", "qualname": "ComplexRBF.optimize", "kind": "function", "doc": "<p>Run CRBF adaptation.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal:\n    Either:\n      - 1D signal x[k] with shape (N,), or\n      - regressor matrix U with shape (N, input_dim).\ndesired_signal:\n    Desired signal d[k], shape (N,).\nverbose:\n    If True, prints runtime.\nreturn_internal_states:\n    If True, returns final centers/spreads and last activation vector in result.extra.\nsafe_eps:\n    Small epsilon to protect denominators (sigma and other divisions).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    outputs:\n        Model output y[k].\n    errors:\n        A priori error e[k] = d[k] - y[k].\n    coefficients:\n        History of neuron weights w[k] (shape (N+1, n_neurons) in base history).\n    error_type:\n        \"a_priori\".</p>\n\n<h2 id=\"extra-when-return_internal_statestrue\">Extra (when return_internal_states=True)</h2>\n\n<p>extra[\"centers_last\"]:\n    Final centers array (n_neurons, input_dim).\nextra[\"sigma_last\"]:\n    Final spreads array (n_neurons,).\nextra[\"last_activation\"]:\n    Last activation vector f(u_k) (n_neurons,).\nextra[\"last_regressor\"]:\n    Last regressor u_k (input_dim,).</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron", "kind": "class", "doc": "<p>Multilayer Perceptron (MLP) adaptive model with momentum (real-valued).</p>\n\n<p>Online adaptation of a 2-hidden-layer feedforward neural network using a\nstochastic-gradient update with momentum. The model is treated as an\nadaptive nonlinear filter.</p>\n\n<p>The forward pass is:</p>\n\n<p>$$v_1[k] = W_1 u[k] - b_1, \\qquad y_1[k] = \\phi(v_1[k]),$$</p>\n\n<p>$$v_2[k] = W_2 y_1[k] - b_2, \\qquad y_2[k] = \\phi(v_2[k]),$$</p>\n\n<p>$$y[k] = w_3^T y_2[k] - b_3,$$</p>\n\n<p>where <code>\\phi</code> is either <code>tanh</code> or <code>sigmoid</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>n_neurons : int, optional\n    Number of neurons in each hidden layer. Default is 10.\ninput_dim : int, optional\n    Dimension of the regressor vector <code>u[k]</code>. Default is 3.\n    If <code>optimize()</code> is called with a 1D input signal, this must be 3\n    (see Notes).\nstep_size : float, optional\n    Gradient step size <code>mu</code>. Default is 1e-2.\nmomentum : float, optional\n    Momentum factor in <code>[0, 1)</code>. Default is 0.9.\nactivation : {\"tanh\", \"sigmoid\"}, optional\n    Activation function used in both hidden layers. Default is <code>\"tanh\"</code>.\nw_init : array_like of float, optional\n    Optional initialization for the output-layer weights <code>w_3(0)</code>, with\n    shape <code>(n_neurons,)</code>. If None, Xavier/Glorot-style uniform\n    initialization is used for all weights.\nrng : numpy.random.Generator, optional\n    Random generator used for initialization.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Real-valued only\n    This implementation is restricted to real-valued signals and parameters\n    (<code>supports_complex=False</code>). The constraint is enforced via\n    <code>@ensure_real_signals</code> on <code>optimize()</code>.</p>\n\n<p>Input formats\n    The method <code>optimize()</code> accepts two input formats:</p>\n\n<pre><code>1. **Regressor matrix** ``U`` with shape ``(N, input_dim)``:\n   each row is used directly as ``u[k]``.\n\n2. **Scalar input signal** ``x[k]`` with shape ``(N,)``:\n   a 3-dimensional regressor is formed internally as\n\n   $$u[k] = [x[k],\\ d[k-1],\\ x[k-1]]^T,$$\n\n   therefore this mode requires ``input_dim = 3``.\n</code></pre>\n\n<p>Parameter update (as implemented)\n    Let the a priori error be <code>e[k] = d[k] - y[k]</code>. This implementation\n    applies a momentum update of the form</p>\n\n<pre><code>$$\\theta[k+1] = \\theta[k] + \\Delta\\theta[k] + \\beta\\,\\Delta\\theta[k-1],$$\n\nwhere ``\\beta`` is the momentum factor and ``\\Delta\\theta[k]`` is a\ngradient step proportional to ``e[k]``. (See source for the exact\nper-parameter expressions.)\n</code></pre>\n\n<p>Library conventions\n    - The base class <code>filter_order</code> is used only as a size indicator\n      (set to <code>n_neurons - 1</code>).\n    - <code>OptimizationResult.coefficients</code> stores a <em>proxy</em> coefficient\n      history: the output-layer weight vector <code>w3</code> as tracked through\n      <code>self.w</code> for compatibility with the base API.\n    - Full parameter trajectories can be returned in <code>result.extra</code> when\n      <code>return_internal_states=True</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.__init__", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">n_neurons</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">momentum</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.9</span>,</span><span class=\"param\">\t<span class=\"n\">activation</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;tanh&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">rng</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">_generator</span><span class=\"o\">.</span><span class=\"n\">Generator</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.supports_complex", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.n_neurons", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.n_neurons", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.input_dim", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.input_dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.step_size", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.step_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.momentum", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.momentum", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.w1", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.w1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.w2", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.w2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.w3", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.w3", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.b1", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.b1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.b2", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.b2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.b3", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.b3", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.prev_dw1", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.prev_dw1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.prev_dw2", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.prev_dw2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.prev_dw3", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.prev_dw3", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.prev_db1", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.prev_db1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.prev_db2", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.prev_db2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.prev_db3", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.prev_db3", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.w", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.w_history", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.MultilayerPerceptron.optimize", "modulename": "pydaptivefiltering", "qualname": "MultilayerPerceptron.optimize", "kind": "function", "doc": "<p>Executes the online MLP adaptation loop (with momentum).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Either:\n    - regressor matrix <code>U</code> with shape <code>(N, input_dim)</code>, or\n    - scalar input signal <code>x[k]</code> with shape <code>(N,)</code> (in which case the\n      regressor is built as <code>u[k] = [x[k], d[k-1], x[k-1]]</code> and\n      requires <code>input_dim = 3</code>).\ndesired_signal : array_like of float\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, stores parameter snapshots in <code>result.extra</code> (may be memory\n    intensive for long runs).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        Scalar output sequence <code>y[k]</code>.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Proxy coefficient history recorded by the base class (tracks\n        the output-layer weights <code>w3</code>).\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code> with:\n        - <code>w1_hist</code> : list of ndarray\n            Hidden-layer-1 weight snapshots.\n        - <code>w2_hist</code> : list of ndarray\n            Hidden-layer-2 weight snapshots.\n        - <code>w3_hist</code> : list of ndarray\n            Output-layer weight snapshots.\n        - <code>b1_hist</code> : list of ndarray\n            Bias-1 snapshots.\n        - <code>b2_hist</code> : list of ndarray\n            Bias-2 snapshots.\n        - <code>b3_hist</code> : list of float\n            Bias-3 snapshots.\n        - <code>activation</code> : str\n            Activation identifier (<code>\"tanh\"</code> or <code>\"sigmoid\"</code>).</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.RBF", "modulename": "pydaptivefiltering", "qualname": "RBF", "kind": "class", "doc": "<p>Radial Basis Function (RBF) adaptive model (real-valued).</p>\n\n<p>Online adaptation of an RBF network with Gaussian basis functions, following\nDiniz (Alg. 11.5). The algorithm updates:</p>\n\n<ul>\n<li>output weights <code>w</code> (one weight per neuron),</li>\n<li>centers <code>c_i</code> (stored in <code>vet</code>),</li>\n<li>spreads <code>sigma_i</code> (stored in <code>sigma</code>).</li>\n</ul>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>n_neurons : int\n    Number of RBF neurons (basis functions).\ninput_dim : int\n    Dimension of the regressor vector <code>u[k]</code>. If <code>optimize()</code> is called\n    with a 1D input signal, this is interpreted as the tap length.\nur : float, optional\n    Step size for center updates. Default is 1e-2.\nuw : float, optional\n    Step size for output-weight updates. Default is 1e-2.\nus : float, optional\n    Step size for spread (sigma) updates. Default is 1e-2.\nw_init : array_like of float, optional\n    Initial output-weight vector <code>w(0)</code> with shape <code>(n_neurons,)</code>.\n    If None, initializes from a standard normal distribution.\nsigma_init : float, optional\n    Initial spread value used for all neurons (must be positive). Default is 1.0.\ncenters_init_scale : float, optional\n    Scale used for random initialization of centers. Default is 0.5.\nrng : numpy.random.Generator, optional\n    Random generator used for reproducible initialization.\nsafe_eps : float, optional\n    Small positive constant used to guard denominators (e.g., <code>sigma^2</code> and\n    <code>sigma^3</code>). Default is 1e-12.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Real-valued only\n    This implementation is restricted to real-valued signals and parameters\n    (<code>supports_complex=False</code>). The constraint is enforced via\n    <code>@ensure_real_signals</code> on <code>optimize()</code>.</p>\n\n<p>Model\n    For a regressor vector <code>u[k] \\in \\mathbb{R}^{D}</code>, define Gaussian basis\n    functions:</p>\n\n<pre><code>$$\\phi_i(u[k]) = \\exp\\left(-\\frac{\\|u[k] - c_i\\|^2}{\\sigma_i^2}\\right),$$\n\nwhere ``c_i`` is the center and ``sigma_i &gt; 0`` is the spread of neuron ``i``.\nThe network output is\n\n$$y[k] = \\sum_{i=1}^{Q} w_i\\, \\phi_i(u[k]) = w^T \\phi(u[k]),$$\n\nwhere ``Q = n_neurons`` and ``\\phi(u[k]) \\in \\mathbb{R}^{Q}`` stacks all\nactivations.\n</code></pre>\n\n<p>Input formats\n    The method <code>optimize()</code> accepts two input formats:</p>\n\n<pre><code>1. **Regressor matrix** ``U`` with shape ``(N, input_dim)``:\n   each row is used directly as ``u[k]``.\n\n2. **Scalar input signal** ``x[k]`` with shape ``(N,)``:\n   tapped-delay regressors of length ``input_dim`` are built as\n\n   $$u[k] = [x[k], x[k-1], \\ldots, x[k-input\\_dim+1]]^T.$$\n</code></pre>\n\n<p>Library conventions\n    - <code>OptimizationResult.coefficients</code> stores the history of the <strong>output\n      weights</strong> <code>w</code> (the neuron output layer).\n    - Centers and spreads are returned via <code>result.extra</code> when\n      <code>return_internal_states=True</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.RBF.__init__", "modulename": "pydaptivefiltering", "qualname": "RBF.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">n_neurons</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">ur</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">uw</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">us</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">sigma_init</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">centers_init_scale</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">rng</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">_generator</span><span class=\"o\">.</span><span class=\"n\">Generator</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span>)</span>"}, {"fullname": "pydaptivefiltering.RBF.supports_complex", "modulename": "pydaptivefiltering", "qualname": "RBF.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.RBF.n_neurons", "modulename": "pydaptivefiltering", "qualname": "RBF.n_neurons", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.RBF.input_dim", "modulename": "pydaptivefiltering", "qualname": "RBF.input_dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.RBF.ur", "modulename": "pydaptivefiltering", "qualname": "RBF.ur", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.RBF.uw", "modulename": "pydaptivefiltering", "qualname": "RBF.uw", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.RBF.us", "modulename": "pydaptivefiltering", "qualname": "RBF.us", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.RBF.vet", "modulename": "pydaptivefiltering", "qualname": "RBF.vet", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.RBF.sigma", "modulename": "pydaptivefiltering", "qualname": "RBF.sigma", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.RBF.w_history", "modulename": "pydaptivefiltering", "qualname": "RBF.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.RBF.optimize", "modulename": "pydaptivefiltering", "qualname": "RBF.optimize", "kind": "function", "doc": "<p>Executes the RBF online adaptation loop.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Either:\n    - regressor matrix <code>U</code> with shape <code>(N, input_dim)</code>, or\n    - scalar input signal <code>x[k]</code> with shape <code>(N,)</code> (tapped-delay\n      regressors are built internally).\ndesired_signal : array_like of float\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes final centers/spreads and last activation vector\n    in <code>result.extra</code>.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        Scalar output sequence <code>y[k] = w^T \\phi(u[k])</code>.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Output-weight history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code> with:\n        - <code>centers_last</code> : ndarray of float\n            Final centers array (shape <code>(n_neurons, input_dim)</code>).\n        - <code>sigma_last</code> : ndarray of float\n            Final spreads vector (shape <code>(n_neurons,)</code>).\n        - <code>last_phi</code> : ndarray of float\n            Last basis-function activation vector <code>\\phi(u[k])</code> (shape <code>(n_neurons,)</code>).</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.VolterraLMS", "modulename": "pydaptivefiltering", "qualname": "VolterraLMS", "kind": "class", "doc": "<p>Second-order Volterra LMS adaptive filter (real-valued).</p>\n\n<p>Volterra LMS (Diniz, Alg. 11.1) using a second-order Volterra expansion.\nThe adaptive model augments a linear tapped-delay regressor with all\nquadratic products (including squares) and performs an LMS-type update on\nthe expanded coefficient vector.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>memory : int, optional\n    Linear memory length <code>L</code>. The linear delay line is\n    <code>[x[k], x[k-1], ..., x[k-L+1]]</code>. Default is 3.\nstep_size : float or array_like of float, optional\n    Step size <code>mu</code>. Can be either:\n    - a scalar (same step for all coefficients), or\n    - a vector with shape <code>(n_coeffs,)</code> for per-term step scaling.\n    Default is 1e-2.\nw_init : array_like of float, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(n_coeffs,)</code>. If None,\n    initializes with zeros.\nsafe_eps : float, optional\n    Small positive constant kept for API consistency across the library.\n    (Not used directly by this implementation.) Default is 1e-12.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Real-valued only\n    This implementation is restricted to real-valued signals and coefficients\n    (<code>supports_complex=False</code>). The constraint is enforced via\n    <code>@ensure_real_signals</code> on <code>optimize()</code>.</p>\n\n<p>Volterra regressor (as implemented)\n    Let the linear delay line be</p>\n\n<pre><code>$$x_{lin}[k] = [x[k], x[k-1], \\ldots, x[k-L+1]]^T \\in \\mathbb{R}^{L}.$$\n\nThe second-order Volterra regressor is constructed as\n\n$$u[k] =\n</code></pre>\n\n<p>\\begin{bmatrix}\n    x_{lin}[k] \\\n    \\mathrm{vec}\\bigl(x_{lin}[k] x_{lin}^T[k]\\bigr)_{i \\le j}\n\\end{bmatrix}\n\\in \\mathbb{R}^{n_{coeffs}},$$</p>\n\n<pre><code>where the quadratic block contains all products ``x_{lin,i}[k] x_{lin,j}[k]``\nfor ``0 \\le i \\le j \\le L-1`` (unique terms only).\n\nThe number of coefficients is therefore\n\n$$n_{coeffs} = L + \\frac{L(L+1)}{2}.$$\n</code></pre>\n\n<p>LMS recursion (a priori)\n    With</p>\n\n<pre><code>$$y[k] = w^T[k] u[k], \\qquad e[k] = d[k] - y[k],$$\n\nthe update implemented here is\n\n$$w[k+1] = w[k] + 2\\mu\\, e[k] \\, u[k],$$\n\nwhere ``\\mu`` may be scalar or element-wise (vector step).\n</code></pre>\n\n<p>Implementation details\n    - The coefficient vector <code>self.w</code> stores the full Volterra parameter\n      vector (linear + quadratic) and is recorded by the base class.\n    - The quadratic term ordering matches the nested loops used in\n      <code>_create_volterra_regressor()</code> (i increasing, j from i to L-1).</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.VolterraLMS.__init__", "modulename": "pydaptivefiltering", "qualname": "VolterraLMS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">memory</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span>)</span>"}, {"fullname": "pydaptivefiltering.VolterraLMS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "VolterraLMS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.VolterraLMS.memory", "modulename": "pydaptivefiltering", "qualname": "VolterraLMS.memory", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.VolterraLMS.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "VolterraLMS.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.VolterraLMS.w", "modulename": "pydaptivefiltering", "qualname": "VolterraLMS.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.VolterraLMS.w_history", "modulename": "pydaptivefiltering", "qualname": "VolterraLMS.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.VolterraLMS.optimize", "modulename": "pydaptivefiltering", "qualname": "VolterraLMS.optimize", "kind": "function", "doc": "<p>Executes the Volterra LMS adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of float\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes the last internal states in <code>result.extra</code>:\n    <code>\"last_regressor\"</code>, <code>\"memory\"</code>, and <code>\"n_coeffs\"</code>.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        Scalar a priori output sequence, <code>y[k] = w^T[k] u[k]</code>.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Volterra coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.VolterraRLS", "modulename": "pydaptivefiltering", "qualname": "VolterraRLS", "kind": "class", "doc": "<p>Second-order Volterra RLS adaptive filter (real-valued).</p>\n\n<p>Volterra RLS (Diniz, Alg. 11.2) using a second-order Volterra expansion and\nan RLS update applied to the expanded regressor. The model augments a linear\ntapped-delay regressor with all unique quadratic products (including\nsquares) and estimates the corresponding coefficient vector via RLS.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>memory : int, optional\n    Linear memory length <code>L</code>. The linear delay line is\n    <code>[x[k], x[k-1], ..., x[k-L+1]]</code>. Default is 3.\nforgetting_factor : float, optional\n    Forgetting factor <code>lambda</code> with <code>0 &lt; lambda &lt;= 1</code>. Default is 0.98.\ndelta : float, optional\n    Regularization parameter used to initialize the inverse correlation\n    matrix as <code>P(0) = I/delta</code> (requires <code>delta &gt; 0</code>). Default is 1.0.\nw_init : array_like of float, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(n_coeffs,)</code>. If None,\n    initializes with zeros.\nsafe_eps : float, optional\n    Small positive constant used to guard denominators. Default is 1e-12.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Real-valued only\n    This implementation is restricted to real-valued signals and coefficients\n    (<code>supports_complex=False</code>). The constraint is enforced via\n    <code>@ensure_real_signals</code> on <code>optimize()</code>.</p>\n\n<p>Volterra regressor (as implemented)\n    Let the linear delay line be</p>\n\n<pre><code>$$x_{lin}[k] = [x[k], x[k-1], \\ldots, x[k-L+1]]^T \\in \\mathbb{R}^{L}.$$\n\nThe second-order Volterra regressor is constructed as\n\n$$u[k] =\n</code></pre>\n\n<p>\\begin{bmatrix}\n    x_{lin}[k] \\\n    \\mathrm{vec}\\bigl(x_{lin}[k] x_{lin}^T[k]\\bigr)_{i \\le j}\n\\end{bmatrix}\n\\in \\mathbb{R}^{n_{coeffs}},$$</p>\n\n<pre><code>where the quadratic block contains all products ``x_{lin,i}[k] x_{lin,j}[k]``\nfor ``0 \\le i \\le j \\le L-1``.\n\nThe number of coefficients is\n\n$$n_{coeffs} = L + \\frac{L(L+1)}{2}.$$\n</code></pre>\n\n<p>RLS recursion (a priori form)\n    With</p>\n\n<pre><code>$$y[k] = w^T[k-1] u[k], \\qquad e[k] = d[k] - y[k],$$\n\ndefine the gain\n\n$$g[k] = \\frac{P[k-1] u[k]}{\\lambda + u^T[k] P[k-1] u[k]},$$\n\nthe inverse correlation update\n\n$$P[k] = \\frac{1}{\\lambda}\\left(P[k-1] - g[k] u^T[k] P[k-1]\\right),$$\n\nand the coefficient update\n\n$$w[k] = w[k-1] + g[k] e[k].$$\n</code></pre>\n\n<p>A posteriori quantities\n    If requested, this implementation also computes the <em>a posteriori</em>\n    output/error after updating the coefficients at time <code>k</code>:</p>\n\n<pre><code>$$y^{post}[k] = w^T[k] u[k], \\qquad e^{post}[k] = d[k] - y^{post}[k].$$\n</code></pre>\n\n<p>Implementation details\n    - The denominator <code>lambda + u^T P u</code> is guarded by <code>safe_eps</code> to avoid\n      numerical issues when very small.\n    - Coefficient history is recorded via the base class.\n    - The quadratic-term ordering matches <code>_create_volterra_regressor()</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.VolterraRLS.__init__", "modulename": "pydaptivefiltering", "qualname": "VolterraRLS.__init__", "kind": "function", "doc": "<h2 id=\"parameters\">Parameters</h2>\n\n<p>memory:\n    Linear memory length L. Determines number of Volterra coefficients:\n    n_coeffs = L + L(L+1)/2.\nforgetting_factor:\n    Forgetting factor \u03bb (typically close to 1). Must satisfy 0 &lt; \u03bb &lt;= 1.\ndelta:\n    Positive regularization for initializing the inverse correlation matrix:\n    P[0] = I / delta.\nw_init:\n    Optional initial coefficient vector (length n_coeffs). If None, zeros.\nsafe_eps:\n    Small epsilon to guard denominators.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">memory</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>,</span><span class=\"param\">\t<span class=\"n\">forgetting_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.98</span>,</span><span class=\"param\">\t<span class=\"n\">delta</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">*</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span>)</span>"}, {"fullname": "pydaptivefiltering.VolterraRLS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "VolterraRLS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.VolterraRLS.memory", "modulename": "pydaptivefiltering", "qualname": "VolterraRLS.memory", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.VolterraRLS.lam", "modulename": "pydaptivefiltering", "qualname": "VolterraRLS.lam", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.VolterraRLS.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "VolterraRLS.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.VolterraRLS.w", "modulename": "pydaptivefiltering", "qualname": "VolterraRLS.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.VolterraRLS.P", "modulename": "pydaptivefiltering", "qualname": "VolterraRLS.P", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.VolterraRLS.w_history", "modulename": "pydaptivefiltering", "qualname": "VolterraRLS.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.VolterraRLS.optimize", "modulename": "pydaptivefiltering", "qualname": "VolterraRLS.optimize", "kind": "function", "doc": "<p>Executes the Volterra RLS adaptation loop over paired input/desired sequences.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : array_like of float\n    Desired sequence <code>d[k]</code> with shape <code>(N,)</code> (will be flattened).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes additional internal sequences in <code>result.extra</code>,\n    including a posteriori output/error and last gain/denominator.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of float, shape <code>(N,)</code>\n        Scalar a priori output sequence, <code>y[k] = w^T[k-1] u[k]</code>.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Scalar a priori error sequence, <code>e[k] = d[k] - y[k]</code>.\n    - coefficients : ndarray of float\n        Volterra coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"a_priori\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code> with:\n        - <code>posteriori_outputs</code> : ndarray of float\n            A posteriori output sequence <code>y^{post}[k]</code>.\n        - <code>posteriori_errors</code> : ndarray of float\n            A posteriori error sequence <code>e^{post}[k]</code>.\n        - <code>last_gain</code> : ndarray of float\n            Last RLS gain vector <code>g[k]</code>.\n        - <code>last_den</code> : float\n            Last gain denominator <code>lambda + u^T P u</code>.\n        - <code>last_regressor</code> : ndarray of float\n            Last Volterra regressor <code>u[k]</code>.\n        - <code>memory</code> : int\n            Linear memory length <code>L</code>.\n        - <code>n_coeffs</code> : int\n            Number of Volterra coefficients.\n        - <code>forgetting_factor</code> : float\n            The forgetting factor <code>lambda</code> used.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.CFDLMS", "modulename": "pydaptivefiltering", "qualname": "CFDLMS", "kind": "class", "doc": "<p>Constrained Frequency-Domain LMS (CFDLMS) for real-valued signals (block adaptive).</p>\n\n<p>Implements the Constrained Frequency-Domain LMS algorithm (Algorithm 12.4, Diniz)\nfor identifying/estimating a real-valued FIR system in a block-wise frequency-domain\nframework with a time-domain constraint (to control circular convolution / enforce\neffective FIR support).</p>\n\n<h2 id=\"block-structure-and-main-variables\">Block structure and main variables</h2>\n\n<p>Let:\n    - M: number of subbands / FFT size (also the block length in frequency domain),\n    - L: decimation / number of fresh time samples per iteration (block advance),\n    - Nw: time-support (per subband) of the adaptive filters, so each subband filter\n          has length (Nw+1) in the <em>time-lag</em> axis (columns of <code>ww</code>).</p>\n\n<h2 id=\"internal-coefficient-representation\">Internal coefficient representation</h2>\n\n<p>The adaptive parameters are stored as a complex matrix:</p>\n\n<pre><code>ww  in C^{M x (Nw+1)}\n</code></pre>\n\n<p>where each row corresponds to one frequency bin (subband), and each column is a\ndelay-tap in the <em>block</em> (overlap) dimension.</p>\n\n<p>For compatibility with the base API:\n    - <code>self.w</code> stores a flattened real view of <code>ww</code> (real part only),\n    - <code>OptimizationResult.coefficients</code> comes from the base <code>w_history</code> (flattened),\n    - the full matrix trajectory is returned in <code>result.extra[\"ww_history\"]</code>.</p>\n\n<h2 id=\"signal-processing-conventions-as-implemented\">Signal processing conventions (as implemented)</h2>\n\n<p>Per iteration k (block index):</p>\n\n<ul>\n<li>Build an M-length time vector from the most recent input segment (reversed):\n    x_p = [x[kL+M-1], ..., x[kL]]^T\nthen compute a *unitary* FFT:\n    ui = FFT(x_p) / sqrt(M)</li>\n</ul>\n\n<ul>\n<li><p>Maintain a regressor matrix <code>uu</code> with shape (M, Nw+1) containing the most recent\nNw+1 frequency-domain regressors (columns shift right each iteration).</p></li>\n<li><p>Compute frequency-domain output per bin:\n    uy = sum_j uu[:, j] * ww[:, j]\nand return to time domain:\n    y_block = IFFT(uy) * sqrt(M)</p>\n\n<p>Only the first L samples are used as the \u201cvalid\u201d output of this block.</p></li>\n</ul>\n\n<h2 id=\"error-energy-smoothing-and-update\">Error, energy smoothing, and update</h2>\n\n<p>The algorithm forms an L-length error (in the reversed time order used internally),\nzero-pads it to length M, and FFTs it (unitary) to obtain <code>et</code>.</p>\n\n<p>A smoothed energy estimate per bin is kept:\n    sig[k] = (1-a) sig[k-1] + a |ui|^2\nwhere <code>a = smoothing</code>.</p>\n\n<p>The normalized per-bin step is:\n    gain = step / (gamma + (Nw+1) * sig)</p>\n\n<p>A preliminary frequency-domain correction is built:\n    wwc = gain[:,None] * conj(uu) * et[:,None]</p>\n\n<h2 id=\"constrained-time-domain-projection\">Constrained / time-domain projection</h2>\n\n<p>The \u201cconstraint\u201d is applied by transforming wwc along axis=0 (FFT across bins),\nzeroing time indices &gt;= L (i.e., enforcing an L-sample time support),\nand transforming back (IFFT). This is the standard \u201cconstrained\u201d step that reduces\ncircular-convolution artifacts.</p>\n\n<h2 id=\"returned-sequences\">Returned sequences</h2>\n\n<ul>\n<li><code>outputs</code>: real-valued estimated output, length = n_iters * L</li>\n<li><code>errors</code>:  real-valued output error (d - y), same length as outputs</li>\n<li><code>error_type=\"output_error\"</code> (block output error, not a priori scalar error)</li>\n</ul>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int, default=5\n    Subband filter order Nw (number of taps is Nw+1 along the overlap dimension).\nn_subbands : int, default=64\n    FFT size M (number of subbands / frequency bins).\ndecimation : int, optional\n    Block advance L (samples per iteration). If None, defaults to M//2.\nstep_size : float, default=0.1\n    Global step size (mu).\ngamma : float, default=1e-2\n    Regularization constant in the normalization denominator (&gt;0).\nsmoothing : float, default=0.01\n    Exponential smoothing factor a in (0,1].\nw_init : array_like, optional\n    Initial coefficients. Can be either:\n    - matrix shape (M, Nw+1), or\n    - flat length M*(Nw+1), reshaped internally.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<ul>\n<li>Real-valued interface: input_signal and desired_signal are enforced real.\nInternally complex arithmetic is used due to FFT processing.</li>\n<li>This is a block algorithm: one iteration produces L output samples.</li>\n</ul>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.CFDLMS.__init__", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">n_subbands</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">64</span>,</span><span class=\"param\">\t<span class=\"n\">decimation</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">smoothing</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.CFDLMS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.CFDLMS.M", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.M", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.CFDLMS.L", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.L", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.CFDLMS.Nw", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.Nw", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.CFDLMS.step", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.step", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.CFDLMS.gamma", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.gamma", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.CFDLMS.smoothing", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.smoothing", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.CFDLMS.step_size", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.step_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.CFDLMS.ww", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.ww", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.CFDLMS.uu", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.uu", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.CFDLMS.sig", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.sig", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.CFDLMS.w", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.CFDLMS.w_history", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.CFDLMS.ww_history", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.ww_history", "kind": "variable", "doc": "<p></p>\n", "annotation": ": list[numpy.ndarray]"}, {"fullname": "pydaptivefiltering.CFDLMS.reset_filter", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.reset_filter", "kind": "function", "doc": "<p>Reset coefficients/history.</p>\n\n<p>If w_new is:</p>\n\n<ul>\n<li>None: zeros</li>\n<li>shape (M, Nw+1): used directly</li>\n<li>flat of length M*(Nw+1): reshaped</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">w_new</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.CFDLMS.optimize", "modulename": "pydaptivefiltering", "qualname": "CFDLMS.optimize", "kind": "function", "doc": "<p>Run CFDLMS adaptation over real-valued (x[n], d[n]) in blocks.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Input sequence x[n], shape (N,).\ndesired_signal : array_like of float\n    Desired sequence d[n], shape (N,).\nverbose : bool, default=False\n    If True, prints runtime and basic iteration stats.\nreturn_internal_states : bool, default=False\n    If True, includes additional internal trajectories in result.extra.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    outputs : ndarray of float, shape (n_iters * L,)\n        Concatenated block outputs (L per iteration).\n    errors : ndarray of float, shape (n_iters * L,)\n        Output error sequence e[n] = d[n] - y[n].\n    coefficients : ndarray\n        Flattened coefficient history (from base class; real part of ww).\n    error_type : str\n        \"output_error\".\n    extra : dict\n        Always contains:\n            - \"ww_history\": list of ndarray, each shape (M, Nw+1)\n            - \"n_iters\": int\n        If return_internal_states=True, also contains:\n            - \"sig\": ndarray, shape (M,) final smoothed per-bin energy\n            - \"sig_history\": ndarray, shape (n_iters, M)</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.DLCLLMS", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS", "kind": "class", "doc": "<p>Delayless Closed-Loop Subband LMS (DLCLLMS) for real-valued fullband signals.</p>\n\n<p>Implements the Delayless Closed-Loop Subband LMS adaptive filtering algorithm\n(Algorithm 12.3, Diniz) using:</p>\n\n<ul>\n<li>a DFT analysis bank (complex subband signals),</li>\n<li>a polyphase Nyquist / fractional-delay prototype (Ed) to realize the delayless\nclosed-loop structure,</li>\n<li>and an equivalent fullband FIR mapping (GG) used to generate the output in the\ntime domain.</li>\n</ul>\n\n<h2 id=\"high-level-operation-as-implemented\">High-level operation (as implemented)</h2>\n\n<p>Processing is block-based with block length:\n    L = M   (M = number of subbands / DFT size)</p>\n\n<p>For each block k:\n  1) Form a reversed block x_p and pass each sample through a per-branch fractional-delay\n     structure (polyphase) driven by <code>Ed</code>, producing x_frac (length M).\n  2) Compute subband input:\n        x_sb = F @ x_frac\n     where F is the (non-unitary) DFT matrix (MATLAB dftmtx convention).\n  3) Map current subband coefficients to an equivalent fullband FIR:\n        GG = equivalent_fullband(w_sb)\n     and filter the fullband input block through GG (with state) to produce y_block.\n  4) Compute fullband error e_block = d_block - y_block.\n  5) Pass the reversed error block through the same fractional-delay structure to get e_frac,\n     then compute subband error:\n        e_sb = F @ e_frac\n  6) Update subband coefficients with an LMS-like recursion using a subband delay line x_cl\n     and a smoothed power estimate sig[m]:\n        sig[m] = (1-a) sig[m] + a |x_sb[m]|^2\n        mu_n  = step / (gamma + (Nw+1) * sig[m])\n        w_sb[m,:] &lt;- w_sb[m,:] + 2 * mu_n * conj(e_sb[m]) * x_cl[m,:]</p>\n\n<h2 id=\"coefficient-representation-and-mapping\">Coefficient representation and mapping</h2>\n\n<ul>\n<li><p>Subband coefficients are stored in:\n  w_sb : complex ndarray, shape (M, Nw+1)</p></li>\n<li><p>For output synthesis and for the base API, an equivalent fullband FIR is built:\n    GG : real ndarray, length (M*Nw)</p>\n\n<p>The mapping matches the provided MATLAB logic:</p>\n\n<ul>\n<li>Compute ww = real(F^H w_sb) / M</li>\n<li>For branch m=0: take ww[0, :Nw]</li>\n<li>For m>=1: convolve ww[m,:] with Ed[m-1,:] and extract a length-Nw segment\nstarting at (Dint+1), where Dint=(P-1)//2 and P is the polyphase length.</li>\n</ul></li>\n<li><p>The base-class coefficient vector <code>self.w</code> stores GG (float), and\n<code>OptimizationResult.coefficients</code> contains the history of GG recorded <strong>once per block</strong>\n(plus the initial entry).</p></li>\n</ul>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int, default=5\n    Subband filter order Nw (number of taps per subband delay line is Nw+1).\nn_subbands : int, default=4\n    Number of subbands M (DFT size). Also equals the processing block length L.\nstep_size : float, default=0.1\n    Global LMS step size.\ngamma : float, default=1e-2\n    Regularization constant in the normalized step denominator (&gt;0 recommended).\na : float, default=1e-2\n    Exponential smoothing factor for subband power sig in (0,1].\nnyquist_len : int, default=2\n    Length Nfd of the Nyquist (fractional-delay) prototype used to build Ed.\nw_init : array_like, optional\n    Initial subband coefficient matrix. Can be either:\n      - shape (M, Nw+1), or\n      - flat length M*(Nw+1), reshaped internally.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<ul>\n<li>Real-valued interface (input_signal and desired_signal enforced real). Internal\ncomputations use complex subband signals.</li>\n<li>This implementation processes only <code>n_used = floor(N/M)*M</code> samples. Any tail\nsamples (N - n_used) are left with output=0 and error=d in that region.</li>\n<li>The reported <code>error_type</code> is \"output_error\" (fullband output error sequence).</li>\n</ul>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.DLCLLMS.__init__", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">n_subbands</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">a</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">nyquist_len</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.DLCLLMS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.DLCLLMS.M", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.M", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.DLCLLMS.Nw", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.Nw", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.DLCLLMS.step_size", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.DLCLLMS.gamma", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.gamma", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.DLCLLMS.a", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.a", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.DLCLLMS.nyquist_len", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.nyquist_len", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.DLCLLMS.Ed", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.Ed", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.DLCLLMS.F", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.F", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.DLCLLMS.w_sb", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.w_sb", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.DLCLLMS.x_cl", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.x_cl", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.DLCLLMS.sig", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.sig", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.DLCLLMS.w_history", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.DLCLLMS.reset_filter", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.reset_filter", "kind": "function", "doc": "<p>Reset coefficients and history.</p>\n\n<ul>\n<li>If w_new is provided:\n<ul>\n<li>If shape (M, Nw+1): interpreted as subband coefficients.</li>\n<li>If flat of length M*(Nw+1): reshaped as subband coefficients.</li>\n</ul></li>\n<li>Resets internal states (x_cl, sig, fractional-delay, FIR state).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">w_new</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.DLCLLMS.optimize", "modulename": "pydaptivefiltering", "qualname": "DLCLLMS.optimize", "kind": "function", "doc": "<p>Run DLCLLMS adaptation block-by-block.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Fullband input x[n], shape (N,).\ndesired_signal : array_like of float\n    Fullband desired d[n], shape (N,).\nverbose : bool, default=False\n    If True, prints runtime and block stats.\nreturn_internal_states : bool, default=False\n    If True, returns additional internal trajectories in result.extra.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    outputs : ndarray of float, shape (N,)\n        Estimated fullband output y[n]. Only the first <code>n_used</code> samples are\n        produced by block processing; remaining tail (if any) is zero.\n    errors : ndarray of float, shape (N,)\n        Fullband error e[n] = d[n] - y[n]. Tail (if any) equals d[n] there.\n    coefficients : ndarray\n        History of equivalent fullband FIR vectors GG (length M*Nw), stored\n        once per processed block (plus initial entry).\n    error_type : str\n        \"output_error\".</p>\n\n<pre><code>extra : dict\n    Always contains:\n        - \"n_blocks\": number of processed blocks\n        - \"block_len\": block length (equals M)\n        - \"n_used\": number of processed samples (multiple of M)\n    If return_internal_states=True, also contains:\n        - \"sig_history\": ndarray (n_blocks, M) of smoothed subband power\n        - \"w_sb_final\": final subband coefficient matrix (M, Nw+1)\n</code></pre>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.OLSBLMS", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS", "kind": "class", "doc": "<p>Open-Loop Subband LMS (OLSBLMS) for real-valued fullband signals.</p>\n\n<p>Implements the Open-Loop Subband LMS adaptive filtering algorithm\n(Algorithm 12.1, Diniz) using an analysis/synthesis filterbank with\nsubband-adaptive FIR filters.</p>\n\n<h2 id=\"high-level-operation-as-implemented\">High-level operation (as implemented)</h2>\n\n<p>Given fullband input x[n] and desired d[n], and an M-channel analysis bank h_k[m],\nthe algorithm proceeds in two stages:</p>\n\n<p>(A) Analysis + Decimation (open-loop)\n    For each subband m = 0..M-1:\n      - Filter the fullband input and desired with the analysis filter:\n            x_aux[m] = filter(hk[m], 1, x)\n            d_aux[m] = filter(hk[m], 1, d)\n      - Decimate by L (keep samples 0, L, 2L, ...):\n            x_sb[m] = x_aux[m][::L]\n            d_sb[m] = d_aux[m][::L]</p>\n\n<pre><code>The adaptation length is:\n    N_iter = min_m len(x_sb[m]) and len(d_sb[m])\n(i.e., all subbands are truncated to the shortest decimated sequence).\n</code></pre>\n\n<p>(B) Subband LMS adaptation (per-sample in decimated time)\n    Each subband has its own tapped-delay line x_ol[m,:] of length (Nw+1) and\n    its own coefficient vector w_mat[m,:] (also length Nw+1).</p>\n\n<pre><code>For each decimated-time index k = 0..N_iter-1, and for each subband m:\n  - Update subband delay line:\n        x_ol[m,0] = x_sb[m,k]\n  - Compute subband output and error:\n        y_sb[m,k] = w_mat[m]^T x_ol[m]\n        e_sb[m,k] = d_sb[m,k] - y_sb[m,k]\n  - Update a smoothed subband energy estimate:\n        sig_ol[m] = (1-a) sig_ol[m] + a * x_sb[m,k]^2\n  - Normalized LMS-like step:\n        mu_m = (2*step) / (gamma + (Nw+1)*sig_ol[m])\n  - Coefficient update:\n        w_mat[m] &lt;- w_mat[m] + mu_m * e_sb[m,k] * x_ol[m]\n</code></pre>\n\n<h2 id=\"fullband-reconstruction-convenience-synthesis\">Fullband reconstruction (convenience synthesis)</h2>\n\n<p>After adaptation, a fullband output is reconstructed via the synthesis bank f_k[m]:</p>\n\n<ul>\n<li>Upsample each subband output by L (zero-stuffing), then filter:\ny_up[m]   = upsample(y_sb[m], L)\ny_full[m] = filter(fk[m], 1, y_up[m])</li>\n<li>Sum across subbands:\ny[n] = sum_m y_full[m][n]\nThe returned error is the fullband output error e[n] = d[n] - y[n].</li>\n</ul>\n\n<h2 id=\"coefficient-representation-and-history\">Coefficient representation and history</h2>\n\n<ul>\n<li>The adaptive parameters are stored as:\nw_mat : ndarray, shape (M, Nw+1), dtype=float</li>\n<li>For compatibility with the base class, <code>self.w</code> is a flattened view of w_mat\n(row-major), and <code>OptimizationResult.coefficients</code> contains the stacked history\nof this flattened vector (recorded once per decimated-time iteration, plus the\ninitial entry).</li>\n<li>The full (M, Nw+1) snapshots are also stored in <code>extra[\"w_matrix_history\"]</code>.</li>\n</ul>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>n_subbands : int\n    Number of subbands (M).\nanalysis_filters : array_like\n    Analysis bank hk with shape (M, Lh).\nsynthesis_filters : array_like\n    Synthesis bank fk with shape (M, Lf).\nfilter_order : int\n    Subband FIR order Nw (number of taps per subband is Nw+1).\nstep_size : float, default=0.1\n    Global LMS step-size factor.\ngamma : float, default=1e-2\n    Regularization term in the normalized denominator (&gt;0 recommended).\na : float, default=0.01\n    Exponential smoothing factor for subband energy estimates in (0,1].\ndecimation_factor : int, optional\n    Decimation factor L. If None, uses L=M.\nw_init : array_like, optional\n    Initial subband coefficients. Can be:\n      - shape (M, Nw+1), or\n      - flat of length M*(Nw+1), reshaped row-major.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<ul>\n<li>Real-valued interface (input_signal and desired_signal enforced real).</li>\n<li>This is an <em>open-loop</em> structure: subband regressors are formed from the\nanalysis-filtered fullband input, independent of any reconstructed fullband\noutput loop.</li>\n<li>Subband MSE curves are provided as <code>mse_subbands = e_sb**2</code> and\n<code>mse_overall = mean_m mse_subbands[m,k]</code>.</li>\n</ul>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.OLSBLMS.__init__", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">n_subbands</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">analysis_filters</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">synthesis_filters</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">a</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">decimation_factor</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.OLSBLMS.supports_complex", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "False"}, {"fullname": "pydaptivefiltering.OLSBLMS.M", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.M", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.OLSBLMS.Nw", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.Nw", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.OLSBLMS.L", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.L", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.OLSBLMS.step", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.step", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.OLSBLMS.gamma", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.gamma", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.OLSBLMS.a", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.a", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.OLSBLMS.step_size", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.step_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.OLSBLMS.hk", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.hk", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.OLSBLMS.fk", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.fk", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.OLSBLMS.w_mat", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.w_mat", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.OLSBLMS.w", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.OLSBLMS.w_history", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.OLSBLMS.w_matrix_history", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.w_matrix_history", "kind": "variable", "doc": "<p></p>\n", "annotation": ": list[numpy.ndarray]"}, {"fullname": "pydaptivefiltering.OLSBLMS.default_test_init_kwargs", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.default_test_init_kwargs", "kind": "function", "doc": "<p>Override in subclasses to provide init kwargs for standardized tests.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">order</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.OLSBLMS.optimize", "modulename": "pydaptivefiltering", "qualname": "OLSBLMS.optimize", "kind": "function", "doc": "<p>Run OLSBLMS adaptation.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of float\n    Fullband input x[n], shape (N,).\ndesired_signal : array_like of float\n    Fullband desired d[n], shape (N,).\nverbose : bool, default=False\n    If True, prints runtime and iteration count.\nreturn_internal_states : bool, default=False\n    If True, returns additional internal states in result.extra.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    outputs : ndarray of float, shape (N,)\n        Fullband reconstructed output y[n] obtained by synthesis of the\n        subband outputs after adaptation.\n    errors : ndarray of float, shape (N,)\n        Fullband output error e[n] = d[n] - y[n].\n    coefficients : ndarray\n        Flattened coefficient history of w_mat, shape\n        (#snapshots, M*(Nw+1)), where snapshots are recorded once per\n        subband-iteration (decimated-time step), plus the initial entry.\n    error_type : str\n        \"output_error\".</p>\n\n<pre><code>extra : dict\n    Always contains:\n      - \"w_matrix_history\": list of (M, Nw+1) coefficient snapshots\n      - \"subband_outputs\": ndarray (M, N_iter)\n      - \"subband_errors\": ndarray (M, N_iter)\n      - \"mse_subbands\": ndarray (M, N_iter) with e_sb**2\n      - \"mse_overall\": ndarray (N_iter,) mean subband MSE per iteration\n    If return_internal_states=True, also contains:\n      - \"sig_ol\": final subband energy estimates, shape (M,)\n</code></pre>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.AffineProjectionCM", "modulename": "pydaptivefiltering", "qualname": "AffineProjectionCM", "kind": "class", "doc": "<p>Complex Affine-Projection Constant-Modulus (AP-CM) adaptive filter.</p>\n\n<p>Blind affine-projection algorithm based on the constant-modulus criterion,\nfollowing Diniz (Alg. 13.4). This implementation uses a <em>unit-modulus</em>\nreference (i.e., target magnitude equal to 1) obtained by normalizing the\naffine-projection output vector.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int, optional\n    Adaptive FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\n    Default is 5.\nstep_size : float, optional\n    Adaptation step size <code>mu</code>. Default is 0.1.\nmemory_length : int, optional\n    Reuse factor <code>L</code> (number of past regressors reused). The affine-\n    projection block size is therefore <code>P = L + 1</code> columns. Default is 2.\ngamma : float, optional\n    Levenberg-Marquardt regularization factor <code>gamma</code> used in the\n    <code>(L + 1) x (L + 1)</code> normal-equation system for numerical stability.\n    Default is 1e-6.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>At iteration <code>k</code>, form the regressor block matrix:</p>\n\n<ul>\n<li><code>X(k) \u2208 C^{(M+1) x (L+1)}</code>, whose columns are the most recent regressor\nvectors (newest in column 0).</li>\n</ul>\n\n<p>The affine-projection output vector is:</p>\n\n<p>$$y_{ap}(k) = X^H(k) w(k)  \\in \\mathbb{C}^{L+1}.$$</p>\n\n<p>This implementation uses a <em>unit-circle projection</em> (normalization) as the\nconstant-modulus \"reference\":</p>\n\n<p>$$d_{ap}(k) = \\frac{y_{ap}(k)}{|y_{ap}(k)|},$$</p>\n\n<p>applied element-wise, with a small threshold to avoid division by zero.</p>\n\n<p>The error vector is:</p>\n\n<p>$$e_{ap}(k) = d_{ap}(k) - y_{ap}(k).$$</p>\n\n<p>The update direction <code>g(k)</code> is obtained by solving the regularized system:</p>\n\n<p>$$(X^H(k) X(k) + \\gamma I_{L+1})\\, g(k) = e_{ap}(k),$$</p>\n\n<p>and the coefficient update is:</p>\n\n<p>$$w(k+1) = w(k) + \\mu X(k) g(k).$$</p>\n\n<h2 id=\"references\">References</h2>\n\n<p>Implementation*, 5th ed., Algorithm 13.4.</p>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.AffineProjectionCM.__init__", "modulename": "pydaptivefiltering", "qualname": "AffineProjectionCM.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">memory_length</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.AffineProjectionCM.supports_complex", "modulename": "pydaptivefiltering", "qualname": "AffineProjectionCM.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.AffineProjectionCM.step_size", "modulename": "pydaptivefiltering", "qualname": "AffineProjectionCM.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.AffineProjectionCM.memory_length", "modulename": "pydaptivefiltering", "qualname": "AffineProjectionCM.memory_length", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.AffineProjectionCM.gamma", "modulename": "pydaptivefiltering", "qualname": "AffineProjectionCM.gamma", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.AffineProjectionCM.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "AffineProjectionCM.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.AffineProjectionCM.optimize", "modulename": "pydaptivefiltering", "qualname": "AffineProjectionCM.optimize", "kind": "function", "doc": "<p>Executes the AP-CM adaptation loop over an input sequence.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : None, optional\n    Ignored. This is a blind algorithm: the reference is derived from\n    the output via unit-modulus normalization.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes the last internal states in <code>result.extra</code>:\n    <code>\"last_update_factor\"</code> (<code>g(k)</code>) and <code>\"last_regressor_matrix\"</code>\n    (<code>X(k)</code>).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Scalar output sequence, <code>y[k] = y_ap(k)[0]</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        Scalar CM error sequence, <code>e[k] = e_ap(k)[0]</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"blind_constant_modulus\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.CMA", "modulename": "pydaptivefiltering", "qualname": "CMA", "kind": "class", "doc": "<p>Constant-Modulus Algorithm (CMA) for blind adaptive filtering (complex-valued).</p>\n\n<p>The CMA adapts an FIR equalizer to produce an output with (approximately)\nconstant modulus, making it useful for blind equalization of constant-envelope\nand near-constant-envelope modulations (e.g., PSK and some QAM regimes).</p>\n\n<p>This implementation follows Diniz (Alg. 13.2) using the classical CMA(2,2)\ninstantaneous gradient approximation.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int, optional\n    FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\n    Default is 5.\nstep_size : float, optional\n    Adaptation step size <code>mu</code>. Default is 0.01.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Let the regressor vector be <code>x_k = [x[k], x[k-1], ..., x[k-M]]^T</code> and the\nfilter output:</p>\n\n<p>$$y(k) = w^H(k) x_k.$$</p>\n\n<p>CMA(2,2) is commonly derived from minimizing the instantaneous cost:</p>\n\n<p>$$J(k) = \\left(|y(k)|^2 - R_2\\right)^2,$$</p>\n\n<p>where <code>R2</code> is the dispersion constant. Using an instantaneous gradient\napproximation, define the scalar error:</p>\n\n<p>$$e(k) = |y(k)|^2 - R_2,$$</p>\n\n<p>and the (complex) gradient factor:</p>\n\n<p>$$\\phi(k) = 2\\, e(k)\\, y^*(k).$$</p>\n\n<p>The coefficient update is then:</p>\n\n<p>$$w(k+1) = w(k) - \\mu\\, \\phi(k)\\, x_k.$$</p>\n\n<p>Dispersion constant\n<s>~</s><s>~</s><s>~</s>~~~~\nIn theory, <code>R2</code> depends on the source constellation statistics and is\noften written as:</p>\n\n<p>$$R_2 = \\frac{\\mathbb{E}[|s(k)|^4]}{\\mathbb{E}[|s(k)|^2]}.$$</p>\n\n<p>In practice, when the source <code>s(k)</code> is not available (blind setting),\n<code>R2</code> is typically chosen from prior knowledge of the modulation or\nestimated from a proxy sequence. If this implementation estimates <code>R2</code>\nfrom data, it should specify which sequence is used (e.g., input vs output).</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.CMA.__init__", "modulename": "pydaptivefiltering", "qualname": "CMA.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.CMA.supports_complex", "modulename": "pydaptivefiltering", "qualname": "CMA.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.CMA.step_size", "modulename": "pydaptivefiltering", "qualname": "CMA.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.CMA.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "CMA.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.CMA.optimize", "modulename": "pydaptivefiltering", "qualname": "CMA.optimize", "kind": "function", "doc": "<p>Executes the CMA adaptation loop over an input sequence.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : None, optional\n    Ignored. This is a blind algorithm: it does not require a desired\n    reference signal.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes internal quantities in <code>result.extra</code> (e.g.,\n    the dispersion constant <code>R2</code> and/or the last/trajectory of\n    <code>phi(k)</code> depending on the implementation).\nsafe_eps : float, optional\n    Small epsilon used to avoid division by zero if <code>R2</code> is estimated\n    from sample moments. Default is 1e-12.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Output sequence <code>y[k]</code>.\n    - errors : ndarray of float or complex, shape <code>(N,)</code>\n        CMA error sequence <code>e[k] = |y(k)|^2 - R2</code> (usually real-valued).\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"blind_constant_modulus\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.Godard", "modulename": "pydaptivefiltering", "qualname": "Godard", "kind": "class", "doc": "<p>Godard blind adaptive algorithm (complex-valued).</p>\n\n<p>The Godard criterion generalizes constant-modulus equalization by using\nexponents <code>p</code> and <code>q</code> in a family of dispersion-based cost functions.\nIt is commonly used for blind channel equalization and includes CMA(2,2)\nas a special case.</p>\n\n<p>This implementation follows Diniz (Alg. 13.1) and estimates the dispersion\nconstant <code>R_q</code> directly from the <em>input sequence</em> via sample moments.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int, optional\n    FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\n    Default is 5.\nstep_size : float, optional\n    Adaptation step size <code>mu</code>. Default is 0.01.\np_exponent : int, optional\n    Exponent <code>p</code> used in the Godard cost / gradient factor. Default is 2.\nq_exponent : int, optional\n    Exponent <code>q</code> used in the modulus term. Default is 2.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Let the regressor vector be <code>x_k = [x[k], x[k-1], ..., x[k-M]]^T</code> and the\noutput:</p>\n\n<p>$$y(k) = w^H(k) x_k.$$</p>\n\n<p>Define the dispersion error (scalar):</p>\n\n<p>$$e(k) = |y(k)|^q - R_q.$$</p>\n\n<p>In this implementation, the dispersion constant is estimated from the input\nusing sample moments:</p>\n\n<p>$$R_q \\approx \\frac{\\mathbb{E}[|x|^{2q}]}{\\mathbb{E}[|x|^q]}\n\\approx \\frac{\\frac{1}{N}\\sum_k |x(k)|^{2q}}\n             {\\frac{1}{N}\\sum_k |x(k)|^q},$$</p>\n\n<p>with a small <code>safe_eps</code> to prevent division by zero.</p>\n\n<p>The instantaneous complex gradient factor is computed as:</p>\n\n<p>$$\\phi(k) = p\\,q\\, e(k)^{p-1}\\, |y(k)|^{q-2}\\, y^*(k),$$</p>\n\n<p>and the coefficient update used here is:</p>\n\n<p>$$w(k+1) = w(k) - \\frac{\\mu}{2}\\, \\phi(k)\\, x_k.$$</p>\n\n<p>Numerical stability\n<s>~</s><s>~</s><s>~</s>~~~~\nWhen <code>|y(k)|</code> is very small, the term <code>|y(k)|^{q-2}</code> can be ill-defined\nfor <code>q &lt; 2</code> or can amplify noise. This implementation sets <code>phi(k)=0</code>\nwhen <code>|y(k)| &lt;= safe_eps</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.Godard.__init__", "modulename": "pydaptivefiltering", "qualname": "Godard.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">p_exponent</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">q_exponent</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.Godard.supports_complex", "modulename": "pydaptivefiltering", "qualname": "Godard.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.Godard.step_size", "modulename": "pydaptivefiltering", "qualname": "Godard.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.Godard.p", "modulename": "pydaptivefiltering", "qualname": "Godard.p", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.Godard.q", "modulename": "pydaptivefiltering", "qualname": "Godard.q", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.Godard.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "Godard.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.Godard.optimize", "modulename": "pydaptivefiltering", "qualname": "Godard.optimize", "kind": "function", "doc": "<p>Executes the Godard adaptation loop over an input sequence.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : None, optional\n    Ignored. This is a blind algorithm: no desired reference is used.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes internal signals in <code>result.extra</code>:\n    <code>\"dispersion_constant\"</code> (estimated <code>R_q</code>) and <code>\"phi_gradient\"</code>\n    (trajectory of <code>phi(k)</code> with shape <code>(N,)</code>).\nsafe_eps : float, optional\n    Small epsilon used to avoid division by zero when estimating\n    <code>R_q</code> and to gate the computation of <code>phi(k)</code> when <code>|y(k)|</code> is\n    close to zero. Default is 1e-12.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Output sequence <code>y[k]</code>.\n    - errors : ndarray of float, shape <code>(N,)</code>\n        Dispersion error sequence <code>e[k] = |y(k)|^q - R_q</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"blind_godard\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.Sato", "modulename": "pydaptivefiltering", "qualname": "Sato", "kind": "class", "doc": "<p>Sato blind adaptive algorithm (complex-valued).</p>\n\n<p>The Sato criterion is an early blind equalization method particularly\nassociated with multilevel PAM/QAM-type signals. It adapts an FIR equalizer\nby pulling the output toward a fixed magnitude level through the complex\nsign function, using a dispersion constant <code>zeta</code>.</p>\n\n<p>This implementation follows Diniz (Alg. 13.3) and estimates <code>zeta</code> from\nthe <em>input sequence</em> via sample moments.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filter_order : int, optional\n    FIR filter order <code>M</code>. The number of coefficients is <code>M + 1</code>.\n    Default is 5.\nstep_size : float, optional\n    Adaptation step size <code>mu</code>. Default is 0.01.\nw_init : array_like of complex, optional\n    Initial coefficient vector <code>w(0)</code> with shape <code>(M + 1,)</code>. If None,\n    initializes with zeros.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>Let the regressor vector be <code>x_k = [x[k], x[k-1], ..., x[k-M]]^T</code> and the\noutput:</p>\n\n<p>$$y(k) = w^H(k) x_k.$$</p>\n\n<p>Define the complex sign function (unit-circle projection):</p>\n\n<p>$$\\mathrm{csgn}(y) =\n\\begin{cases}\n\\dfrac{y}{|y|}, &amp; |y| &gt; 0 \\\n0, &amp; |y| = 0\n\\end{cases}$$</p>\n\n<p>The Sato error is:</p>\n\n<p>$$e(k) = y(k) - \\zeta\\, \\mathrm{csgn}(y(k)).$$</p>\n\n<p>The coefficient update used here is:</p>\n\n<p>$$w(k+1) = w(k) - \\mu\\, e^*(k)\\, x_k.$$</p>\n\n<p>Dispersion constant\n<s>~</s><s>~</s><s>~</s>~~~~\nIn this implementation, the dispersion constant is estimated from the input\nusing sample moments:</p>\n\n<p>$$\\zeta \\approx \\frac{\\mathbb{E}[|x|^2]}{\\mathbb{E}[|x|]}\n\\approx \\frac{\\frac{1}{N}\\sum_k |x(k)|^2}\n             {\\frac{1}{N}\\sum_k |x(k)|},$$</p>\n\n<p>with a small <code>safe_eps</code> to avoid division by zero.</p>\n\n<p>Numerical stability\n<s>~</s><s>~</s><s>~</s>~~~~\nTo avoid instability when <code>|y(k)|</code> is very small, this implementation\nsets <code>csgn(y(k)) = 0</code> when <code>|y(k)| &lt;= safe_eps</code>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.Sato.__init__", "modulename": "pydaptivefiltering", "qualname": "Sato.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filter_order</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">step_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">w_init</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.Sato.supports_complex", "modulename": "pydaptivefiltering", "qualname": "Sato.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.Sato.step_size", "modulename": "pydaptivefiltering", "qualname": "Sato.step_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "pydaptivefiltering.Sato.n_coeffs", "modulename": "pydaptivefiltering", "qualname": "Sato.n_coeffs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "pydaptivefiltering.Sato.optimize", "modulename": "pydaptivefiltering", "qualname": "Sato.optimize", "kind": "function", "doc": "<p>Executes the Sato adaptation loop over an input sequence.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like of complex\n    Input sequence <code>x[k]</code> with shape <code>(N,)</code> (will be flattened).\ndesired_signal : None, optional\n    Ignored. This is a blind algorithm: no desired reference is used.\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, includes internal signals in <code>result.extra</code>:\n    <code>\"dispersion_constant\"</code> (estimated <code>zeta</code>) and\n    <code>\"sato_sign_track\"</code> (trajectory of <code>csgn(y(k))</code> with shape\n    <code>(N,)</code>).\nsafe_eps : float, optional\n    Small epsilon used to avoid division by zero when estimating\n    <code>zeta</code> and to gate the computation of <code>csgn(y(k))</code> when <code>|y(k)|</code>\n    is close to zero. Default is 1e-12.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    Result object with fields:\n    - outputs : ndarray of complex, shape <code>(N,)</code>\n        Output sequence <code>y[k]</code>.\n    - errors : ndarray of complex, shape <code>(N,)</code>\n        Sato error sequence <code>e[k] = y(k) - zeta*csgn(y(k))</code>.\n    - coefficients : ndarray of complex\n        Coefficient history recorded by the base class.\n    - error_type : str\n        Set to <code>\"blind_sato\"</code>.\n    - extra : dict, optional\n        Present only if <code>return_internal_states=True</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.Kalman", "modulename": "pydaptivefiltering", "qualname": "Kalman", "kind": "class", "doc": "<p>Kalman filter for state estimation (real or complex-valued).</p>\n\n<p>Implements the discrete-time Kalman filter recursion for linear state-space\nmodels with additive process and measurement noise. Matrices may be constant\n(single <code>ndarray</code>) or time-varying (a sequence of arrays indexed by <code>k</code>).</p>\n\n<p>The model used is:</p>\n\n<p>$$x(k) = A(k-1) x(k-1) + B(k) n(k),$$</p>\n\n<p>$$y(k) = C^T(k) x(k) + n_1(k),$$</p>\n\n<p>where \\( n(k) \\) is the process noise with covariance \\( R_n(k) \\) and\n\\( n_1(k) \\) is the measurement noise with covariance \\( R_{n1}(k) \\).</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>API integration\n<s>~</s><s>~</s><s>~</s>\nThis class inherits from <code>~pydaptivefiltering.base.AdaptiveFilter</code> to\nshare a common interface. Here, the \"weights\" are the state estimate:\n<code>self.w</code> stores the current state vector (flattened), and\n<code>self.w_history</code> stores the covariance matrices over time.</p>\n\n<p>Time-varying matrices\n<s>~</s><s>~</s><s>~</s><s>~</s>~\nAny of <code>A</code>, <code>C_T</code>, <code>B</code>, <code>Rn</code>, <code>Rn1</code> may be provided either as:</p>\n\n<ul>\n<li>a constant <code>ndarray</code>, used for all k; or</li>\n<li>a sequence (list/tuple) of <code>ndarray</code>, where element <code>k</code> is used at time k.</li>\n</ul>\n\n<p>Dimensions\n<s>~</s><s>~</s>\nLet <code>n</code> be the state dimension, <code>p</code> the measurement dimension, and <code>q</code>\nthe process-noise dimension. Then:</p>\n\n<ul>\n<li><code>A(k)</code> has shape <code>(n, n)</code></li>\n<li><code>C_T(k)</code> has shape <code>(p, n)</code>  (note: this is \\( C^T \\))</li>\n<li><code>B(k)</code> has shape <code>(n, q)</code></li>\n<li><code>Rn(k)</code> has shape <code>(q, q)</code></li>\n<li><code>Rn1(k)</code> has shape <code>(p, p)</code></li>\n</ul>\n\n<p>If <code>B</code> is not provided, the implementation uses <code>B = I</code> (thus <code>q = n</code>),\nand expects <code>Rn</code> to be shape <code>(n, n)</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>A : ndarray or Sequence[ndarray]\n    State transition matrix \\( A(k-1) \\) with shape <code>(n, n)</code>.\nC_T : ndarray or Sequence[ndarray]\n    Measurement matrix \\( C^T(k) \\) with shape <code>(p, n)</code>.\nRn : ndarray or Sequence[ndarray]\n    Process noise covariance \\( R_n(k) \\) with shape <code>(q, q)</code>.\nRn1 : ndarray or Sequence[ndarray]\n    Measurement noise covariance \\( R_{n1}(k) \\) with shape <code>(p, p)</code>.\nB : ndarray or Sequence[ndarray], optional\n    Process noise input matrix \\( B(k) \\) with shape <code>(n, q)</code>.\n    If None, uses identity.\nx_init : ndarray, optional\n    Initial state estimate \\( x(0|0) \\). Accepts shapes compatible with\n    <code>(n,)</code>, <code>(n,1)</code>, or <code>(1,n)</code>. If None, initializes with zeros.\nRe_init : ndarray, optional\n    Initial estimation error covariance \\( R_e(0|0) \\) with shape <code>(n, n)</code>.\n    If None, initializes with identity.</p>\n\n<h2 id=\"references\">References</h2>\n\n<div class=\"footnotes\">\n<hr />\n<ol>\n</ol>\n</div>\n", "bases": "pydaptivefiltering.base.AdaptiveFilter"}, {"fullname": "pydaptivefiltering.Kalman.__init__", "modulename": "pydaptivefiltering", "qualname": "Kalman.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">C_T</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">Rn</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">Rn1</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">B</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">],</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">x_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">Re_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "pydaptivefiltering.Kalman.supports_complex", "modulename": "pydaptivefiltering", "qualname": "Kalman.supports_complex", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "pydaptivefiltering.Kalman.A", "modulename": "pydaptivefiltering", "qualname": "Kalman.A", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Union[numpy.ndarray, Sequence[numpy.ndarray]]"}, {"fullname": "pydaptivefiltering.Kalman.C_T", "modulename": "pydaptivefiltering", "qualname": "Kalman.C_T", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Union[numpy.ndarray, Sequence[numpy.ndarray]]"}, {"fullname": "pydaptivefiltering.Kalman.Rn", "modulename": "pydaptivefiltering", "qualname": "Kalman.Rn", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Union[numpy.ndarray, Sequence[numpy.ndarray]]"}, {"fullname": "pydaptivefiltering.Kalman.Rn1", "modulename": "pydaptivefiltering", "qualname": "Kalman.Rn1", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Union[numpy.ndarray, Sequence[numpy.ndarray]]"}, {"fullname": "pydaptivefiltering.Kalman.B", "modulename": "pydaptivefiltering", "qualname": "Kalman.B", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Union[numpy.ndarray, Sequence[numpy.ndarray], NoneType]"}, {"fullname": "pydaptivefiltering.Kalman.x", "modulename": "pydaptivefiltering", "qualname": "Kalman.x", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.Kalman.Re", "modulename": "pydaptivefiltering", "qualname": "Kalman.Re", "kind": "variable", "doc": "<p></p>\n", "annotation": ": numpy.ndarray"}, {"fullname": "pydaptivefiltering.Kalman.regressor", "modulename": "pydaptivefiltering", "qualname": "Kalman.regressor", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.Kalman.w", "modulename": "pydaptivefiltering", "qualname": "Kalman.w", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.Kalman.w_history", "modulename": "pydaptivefiltering", "qualname": "Kalman.w_history", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "pydaptivefiltering.Kalman.optimize", "modulename": "pydaptivefiltering", "qualname": "Kalman.optimize", "kind": "function", "doc": "<p>Executes the Kalman recursion for a sequence of measurements <code>y[k]</code>.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_signal : array_like\n    Measurement sequence <code>y[k]</code>. Accepted shapes:\n    - <code>(N,)</code>       for scalar measurements\n    - <code>(N, p)</code>     for p-dimensional measurements\n    - <code>(N, p, 1)</code>  also accepted (squeezed to <code>(N, p)</code>)\ndesired_signal : array_like, optional\n    Ignored (kept only for API standardization).\nverbose : bool, optional\n    If True, prints the total runtime after completion.\nreturn_internal_states : bool, optional\n    If True, returns selected internal values in <code>result.extra</code>.\nsafe_eps : float, optional\n    Small positive value used to regularize the innovation covariance\n    matrix if a linear solve fails (numerical stabilization).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationResult\n    outputs : ndarray\n        State estimates <code>x(k|k)</code>, shape <code>(N, n)</code>.\n    errors : ndarray\n        Innovations <code>v(k) = y(k) - C^T(k) x(k|k-1)</code>, shape <code>(N, p)</code>.\n    coefficients : ndarray\n        Covariance history <code>R_e(k|k)</code>, shape <code>(N, n, n)</code>.\n    error_type : str\n        <code>\"innovation\"</code>.\n    extra : dict, optional\n        Present only if <code>return_internal_states=True</code>. See below.</p>\n\n<h2 id=\"extra-when-return_internal_statestrue\">Extra (when return_internal_states=True)</h2>\n\n<p>kalman_gain_last : ndarray\n    Kalman gain <code>K</code> at the last iteration, shape <code>(n, p)</code>.\npredicted_state_last : ndarray\n    Predicted state <code>x(k|k-1)</code> at the last iteration, shape <code>(n,)</code>.\npredicted_cov_last : ndarray\n    Predicted covariance <code>R_e(k|k-1)</code> at the last iteration, shape <code>(n, n)</code>.\ninnovation_cov_last : ndarray\n    Innovation covariance <code>S</code> at the last iteration, shape <code>(p, p)</code>.\nsafe_eps : float\n    The stabilization epsilon used when regularizing <code>S</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">desired_signal</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_internal_states</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">safe_eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-12</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pydaptivefiltering</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">OptimizationResult</span>:</span></span>", "funcdef": "def"}, {"fullname": "pydaptivefiltering.info", "modulename": "pydaptivefiltering", "qualname": "info", "kind": "function", "doc": "<p>Imprime informa\u00e7\u00f5es sobre a cobertura de algoritmos da biblioteca.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();